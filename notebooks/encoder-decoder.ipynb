{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "         \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(500, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 9)\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 18),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(18, 36),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(36, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 500),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(df.columns[:20], axis=1)\n",
    "y = df[df.columns[3:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AE()\n",
    " \n",
    "# Validation using MSE Loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    " \n",
    "# Using an Adam Optimizer with lr = 0.1\n",
    "optimizer = torch.optim.Adam(encoder.parameters(),\n",
    "                             lr = 1e-3,\n",
    "                             weight_decay = 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, torch.tensor(y.values, dtype=torch.float32), test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder():\n",
    "    min_loss = 1000\n",
    "    encoder.train()\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    patience = 0\n",
    "    while True:\n",
    "        for i, _ in train_loader:\n",
    "            reconstruct = encoder(i)\n",
    "            loss = loss_function(reconstruct, i)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            if(loss.item() < min_loss):\n",
    "                min_loss = loss.item()\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience == 20:\n",
    "                    return losses\n",
    "        outputs.append((10, i, reconstruct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29fb72790>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "losses = train_encoder()\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Plotting the last 100 values\n",
    "plt.plot(losses[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024000000208616257"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self, input_size=100, out_features=5):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, out_features=20)\n",
    "        self.bn1 = nn.BatchNorm1d(20)\n",
    "        self.fc2 = nn.Linear(in_features=20, out_features=20)\n",
    "        self.bn2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(in_features=20, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(0.0)  # Example dropout, adjust the rate as needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))  # ReLU activation at the output layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredLogarithmicError(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanSquaredLogarithmicError, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.clamp(y_pred, min=0)\n",
    "        y_true = torch.clamp(y_true, min=0)\n",
    "        \n",
    "        return torch.mean((torch.log1p(y_true) - torch.log1p(y_pred)) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = NNModel(9, 17)\n",
    "criterion = MeanSquaredLogarithmicError()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "[1] Training loss: 0.22139, Validation loss: 0.15495\n",
      "[2] Training loss: 0.22113, Validation loss: 0.15497\n",
      "[3] Training loss: 0.22089, Validation loss: 0.15499\n",
      "[4] Training loss: 0.22066, Validation loss: 0.15502\n",
      "[5] Training loss: 0.22046, Validation loss: 0.15505\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23534, Validation loss: 0.34174\n",
      "[2] Training loss: 0.23531, Validation loss: 0.34150\n",
      "[3] Training loss: 0.23527, Validation loss: 0.34126\n",
      "[4] Training loss: 0.23522, Validation loss: 0.34103\n",
      "[5] Training loss: 0.23515, Validation loss: 0.34082\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23063, Validation loss: 0.09676\n",
      "[2] Training loss: 0.23057, Validation loss: 0.09640\n",
      "[3] Training loss: 0.23051, Validation loss: 0.09605\n",
      "[4] Training loss: 0.23045, Validation loss: 0.09572\n",
      "[5] Training loss: 0.23040, Validation loss: 0.09541\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11477, Validation loss: 0.58798\n",
      "[2] Training loss: 0.11461, Validation loss: 0.58826\n",
      "[3] Training loss: 0.11442, Validation loss: 0.58853\n",
      "[4] Training loss: 0.11420, Validation loss: 0.58877\n",
      "[5] Training loss: 0.11398, Validation loss: 0.58900\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21344, Validation loss: 0.48422\n",
      "[2] Training loss: 0.21338, Validation loss: 0.48408\n",
      "[3] Training loss: 0.21330, Validation loss: 0.48410\n",
      "[4] Training loss: 0.21321, Validation loss: 0.48424\n",
      "[5] Training loss: 0.21311, Validation loss: 0.48448\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22027, Validation loss: 0.15182\n",
      "[2] Training loss: 0.22035, Validation loss: 0.15167\n",
      "[3] Training loss: 0.22040, Validation loss: 0.15158\n",
      "[4] Training loss: 0.22043, Validation loss: 0.15155\n",
      "[5] Training loss: 0.22043, Validation loss: 0.15156\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23471, Validation loss: 0.34000\n",
      "[2] Training loss: 0.23471, Validation loss: 0.34018\n",
      "[3] Training loss: 0.23469, Validation loss: 0.34033\n",
      "[4] Training loss: 0.23465, Validation loss: 0.34047\n",
      "[5] Training loss: 0.23461, Validation loss: 0.34060\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23035, Validation loss: 0.09102\n",
      "[2] Training loss: 0.23033, Validation loss: 0.09098\n",
      "[3] Training loss: 0.23030, Validation loss: 0.09096\n",
      "[4] Training loss: 0.23027, Validation loss: 0.09097\n",
      "[5] Training loss: 0.23024, Validation loss: 0.09099\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11261, Validation loss: 0.58827\n",
      "[2] Training loss: 0.11258, Validation loss: 0.58850\n",
      "[3] Training loss: 0.11252, Validation loss: 0.58874\n",
      "[4] Training loss: 0.11244, Validation loss: 0.58897\n",
      "[5] Training loss: 0.11234, Validation loss: 0.58920\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21334, Validation loss: 0.48571\n",
      "[2] Training loss: 0.21334, Validation loss: 0.48565\n",
      "[3] Training loss: 0.21331, Validation loss: 0.48569\n",
      "[4] Training loss: 0.21325, Validation loss: 0.48583\n",
      "[5] Training loss: 0.21317, Validation loss: 0.48604\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22072, Validation loss: 0.15147\n",
      "[2] Training loss: 0.22078, Validation loss: 0.15141\n",
      "[3] Training loss: 0.22081, Validation loss: 0.15139\n",
      "[4] Training loss: 0.22082, Validation loss: 0.15142\n",
      "[5] Training loss: 0.22080, Validation loss: 0.15149\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23420, Validation loss: 0.34064\n",
      "[2] Training loss: 0.23420, Validation loss: 0.34075\n",
      "[3] Training loss: 0.23418, Validation loss: 0.34085\n",
      "[4] Training loss: 0.23415, Validation loss: 0.34094\n",
      "[5] Training loss: 0.23412, Validation loss: 0.34100\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23016, Validation loss: 0.09026\n",
      "[2] Training loss: 0.23014, Validation loss: 0.09029\n",
      "[3] Training loss: 0.23012, Validation loss: 0.09033\n",
      "[4] Training loss: 0.23010, Validation loss: 0.09038\n",
      "[5] Training loss: 0.23007, Validation loss: 0.09042\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11194, Validation loss: 0.58967\n",
      "[2] Training loss: 0.11193, Validation loss: 0.58992\n",
      "[3] Training loss: 0.11189, Validation loss: 0.59016\n",
      "[4] Training loss: 0.11182, Validation loss: 0.59039\n",
      "[5] Training loss: 0.11174, Validation loss: 0.59062\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21346, Validation loss: 0.48532\n",
      "[2] Training loss: 0.21345, Validation loss: 0.48517\n",
      "[3] Training loss: 0.21341, Validation loss: 0.48513\n",
      "[4] Training loss: 0.21334, Validation loss: 0.48520\n",
      "[5] Training loss: 0.21325, Validation loss: 0.48534\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22101, Validation loss: 0.15147\n",
      "[2] Training loss: 0.22107, Validation loss: 0.15138\n",
      "[3] Training loss: 0.22110, Validation loss: 0.15135\n",
      "[4] Training loss: 0.22110, Validation loss: 0.15137\n",
      "[5] Training loss: 0.22108, Validation loss: 0.15143\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23393, Validation loss: 0.34038\n",
      "[2] Training loss: 0.23393, Validation loss: 0.34048\n",
      "[3] Training loss: 0.23393, Validation loss: 0.34057\n",
      "[4] Training loss: 0.23391, Validation loss: 0.34064\n",
      "[5] Training loss: 0.23388, Validation loss: 0.34070\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23008, Validation loss: 0.08996\n",
      "[2] Training loss: 0.23007, Validation loss: 0.08998\n",
      "[3] Training loss: 0.23005, Validation loss: 0.09001\n",
      "[4] Training loss: 0.23003, Validation loss: 0.09004\n",
      "[5] Training loss: 0.23001, Validation loss: 0.09008\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11149, Validation loss: 0.59062\n",
      "[2] Training loss: 0.11148, Validation loss: 0.59083\n",
      "[3] Training loss: 0.11145, Validation loss: 0.59105\n",
      "[4] Training loss: 0.11139, Validation loss: 0.59126\n",
      "[5] Training loss: 0.11131, Validation loss: 0.59147\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21347, Validation loss: 0.48357\n",
      "[2] Training loss: 0.21345, Validation loss: 0.48356\n",
      "[3] Training loss: 0.21337, Validation loss: 0.48369\n",
      "[4] Training loss: 0.21326, Validation loss: 0.48394\n",
      "[5] Training loss: 0.21314, Validation loss: 0.48426\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22128, Validation loss: 0.15117\n",
      "[2] Training loss: 0.22134, Validation loss: 0.15107\n",
      "[3] Training loss: 0.22137, Validation loss: 0.15104\n",
      "[4] Training loss: 0.22137, Validation loss: 0.15105\n",
      "[5] Training loss: 0.22135, Validation loss: 0.15110\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23387, Validation loss: 0.34078\n",
      "[2] Training loss: 0.23388, Validation loss: 0.34092\n",
      "[3] Training loss: 0.23388, Validation loss: 0.34103\n",
      "[4] Training loss: 0.23387, Validation loss: 0.34112\n",
      "[5] Training loss: 0.23385, Validation loss: 0.34120\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23010, Validation loss: 0.08981\n",
      "[2] Training loss: 0.23009, Validation loss: 0.08984\n",
      "[3] Training loss: 0.23008, Validation loss: 0.08988\n",
      "[4] Training loss: 0.23006, Validation loss: 0.08992\n",
      "[5] Training loss: 0.23004, Validation loss: 0.08997\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11125, Validation loss: 0.58941\n",
      "[2] Training loss: 0.11124, Validation loss: 0.58959\n",
      "[3] Training loss: 0.11121, Validation loss: 0.58977\n",
      "[4] Training loss: 0.11116, Validation loss: 0.58995\n",
      "[5] Training loss: 0.11108, Validation loss: 0.59013\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21313, Validation loss: 0.48362\n",
      "[2] Training loss: 0.21312, Validation loss: 0.48338\n",
      "[3] Training loss: 0.21309, Validation loss: 0.48326\n",
      "[4] Training loss: 0.21302, Validation loss: 0.48324\n",
      "[5] Training loss: 0.21294, Validation loss: 0.48332\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22146, Validation loss: 0.15093\n",
      "[2] Training loss: 0.22151, Validation loss: 0.15085\n",
      "[3] Training loss: 0.22153, Validation loss: 0.15083\n",
      "[4] Training loss: 0.22152, Validation loss: 0.15085\n",
      "[5] Training loss: 0.22149, Validation loss: 0.15092\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23379, Validation loss: 0.34037\n",
      "[2] Training loss: 0.23379, Validation loss: 0.34043\n",
      "[3] Training loss: 0.23378, Validation loss: 0.34049\n",
      "[4] Training loss: 0.23377, Validation loss: 0.34053\n",
      "[5] Training loss: 0.23374, Validation loss: 0.34057\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23006, Validation loss: 0.08953\n",
      "[2] Training loss: 0.23005, Validation loss: 0.08955\n",
      "[3] Training loss: 0.23003, Validation loss: 0.08958\n",
      "[4] Training loss: 0.23001, Validation loss: 0.08962\n",
      "[5] Training loss: 0.22999, Validation loss: 0.08966\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11101, Validation loss: 0.59029\n",
      "[2] Training loss: 0.11100, Validation loss: 0.59050\n",
      "[3] Training loss: 0.11097, Validation loss: 0.59070\n",
      "[4] Training loss: 0.11092, Validation loss: 0.59090\n",
      "[5] Training loss: 0.11085, Validation loss: 0.59109\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21323, Validation loss: 0.48099\n",
      "[2] Training loss: 0.21322, Validation loss: 0.48090\n",
      "[3] Training loss: 0.21316, Validation loss: 0.48098\n",
      "[4] Training loss: 0.21307, Validation loss: 0.48120\n",
      "[5] Training loss: 0.21295, Validation loss: 0.48152\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22154, Validation loss: 0.15081\n",
      "[2] Training loss: 0.22159, Validation loss: 0.15073\n",
      "[3] Training loss: 0.22162, Validation loss: 0.15070\n",
      "[4] Training loss: 0.22161, Validation loss: 0.15072\n",
      "[5] Training loss: 0.22158, Validation loss: 0.15078\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23373, Validation loss: 0.34052\n",
      "[2] Training loss: 0.23374, Validation loss: 0.34064\n",
      "[3] Training loss: 0.23374, Validation loss: 0.34075\n",
      "[4] Training loss: 0.23372, Validation loss: 0.34084\n",
      "[5] Training loss: 0.23370, Validation loss: 0.34092\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23006, Validation loss: 0.08943\n",
      "[2] Training loss: 0.23005, Validation loss: 0.08947\n",
      "[3] Training loss: 0.23004, Validation loss: 0.08951\n",
      "[4] Training loss: 0.23002, Validation loss: 0.08955\n",
      "[5] Training loss: 0.23000, Validation loss: 0.08959\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11088, Validation loss: 0.58950\n",
      "[2] Training loss: 0.11088, Validation loss: 0.58968\n",
      "[3] Training loss: 0.11085, Validation loss: 0.58986\n",
      "[4] Training loss: 0.11080, Validation loss: 0.59004\n",
      "[5] Training loss: 0.11073, Validation loss: 0.59021\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21301, Validation loss: 0.48101\n",
      "[2] Training loss: 0.21300, Validation loss: 0.48080\n",
      "[3] Training loss: 0.21296, Validation loss: 0.48072\n",
      "[4] Training loss: 0.21290, Validation loss: 0.48075\n",
      "[5] Training loss: 0.21281, Validation loss: 0.48086\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22161, Validation loss: 0.15065\n",
      "[2] Training loss: 0.22166, Validation loss: 0.15058\n",
      "[3] Training loss: 0.22168, Validation loss: 0.15056\n",
      "[4] Training loss: 0.22167, Validation loss: 0.15058\n",
      "[5] Training loss: 0.22164, Validation loss: 0.15065\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23367, Validation loss: 0.34022\n",
      "[2] Training loss: 0.23367, Validation loss: 0.34029\n",
      "[3] Training loss: 0.23367, Validation loss: 0.34035\n",
      "[4] Training loss: 0.23365, Validation loss: 0.34040\n",
      "[5] Training loss: 0.23363, Validation loss: 0.34044\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23002, Validation loss: 0.08916\n",
      "[2] Training loss: 0.23001, Validation loss: 0.08918\n",
      "[3] Training loss: 0.23000, Validation loss: 0.08921\n",
      "[4] Training loss: 0.22998, Validation loss: 0.08924\n",
      "[5] Training loss: 0.22996, Validation loss: 0.08927\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11073, Validation loss: 0.59013\n",
      "[2] Training loss: 0.11073, Validation loss: 0.59032\n",
      "[3] Training loss: 0.11070, Validation loss: 0.59052\n",
      "[4] Training loss: 0.11065, Validation loss: 0.59071\n",
      "[5] Training loss: 0.11058, Validation loss: 0.59089\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21308, Validation loss: 0.47907\n",
      "[2] Training loss: 0.21307, Validation loss: 0.47906\n",
      "[3] Training loss: 0.21301, Validation loss: 0.47926\n",
      "[4] Training loss: 0.21290, Validation loss: 0.47960\n",
      "[5] Training loss: 0.21278, Validation loss: 0.48001\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22165, Validation loss: 0.15053\n",
      "[2] Training loss: 0.22170, Validation loss: 0.15045\n",
      "[3] Training loss: 0.22173, Validation loss: 0.15042\n",
      "[4] Training loss: 0.22172, Validation loss: 0.15044\n",
      "[5] Training loss: 0.22169, Validation loss: 0.15050\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23365, Validation loss: 0.34064\n",
      "[2] Training loss: 0.23366, Validation loss: 0.34077\n",
      "[3] Training loss: 0.23365, Validation loss: 0.34089\n",
      "[4] Training loss: 0.23364, Validation loss: 0.34099\n",
      "[5] Training loss: 0.23362, Validation loss: 0.34107\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.23004, Validation loss: 0.08928\n",
      "[2] Training loss: 0.23003, Validation loss: 0.08932\n",
      "[3] Training loss: 0.23001, Validation loss: 0.08938\n",
      "[4] Training loss: 0.22999, Validation loss: 0.08944\n",
      "[5] Training loss: 0.22997, Validation loss: 0.08948\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11066, Validation loss: 0.58865\n",
      "[2] Training loss: 0.11066, Validation loss: 0.58881\n",
      "[3] Training loss: 0.11063, Validation loss: 0.58899\n",
      "[4] Training loss: 0.11058, Validation loss: 0.58916\n",
      "[5] Training loss: 0.11051, Validation loss: 0.58932\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21284, Validation loss: 0.48027\n",
      "[2] Training loss: 0.21284, Validation loss: 0.48002\n",
      "[3] Training loss: 0.21281, Validation loss: 0.47989\n",
      "[4] Training loss: 0.21275, Validation loss: 0.47988\n",
      "[5] Training loss: 0.21267, Validation loss: 0.47996\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22167, Validation loss: 0.15042\n",
      "[2] Training loss: 0.22172, Validation loss: 0.15035\n",
      "[3] Training loss: 0.22174, Validation loss: 0.15033\n",
      "[4] Training loss: 0.22173, Validation loss: 0.15036\n",
      "[5] Training loss: 0.22169, Validation loss: 0.15044\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23358, Validation loss: 0.34023\n",
      "[2] Training loss: 0.23358, Validation loss: 0.34028\n",
      "[3] Training loss: 0.23357, Validation loss: 0.34034\n",
      "[4] Training loss: 0.23355, Validation loss: 0.34038\n",
      "[5] Training loss: 0.23353, Validation loss: 0.34041\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22998, Validation loss: 0.08867\n",
      "[2] Training loss: 0.22997, Validation loss: 0.08869\n",
      "[3] Training loss: 0.22996, Validation loss: 0.08871\n",
      "[4] Training loss: 0.22994, Validation loss: 0.08875\n",
      "[5] Training loss: 0.22991, Validation loss: 0.08878\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11051, Validation loss: 0.58978\n",
      "[2] Training loss: 0.11051, Validation loss: 0.58999\n",
      "[3] Training loss: 0.11049, Validation loss: 0.59018\n",
      "[4] Training loss: 0.11044, Validation loss: 0.59037\n",
      "[5] Training loss: 0.11037, Validation loss: 0.59056\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21302, Validation loss: 0.47804\n",
      "[2] Training loss: 0.21301, Validation loss: 0.47801\n",
      "[3] Training loss: 0.21295, Validation loss: 0.47816\n",
      "[4] Training loss: 0.21285, Validation loss: 0.47848\n",
      "[5] Training loss: 0.21272, Validation loss: 0.47887\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22166, Validation loss: 0.15035\n",
      "[2] Training loss: 0.22172, Validation loss: 0.15026\n",
      "[3] Training loss: 0.22174, Validation loss: 0.15023\n",
      "[4] Training loss: 0.22174, Validation loss: 0.15025\n",
      "[5] Training loss: 0.22170, Validation loss: 0.15032\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23355, Validation loss: 0.34057\n",
      "[2] Training loss: 0.23356, Validation loss: 0.34070\n",
      "[3] Training loss: 0.23355, Validation loss: 0.34082\n",
      "[4] Training loss: 0.23354, Validation loss: 0.34092\n",
      "[5] Training loss: 0.23352, Validation loss: 0.34100\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22998, Validation loss: 0.08873\n",
      "[2] Training loss: 0.22997, Validation loss: 0.08877\n",
      "[3] Training loss: 0.22996, Validation loss: 0.08883\n",
      "[4] Training loss: 0.22994, Validation loss: 0.08889\n",
      "[5] Training loss: 0.22992, Validation loss: 0.08893\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11046, Validation loss: 0.58860\n",
      "[2] Training loss: 0.11047, Validation loss: 0.58876\n",
      "[3] Training loss: 0.11044, Validation loss: 0.58893\n",
      "[4] Training loss: 0.11039, Validation loss: 0.58910\n",
      "[5] Training loss: 0.11032, Validation loss: 0.58926\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21277, Validation loss: 0.47925\n",
      "[2] Training loss: 0.21277, Validation loss: 0.47903\n",
      "[3] Training loss: 0.21273, Validation loss: 0.47897\n",
      "[4] Training loss: 0.21266, Validation loss: 0.47902\n",
      "[5] Training loss: 0.21258, Validation loss: 0.47918\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22168, Validation loss: 0.15022\n",
      "[2] Training loss: 0.22173, Validation loss: 0.15014\n",
      "[3] Training loss: 0.22175, Validation loss: 0.15012\n",
      "[4] Training loss: 0.22174, Validation loss: 0.15015\n",
      "[5] Training loss: 0.22170, Validation loss: 0.15022\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23351, Validation loss: 0.34058\n",
      "[2] Training loss: 0.23351, Validation loss: 0.34066\n",
      "[3] Training loss: 0.23351, Validation loss: 0.34073\n",
      "[4] Training loss: 0.23349, Validation loss: 0.34079\n",
      "[5] Training loss: 0.23346, Validation loss: 0.34083\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22995, Validation loss: 0.08887\n",
      "[2] Training loss: 0.22994, Validation loss: 0.08890\n",
      "[3] Training loss: 0.22992, Validation loss: 0.08895\n",
      "[4] Training loss: 0.22990, Validation loss: 0.08899\n",
      "[5] Training loss: 0.22988, Validation loss: 0.08903\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11037, Validation loss: 0.58859\n",
      "[2] Training loss: 0.11037, Validation loss: 0.58877\n",
      "[3] Training loss: 0.11034, Validation loss: 0.58896\n",
      "[4] Training loss: 0.11030, Validation loss: 0.58914\n",
      "[5] Training loss: 0.11023, Validation loss: 0.58931\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21282, Validation loss: 0.47771\n",
      "[2] Training loss: 0.21281, Validation loss: 0.47762\n",
      "[3] Training loss: 0.21276, Validation loss: 0.47772\n",
      "[4] Training loss: 0.21267, Validation loss: 0.47793\n",
      "[5] Training loss: 0.21257, Validation loss: 0.47823\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22166, Validation loss: 0.15016\n",
      "[2] Training loss: 0.22171, Validation loss: 0.15009\n",
      "[3] Training loss: 0.22173, Validation loss: 0.15006\n",
      "[4] Training loss: 0.22172, Validation loss: 0.15009\n",
      "[5] Training loss: 0.22168, Validation loss: 0.15016\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23347, Validation loss: 0.34074\n",
      "[2] Training loss: 0.23347, Validation loss: 0.34085\n",
      "[3] Training loss: 0.23347, Validation loss: 0.34095\n",
      "[4] Training loss: 0.23346, Validation loss: 0.34103\n",
      "[5] Training loss: 0.23343, Validation loss: 0.34110\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22993, Validation loss: 0.08870\n",
      "[2] Training loss: 0.22992, Validation loss: 0.08873\n",
      "[3] Training loss: 0.22990, Validation loss: 0.08878\n",
      "[4] Training loss: 0.22988, Validation loss: 0.08883\n",
      "[5] Training loss: 0.22986, Validation loss: 0.08888\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11031, Validation loss: 0.58832\n",
      "[2] Training loss: 0.11031, Validation loss: 0.58849\n",
      "[3] Training loss: 0.11029, Validation loss: 0.58868\n",
      "[4] Training loss: 0.11024, Validation loss: 0.58886\n",
      "[5] Training loss: 0.11017, Validation loss: 0.58904\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21272, Validation loss: 0.47783\n",
      "[2] Training loss: 0.21271, Validation loss: 0.47759\n",
      "[3] Training loss: 0.21267, Validation loss: 0.47752\n",
      "[4] Training loss: 0.21260, Validation loss: 0.47756\n",
      "[5] Training loss: 0.21252, Validation loss: 0.47770\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22165, Validation loss: 0.15008\n",
      "[2] Training loss: 0.22170, Validation loss: 0.15001\n",
      "[3] Training loss: 0.22172, Validation loss: 0.14999\n",
      "[4] Training loss: 0.22171, Validation loss: 0.15002\n",
      "[5] Training loss: 0.22167, Validation loss: 0.15009\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23341, Validation loss: 0.34053\n",
      "[2] Training loss: 0.23341, Validation loss: 0.34061\n",
      "[3] Training loss: 0.23341, Validation loss: 0.34068\n",
      "[4] Training loss: 0.23339, Validation loss: 0.34074\n",
      "[5] Training loss: 0.23337, Validation loss: 0.34079\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22988, Validation loss: 0.08870\n",
      "[2] Training loss: 0.22987, Validation loss: 0.08874\n",
      "[3] Training loss: 0.22985, Validation loss: 0.08878\n",
      "[4] Training loss: 0.22984, Validation loss: 0.08882\n",
      "[5] Training loss: 0.22981, Validation loss: 0.08886\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11021, Validation loss: 0.58838\n",
      "[2] Training loss: 0.11021, Validation loss: 0.58856\n",
      "[3] Training loss: 0.11019, Validation loss: 0.58874\n",
      "[4] Training loss: 0.11014, Validation loss: 0.58891\n",
      "[5] Training loss: 0.11007, Validation loss: 0.58908\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21282, Validation loss: 0.47647\n",
      "[2] Training loss: 0.21282, Validation loss: 0.47644\n",
      "[3] Training loss: 0.21275, Validation loss: 0.47664\n",
      "[4] Training loss: 0.21265, Validation loss: 0.47705\n",
      "[5] Training loss: 0.21251, Validation loss: 0.47754\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22162, Validation loss: 0.15002\n",
      "[2] Training loss: 0.22168, Validation loss: 0.14994\n",
      "[3] Training loss: 0.22170, Validation loss: 0.14992\n",
      "[4] Training loss: 0.22169, Validation loss: 0.14994\n",
      "[5] Training loss: 0.22165, Validation loss: 0.15001\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23340, Validation loss: 0.34098\n",
      "[2] Training loss: 0.23341, Validation loss: 0.34110\n",
      "[3] Training loss: 0.23341, Validation loss: 0.34121\n",
      "[4] Training loss: 0.23340, Validation loss: 0.34130\n",
      "[5] Training loss: 0.23337, Validation loss: 0.34137\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22989, Validation loss: 0.08826\n",
      "[2] Training loss: 0.22987, Validation loss: 0.08828\n",
      "[3] Training loss: 0.22986, Validation loss: 0.08832\n",
      "[4] Training loss: 0.22984, Validation loss: 0.08838\n",
      "[5] Training loss: 0.22982, Validation loss: 0.08844\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11021, Validation loss: 0.58774\n",
      "[2] Training loss: 0.11021, Validation loss: 0.58792\n",
      "[3] Training loss: 0.11018, Validation loss: 0.58810\n",
      "[4] Training loss: 0.11013, Validation loss: 0.58828\n",
      "[5] Training loss: 0.11006, Validation loss: 0.58846\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21260, Validation loss: 0.47828\n",
      "[2] Training loss: 0.21260, Validation loss: 0.47798\n",
      "[3] Training loss: 0.21256, Validation loss: 0.47783\n",
      "[4] Training loss: 0.21250, Validation loss: 0.47781\n",
      "[5] Training loss: 0.21242, Validation loss: 0.47791\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22162, Validation loss: 0.14991\n",
      "[2] Training loss: 0.22167, Validation loss: 0.14984\n",
      "[3] Training loss: 0.22168, Validation loss: 0.14982\n",
      "[4] Training loss: 0.22167, Validation loss: 0.14985\n",
      "[5] Training loss: 0.22163, Validation loss: 0.14993\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23334, Validation loss: 0.34066\n",
      "[2] Training loss: 0.23335, Validation loss: 0.34073\n",
      "[3] Training loss: 0.23334, Validation loss: 0.34080\n",
      "[4] Training loss: 0.23332, Validation loss: 0.34087\n",
      "[5] Training loss: 0.23330, Validation loss: 0.34092\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22982, Validation loss: 0.08822\n",
      "[2] Training loss: 0.22981, Validation loss: 0.08825\n",
      "[3] Training loss: 0.22980, Validation loss: 0.08829\n",
      "[4] Training loss: 0.22978, Validation loss: 0.08833\n",
      "[5] Training loss: 0.22976, Validation loss: 0.08837\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11007, Validation loss: 0.58799\n",
      "[2] Training loss: 0.11007, Validation loss: 0.58817\n",
      "[3] Training loss: 0.11005, Validation loss: 0.58834\n",
      "[4] Training loss: 0.11000, Validation loss: 0.58852\n",
      "[5] Training loss: 0.10993, Validation loss: 0.58869\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21276, Validation loss: 0.47654\n",
      "[2] Training loss: 0.21275, Validation loss: 0.47654\n",
      "[3] Training loss: 0.21269, Validation loss: 0.47677\n",
      "[4] Training loss: 0.21259, Validation loss: 0.47715\n",
      "[5] Training loss: 0.21248, Validation loss: 0.47762\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22157, Validation loss: 0.14990\n",
      "[2] Training loss: 0.22162, Validation loss: 0.14983\n",
      "[3] Training loss: 0.22164, Validation loss: 0.14980\n",
      "[4] Training loss: 0.22163, Validation loss: 0.14983\n",
      "[5] Training loss: 0.22160, Validation loss: 0.14991\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23331, Validation loss: 0.34135\n",
      "[2] Training loss: 0.23332, Validation loss: 0.34148\n",
      "[3] Training loss: 0.23331, Validation loss: 0.34158\n",
      "[4] Training loss: 0.23330, Validation loss: 0.34167\n",
      "[5] Training loss: 0.23328, Validation loss: 0.34174\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22983, Validation loss: 0.08789\n",
      "[2] Training loss: 0.22982, Validation loss: 0.08792\n",
      "[3] Training loss: 0.22980, Validation loss: 0.08796\n",
      "[4] Training loss: 0.22978, Validation loss: 0.08802\n",
      "[5] Training loss: 0.22976, Validation loss: 0.08809\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.11007, Validation loss: 0.58728\n",
      "[2] Training loss: 0.11007, Validation loss: 0.58746\n",
      "[3] Training loss: 0.11004, Validation loss: 0.58765\n",
      "[4] Training loss: 0.10999, Validation loss: 0.58784\n",
      "[5] Training loss: 0.10993, Validation loss: 0.58803\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21256, Validation loss: 0.47814\n",
      "[2] Training loss: 0.21256, Validation loss: 0.47786\n",
      "[3] Training loss: 0.21252, Validation loss: 0.47773\n",
      "[4] Training loss: 0.21245, Validation loss: 0.47773\n",
      "[5] Training loss: 0.21236, Validation loss: 0.47784\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22156, Validation loss: 0.14979\n",
      "[2] Training loss: 0.22161, Validation loss: 0.14971\n",
      "[3] Training loss: 0.22163, Validation loss: 0.14969\n",
      "[4] Training loss: 0.22162, Validation loss: 0.14972\n",
      "[5] Training loss: 0.22158, Validation loss: 0.14979\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23328, Validation loss: 0.34089\n",
      "[2] Training loss: 0.23328, Validation loss: 0.34096\n",
      "[3] Training loss: 0.23328, Validation loss: 0.34102\n",
      "[4] Training loss: 0.23326, Validation loss: 0.34107\n",
      "[5] Training loss: 0.23324, Validation loss: 0.34111\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22977, Validation loss: 0.08812\n",
      "[2] Training loss: 0.22976, Validation loss: 0.08816\n",
      "[3] Training loss: 0.22974, Validation loss: 0.08822\n",
      "[4] Training loss: 0.22972, Validation loss: 0.08827\n",
      "[5] Training loss: 0.22970, Validation loss: 0.08832\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10998, Validation loss: 0.58753\n",
      "[2] Training loss: 0.10998, Validation loss: 0.58771\n",
      "[3] Training loss: 0.10996, Validation loss: 0.58789\n",
      "[4] Training loss: 0.10991, Validation loss: 0.58807\n",
      "[5] Training loss: 0.10985, Validation loss: 0.58824\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21261, Validation loss: 0.47636\n",
      "[2] Training loss: 0.21261, Validation loss: 0.47631\n",
      "[3] Training loss: 0.21255, Validation loss: 0.47647\n",
      "[4] Training loss: 0.21246, Validation loss: 0.47678\n",
      "[5] Training loss: 0.21234, Validation loss: 0.47719\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22153, Validation loss: 0.14973\n",
      "[2] Training loss: 0.22158, Validation loss: 0.14965\n",
      "[3] Training loss: 0.22160, Validation loss: 0.14963\n",
      "[4] Training loss: 0.22159, Validation loss: 0.14966\n",
      "[5] Training loss: 0.22155, Validation loss: 0.14973\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23325, Validation loss: 0.34136\n",
      "[2] Training loss: 0.23326, Validation loss: 0.34149\n",
      "[3] Training loss: 0.23326, Validation loss: 0.34160\n",
      "[4] Training loss: 0.23325, Validation loss: 0.34170\n",
      "[5] Training loss: 0.23323, Validation loss: 0.34178\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22976, Validation loss: 0.08821\n",
      "[2] Training loss: 0.22975, Validation loss: 0.08825\n",
      "[3] Training loss: 0.22973, Validation loss: 0.08830\n",
      "[4] Training loss: 0.22971, Validation loss: 0.08837\n",
      "[5] Training loss: 0.22969, Validation loss: 0.08844\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10996, Validation loss: 0.58656\n",
      "[2] Training loss: 0.10996, Validation loss: 0.58674\n",
      "[3] Training loss: 0.10993, Validation loss: 0.58692\n",
      "[4] Training loss: 0.10989, Validation loss: 0.58709\n",
      "[5] Training loss: 0.10982, Validation loss: 0.58727\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21247, Validation loss: 0.47756\n",
      "[2] Training loss: 0.21247, Validation loss: 0.47728\n",
      "[3] Training loss: 0.21243, Validation loss: 0.47715\n",
      "[4] Training loss: 0.21237, Validation loss: 0.47714\n",
      "[5] Training loss: 0.21229, Validation loss: 0.47725\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22150, Validation loss: 0.14969\n",
      "[2] Training loss: 0.22155, Validation loss: 0.14962\n",
      "[3] Training loss: 0.22157, Validation loss: 0.14960\n",
      "[4] Training loss: 0.22155, Validation loss: 0.14964\n",
      "[5] Training loss: 0.22151, Validation loss: 0.14971\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23319, Validation loss: 0.34105\n",
      "[2] Training loss: 0.23320, Validation loss: 0.34112\n",
      "[3] Training loss: 0.23319, Validation loss: 0.34118\n",
      "[4] Training loss: 0.23318, Validation loss: 0.34123\n",
      "[5] Training loss: 0.23315, Validation loss: 0.34126\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22970, Validation loss: 0.08776\n",
      "[2] Training loss: 0.22969, Validation loss: 0.08779\n",
      "[3] Training loss: 0.22967, Validation loss: 0.08783\n",
      "[4] Training loss: 0.22965, Validation loss: 0.08788\n",
      "[5] Training loss: 0.22963, Validation loss: 0.08794\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10987, Validation loss: 0.58758\n",
      "[2] Training loss: 0.10987, Validation loss: 0.58778\n",
      "[3] Training loss: 0.10985, Validation loss: 0.58797\n",
      "[4] Training loss: 0.10980, Validation loss: 0.58815\n",
      "[5] Training loss: 0.10973, Validation loss: 0.58834\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21262, Validation loss: 0.47570\n",
      "[2] Training loss: 0.21261, Validation loss: 0.47572\n",
      "[3] Training loss: 0.21253, Validation loss: 0.47598\n",
      "[4] Training loss: 0.21241, Validation loss: 0.47638\n",
      "[5] Training loss: 0.21229, Validation loss: 0.47687\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22147, Validation loss: 0.14963\n",
      "[2] Training loss: 0.22153, Validation loss: 0.14954\n",
      "[3] Training loss: 0.22155, Validation loss: 0.14951\n",
      "[4] Training loss: 0.22154, Validation loss: 0.14954\n",
      "[5] Training loss: 0.22150, Validation loss: 0.14961\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23319, Validation loss: 0.34153\n",
      "[2] Training loss: 0.23321, Validation loss: 0.34166\n",
      "[3] Training loss: 0.23320, Validation loss: 0.34178\n",
      "[4] Training loss: 0.23319, Validation loss: 0.34188\n",
      "[5] Training loss: 0.23317, Validation loss: 0.34196\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22969, Validation loss: 0.08813\n",
      "[2] Training loss: 0.22968, Validation loss: 0.08819\n",
      "[3] Training loss: 0.22967, Validation loss: 0.08825\n",
      "[4] Training loss: 0.22965, Validation loss: 0.08833\n",
      "[5] Training loss: 0.22962, Validation loss: 0.08841\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10986, Validation loss: 0.58598\n",
      "[2] Training loss: 0.10986, Validation loss: 0.58614\n",
      "[3] Training loss: 0.10984, Validation loss: 0.58631\n",
      "[4] Training loss: 0.10979, Validation loss: 0.58647\n",
      "[5] Training loss: 0.10972, Validation loss: 0.58664\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21237, Validation loss: 0.47774\n",
      "[2] Training loss: 0.21237, Validation loss: 0.47742\n",
      "[3] Training loss: 0.21234, Validation loss: 0.47727\n",
      "[4] Training loss: 0.21229, Validation loss: 0.47724\n",
      "[5] Training loss: 0.21221, Validation loss: 0.47733\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22144, Validation loss: 0.14957\n",
      "[2] Training loss: 0.22149, Validation loss: 0.14950\n",
      "[3] Training loss: 0.22151, Validation loss: 0.14949\n",
      "[4] Training loss: 0.22149, Validation loss: 0.14953\n",
      "[5] Training loss: 0.22145, Validation loss: 0.14961\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23313, Validation loss: 0.34129\n",
      "[2] Training loss: 0.23313, Validation loss: 0.34136\n",
      "[3] Training loss: 0.23312, Validation loss: 0.34142\n",
      "[4] Training loss: 0.23310, Validation loss: 0.34147\n",
      "[5] Training loss: 0.23308, Validation loss: 0.34151\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22963, Validation loss: 0.08755\n",
      "[2] Training loss: 0.22961, Validation loss: 0.08757\n",
      "[3] Training loss: 0.22960, Validation loss: 0.08760\n",
      "[4] Training loss: 0.22958, Validation loss: 0.08765\n",
      "[5] Training loss: 0.22956, Validation loss: 0.08771\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10976, Validation loss: 0.58722\n",
      "[2] Training loss: 0.10976, Validation loss: 0.58742\n",
      "[3] Training loss: 0.10973, Validation loss: 0.58762\n",
      "[4] Training loss: 0.10969, Validation loss: 0.58781\n",
      "[5] Training loss: 0.10962, Validation loss: 0.58799\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21259, Validation loss: 0.47557\n",
      "[2] Training loss: 0.21258, Validation loss: 0.47559\n",
      "[3] Training loss: 0.21250, Validation loss: 0.47585\n",
      "[4] Training loss: 0.21239, Validation loss: 0.47628\n",
      "[5] Training loss: 0.21226, Validation loss: 0.47679\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22140, Validation loss: 0.14954\n",
      "[2] Training loss: 0.22145, Validation loss: 0.14945\n",
      "[3] Training loss: 0.22148, Validation loss: 0.14942\n",
      "[4] Training loss: 0.22147, Validation loss: 0.14945\n",
      "[5] Training loss: 0.22143, Validation loss: 0.14951\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23312, Validation loss: 0.34178\n",
      "[2] Training loss: 0.23313, Validation loss: 0.34192\n",
      "[3] Training loss: 0.23313, Validation loss: 0.34204\n",
      "[4] Training loss: 0.23312, Validation loss: 0.34214\n",
      "[5] Training loss: 0.23310, Validation loss: 0.34222\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22962, Validation loss: 0.08806\n",
      "[2] Training loss: 0.22961, Validation loss: 0.08812\n",
      "[3] Training loss: 0.22959, Validation loss: 0.08819\n",
      "[4] Training loss: 0.22958, Validation loss: 0.08827\n",
      "[5] Training loss: 0.22955, Validation loss: 0.08836\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10976, Validation loss: 0.58546\n",
      "[2] Training loss: 0.10976, Validation loss: 0.58562\n",
      "[3] Training loss: 0.10974, Validation loss: 0.58578\n",
      "[4] Training loss: 0.10969, Validation loss: 0.58594\n",
      "[5] Training loss: 0.10962, Validation loss: 0.58611\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21232, Validation loss: 0.47780\n",
      "[2] Training loss: 0.21232, Validation loss: 0.47752\n",
      "[3] Training loss: 0.21229, Validation loss: 0.47740\n",
      "[4] Training loss: 0.21223, Validation loss: 0.47742\n",
      "[5] Training loss: 0.21215, Validation loss: 0.47757\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22138, Validation loss: 0.14948\n",
      "[2] Training loss: 0.22143, Validation loss: 0.14941\n",
      "[3] Training loss: 0.22144, Validation loss: 0.14940\n",
      "[4] Training loss: 0.22143, Validation loss: 0.14943\n",
      "[5] Training loss: 0.22138, Validation loss: 0.14952\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23307, Validation loss: 0.34174\n",
      "[2] Training loss: 0.23307, Validation loss: 0.34181\n",
      "[3] Training loss: 0.23306, Validation loss: 0.34187\n",
      "[4] Training loss: 0.23305, Validation loss: 0.34192\n",
      "[5] Training loss: 0.23302, Validation loss: 0.34195\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22956, Validation loss: 0.08740\n",
      "[2] Training loss: 0.22955, Validation loss: 0.08742\n",
      "[3] Training loss: 0.22953, Validation loss: 0.08745\n",
      "[4] Training loss: 0.22951, Validation loss: 0.08750\n",
      "[5] Training loss: 0.22949, Validation loss: 0.08756\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10967, Validation loss: 0.58672\n",
      "[2] Training loss: 0.10967, Validation loss: 0.58693\n",
      "[3] Training loss: 0.10965, Validation loss: 0.58713\n",
      "[4] Training loss: 0.10960, Validation loss: 0.58733\n",
      "[5] Training loss: 0.10953, Validation loss: 0.58752\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21248, Validation loss: 0.47568\n",
      "[2] Training loss: 0.21248, Validation loss: 0.47561\n",
      "[3] Training loss: 0.21241, Validation loss: 0.47579\n",
      "[4] Training loss: 0.21231, Validation loss: 0.47614\n",
      "[5] Training loss: 0.21220, Validation loss: 0.47659\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22133, Validation loss: 0.14944\n",
      "[2] Training loss: 0.22138, Validation loss: 0.14936\n",
      "[3] Training loss: 0.22140, Validation loss: 0.14933\n",
      "[4] Training loss: 0.22139, Validation loss: 0.14936\n",
      "[5] Training loss: 0.22136, Validation loss: 0.14943\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23305, Validation loss: 0.34195\n",
      "[2] Training loss: 0.23306, Validation loss: 0.34208\n",
      "[3] Training loss: 0.23306, Validation loss: 0.34220\n",
      "[4] Training loss: 0.23304, Validation loss: 0.34229\n",
      "[5] Training loss: 0.23302, Validation loss: 0.34237\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22955, Validation loss: 0.08778\n",
      "[2] Training loss: 0.22954, Validation loss: 0.08784\n",
      "[3] Training loss: 0.22952, Validation loss: 0.08791\n",
      "[4] Training loss: 0.22950, Validation loss: 0.08799\n",
      "[5] Training loss: 0.22948, Validation loss: 0.08807\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10965, Validation loss: 0.58534\n",
      "[2] Training loss: 0.10965, Validation loss: 0.58550\n",
      "[3] Training loss: 0.10962, Validation loss: 0.58566\n",
      "[4] Training loss: 0.10957, Validation loss: 0.58583\n",
      "[5] Training loss: 0.10951, Validation loss: 0.58599\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21230, Validation loss: 0.47730\n",
      "[2] Training loss: 0.21229, Validation loss: 0.47702\n",
      "[3] Training loss: 0.21226, Validation loss: 0.47690\n",
      "[4] Training loss: 0.21220, Validation loss: 0.47692\n",
      "[5] Training loss: 0.21211, Validation loss: 0.47706\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22130, Validation loss: 0.14939\n",
      "[2] Training loss: 0.22135, Validation loss: 0.14932\n",
      "[3] Training loss: 0.22136, Validation loss: 0.14931\n",
      "[4] Training loss: 0.22135, Validation loss: 0.14934\n",
      "[5] Training loss: 0.22131, Validation loss: 0.14942\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23299, Validation loss: 0.34179\n",
      "[2] Training loss: 0.23299, Validation loss: 0.34187\n",
      "[3] Training loss: 0.23299, Validation loss: 0.34193\n",
      "[4] Training loss: 0.23297, Validation loss: 0.34198\n",
      "[5] Training loss: 0.23294, Validation loss: 0.34202\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22949, Validation loss: 0.08753\n",
      "[2] Training loss: 0.22948, Validation loss: 0.08756\n",
      "[3] Training loss: 0.22946, Validation loss: 0.08760\n",
      "[4] Training loss: 0.22944, Validation loss: 0.08766\n",
      "[5] Training loss: 0.22942, Validation loss: 0.08773\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10957, Validation loss: 0.58616\n",
      "[2] Training loss: 0.10957, Validation loss: 0.58636\n",
      "[3] Training loss: 0.10955, Validation loss: 0.58655\n",
      "[4] Training loss: 0.10950, Validation loss: 0.58674\n",
      "[5] Training loss: 0.10943, Validation loss: 0.58693\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21244, Validation loss: 0.47554\n",
      "[2] Training loss: 0.21243, Validation loss: 0.47557\n",
      "[3] Training loss: 0.21235, Validation loss: 0.47587\n",
      "[4] Training loss: 0.21224, Validation loss: 0.47632\n",
      "[5] Training loss: 0.21212, Validation loss: 0.47684\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22126, Validation loss: 0.14935\n",
      "[2] Training loss: 0.22132, Validation loss: 0.14927\n",
      "[3] Training loss: 0.22134, Validation loss: 0.14924\n",
      "[4] Training loss: 0.22133, Validation loss: 0.14927\n",
      "[5] Training loss: 0.22130, Validation loss: 0.14934\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23299, Validation loss: 0.34226\n",
      "[2] Training loss: 0.23300, Validation loss: 0.34240\n",
      "[3] Training loss: 0.23300, Validation loss: 0.34251\n",
      "[4] Training loss: 0.23299, Validation loss: 0.34260\n",
      "[5] Training loss: 0.23297, Validation loss: 0.34268\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22948, Validation loss: 0.08773\n",
      "[2] Training loss: 0.22947, Validation loss: 0.08778\n",
      "[3] Training loss: 0.22945, Validation loss: 0.08785\n",
      "[4] Training loss: 0.22943, Validation loss: 0.08792\n",
      "[5] Training loss: 0.22941, Validation loss: 0.08800\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10957, Validation loss: 0.58498\n",
      "[2] Training loss: 0.10957, Validation loss: 0.58515\n",
      "[3] Training loss: 0.10955, Validation loss: 0.58532\n",
      "[4] Training loss: 0.10950, Validation loss: 0.58549\n",
      "[5] Training loss: 0.10943, Validation loss: 0.58566\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21221, Validation loss: 0.47760\n",
      "[2] Training loss: 0.21221, Validation loss: 0.47725\n",
      "[3] Training loss: 0.21218, Validation loss: 0.47706\n",
      "[4] Training loss: 0.21212, Validation loss: 0.47701\n",
      "[5] Training loss: 0.21205, Validation loss: 0.47707\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22124, Validation loss: 0.14929\n",
      "[2] Training loss: 0.22129, Validation loss: 0.14922\n",
      "[3] Training loss: 0.22130, Validation loss: 0.14920\n",
      "[4] Training loss: 0.22129, Validation loss: 0.14924\n",
      "[5] Training loss: 0.22124, Validation loss: 0.14932\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23293, Validation loss: 0.34190\n",
      "[2] Training loss: 0.23293, Validation loss: 0.34197\n",
      "[3] Training loss: 0.23292, Validation loss: 0.34203\n",
      "[4] Training loss: 0.23290, Validation loss: 0.34208\n",
      "[5] Training loss: 0.23288, Validation loss: 0.34212\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22941, Validation loss: 0.08752\n",
      "[2] Training loss: 0.22940, Validation loss: 0.08755\n",
      "[3] Training loss: 0.22938, Validation loss: 0.08760\n",
      "[4] Training loss: 0.22936, Validation loss: 0.08766\n",
      "[5] Training loss: 0.22934, Validation loss: 0.08773\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10946, Validation loss: 0.58578\n",
      "[2] Training loss: 0.10946, Validation loss: 0.58596\n",
      "[3] Training loss: 0.10943, Validation loss: 0.58615\n",
      "[4] Training loss: 0.10939, Validation loss: 0.58632\n",
      "[5] Training loss: 0.10932, Validation loss: 0.58650\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21243, Validation loss: 0.47529\n",
      "[2] Training loss: 0.21242, Validation loss: 0.47535\n",
      "[3] Training loss: 0.21234, Validation loss: 0.47567\n",
      "[4] Training loss: 0.21223, Validation loss: 0.47621\n",
      "[5] Training loss: 0.21211, Validation loss: 0.47684\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22118, Validation loss: 0.14930\n",
      "[2] Training loss: 0.22123, Validation loss: 0.14922\n",
      "[3] Training loss: 0.22126, Validation loss: 0.14920\n",
      "[4] Training loss: 0.22125, Validation loss: 0.14923\n",
      "[5] Training loss: 0.22121, Validation loss: 0.14930\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23291, Validation loss: 0.34260\n",
      "[2] Training loss: 0.23292, Validation loss: 0.34274\n",
      "[3] Training loss: 0.23292, Validation loss: 0.34286\n",
      "[4] Training loss: 0.23291, Validation loss: 0.34295\n",
      "[5] Training loss: 0.23289, Validation loss: 0.34302\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22942, Validation loss: 0.08718\n",
      "[2] Training loss: 0.22940, Validation loss: 0.08722\n",
      "[3] Training loss: 0.22939, Validation loss: 0.08727\n",
      "[4] Training loss: 0.22937, Validation loss: 0.08734\n",
      "[5] Training loss: 0.22935, Validation loss: 0.08741\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10948, Validation loss: 0.58490\n",
      "[2] Training loss: 0.10948, Validation loss: 0.58509\n",
      "[3] Training loss: 0.10945, Validation loss: 0.58527\n",
      "[4] Training loss: 0.10940, Validation loss: 0.58546\n",
      "[5] Training loss: 0.10933, Validation loss: 0.58564\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21217, Validation loss: 0.47794\n",
      "[2] Training loss: 0.21217, Validation loss: 0.47755\n",
      "[3] Training loss: 0.21214, Validation loss: 0.47734\n",
      "[4] Training loss: 0.21207, Validation loss: 0.47727\n",
      "[5] Training loss: 0.21199, Validation loss: 0.47732\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22117, Validation loss: 0.14920\n",
      "[2] Training loss: 0.22122, Validation loss: 0.14913\n",
      "[3] Training loss: 0.22123, Validation loss: 0.14911\n",
      "[4] Training loss: 0.22122, Validation loss: 0.14914\n",
      "[5] Training loss: 0.22118, Validation loss: 0.14922\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23287, Validation loss: 0.34202\n",
      "[2] Training loss: 0.23287, Validation loss: 0.34209\n",
      "[3] Training loss: 0.23286, Validation loss: 0.34215\n",
      "[4] Training loss: 0.23285, Validation loss: 0.34220\n",
      "[5] Training loss: 0.23282, Validation loss: 0.34223\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22934, Validation loss: 0.08707\n",
      "[2] Training loss: 0.22933, Validation loss: 0.08711\n",
      "[3] Training loss: 0.22931, Validation loss: 0.08716\n",
      "[4] Training loss: 0.22929, Validation loss: 0.08722\n",
      "[5] Training loss: 0.22927, Validation loss: 0.08730\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10938, Validation loss: 0.58603\n",
      "[2] Training loss: 0.10938, Validation loss: 0.58622\n",
      "[3] Training loss: 0.10935, Validation loss: 0.58640\n",
      "[4] Training loss: 0.10931, Validation loss: 0.58658\n",
      "[5] Training loss: 0.10924, Validation loss: 0.58675\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21233, Validation loss: 0.47525\n",
      "[2] Training loss: 0.21232, Validation loss: 0.47528\n",
      "[3] Training loss: 0.21224, Validation loss: 0.47556\n",
      "[4] Training loss: 0.21213, Validation loss: 0.47605\n",
      "[5] Training loss: 0.21201, Validation loss: 0.47665\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22112, Validation loss: 0.14917\n",
      "[2] Training loss: 0.22118, Validation loss: 0.14908\n",
      "[3] Training loss: 0.22120, Validation loss: 0.14906\n",
      "[4] Training loss: 0.22119, Validation loss: 0.14908\n",
      "[5] Training loss: 0.22115, Validation loss: 0.14915\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23286, Validation loss: 0.34270\n",
      "[2] Training loss: 0.23287, Validation loss: 0.34285\n",
      "[3] Training loss: 0.23287, Validation loss: 0.34297\n",
      "[4] Training loss: 0.23286, Validation loss: 0.34308\n",
      "[5] Training loss: 0.23284, Validation loss: 0.34316\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22935, Validation loss: 0.08755\n",
      "[2] Training loss: 0.22934, Validation loss: 0.08761\n",
      "[3] Training loss: 0.22932, Validation loss: 0.08768\n",
      "[4] Training loss: 0.22930, Validation loss: 0.08776\n",
      "[5] Training loss: 0.22928, Validation loss: 0.08785\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10939, Validation loss: 0.58426\n",
      "[2] Training loss: 0.10939, Validation loss: 0.58442\n",
      "[3] Training loss: 0.10936, Validation loss: 0.58459\n",
      "[4] Training loss: 0.10932, Validation loss: 0.58476\n",
      "[5] Training loss: 0.10925, Validation loss: 0.58493\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21208, Validation loss: 0.47785\n",
      "[2] Training loss: 0.21208, Validation loss: 0.47751\n",
      "[3] Training loss: 0.21205, Validation loss: 0.47735\n",
      "[4] Training loss: 0.21199, Validation loss: 0.47733\n",
      "[5] Training loss: 0.21192, Validation loss: 0.47743\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22109, Validation loss: 0.14912\n",
      "[2] Training loss: 0.22114, Validation loss: 0.14906\n",
      "[3] Training loss: 0.22115, Validation loss: 0.14905\n",
      "[4] Training loss: 0.22114, Validation loss: 0.14909\n",
      "[5] Training loss: 0.22109, Validation loss: 0.14917\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23279, Validation loss: 0.34250\n",
      "[2] Training loss: 0.23280, Validation loss: 0.34257\n",
      "[3] Training loss: 0.23279, Validation loss: 0.34263\n",
      "[4] Training loss: 0.23277, Validation loss: 0.34268\n",
      "[5] Training loss: 0.23274, Validation loss: 0.34271\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22928, Validation loss: 0.08693\n",
      "[2] Training loss: 0.22926, Validation loss: 0.08695\n",
      "[3] Training loss: 0.22924, Validation loss: 0.08699\n",
      "[4] Training loss: 0.22922, Validation loss: 0.08704\n",
      "[5] Training loss: 0.22920, Validation loss: 0.08710\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10928, Validation loss: 0.58559\n",
      "[2] Training loss: 0.10928, Validation loss: 0.58579\n",
      "[3] Training loss: 0.10925, Validation loss: 0.58599\n",
      "[4] Training loss: 0.10921, Validation loss: 0.58618\n",
      "[5] Training loss: 0.10914, Validation loss: 0.58637\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21231, Validation loss: 0.47538\n",
      "[2] Training loss: 0.21230, Validation loss: 0.47538\n",
      "[3] Training loss: 0.21223, Validation loss: 0.47563\n",
      "[4] Training loss: 0.21212, Validation loss: 0.47606\n",
      "[5] Training loss: 0.21199, Validation loss: 0.47659\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22104, Validation loss: 0.14911\n",
      "[2] Training loss: 0.22109, Validation loss: 0.14902\n",
      "[3] Training loss: 0.22112, Validation loss: 0.14900\n",
      "[4] Training loss: 0.22111, Validation loss: 0.14902\n",
      "[5] Training loss: 0.22107, Validation loss: 0.14909\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23278, Validation loss: 0.34285\n",
      "[2] Training loss: 0.23280, Validation loss: 0.34299\n",
      "[3] Training loss: 0.23279, Validation loss: 0.34311\n",
      "[4] Training loss: 0.23278, Validation loss: 0.34320\n",
      "[5] Training loss: 0.23276, Validation loss: 0.34328\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22926, Validation loss: 0.08736\n",
      "[2] Training loss: 0.22925, Validation loss: 0.08742\n",
      "[3] Training loss: 0.22923, Validation loss: 0.08749\n",
      "[4] Training loss: 0.22921, Validation loss: 0.08757\n",
      "[5] Training loss: 0.22919, Validation loss: 0.08766\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10928, Validation loss: 0.58414\n",
      "[2] Training loss: 0.10928, Validation loss: 0.58430\n",
      "[3] Training loss: 0.10925, Validation loss: 0.58446\n",
      "[4] Training loss: 0.10921, Validation loss: 0.58463\n",
      "[5] Training loss: 0.10914, Validation loss: 0.58479\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21206, Validation loss: 0.47760\n",
      "[2] Training loss: 0.21206, Validation loss: 0.47729\n",
      "[3] Training loss: 0.21203, Validation loss: 0.47716\n",
      "[4] Training loss: 0.21196, Validation loss: 0.47717\n",
      "[5] Training loss: 0.21188, Validation loss: 0.47729\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22102, Validation loss: 0.14906\n",
      "[2] Training loss: 0.22107, Validation loss: 0.14899\n",
      "[3] Training loss: 0.22108, Validation loss: 0.14897\n",
      "[4] Training loss: 0.22106, Validation loss: 0.14901\n",
      "[5] Training loss: 0.22102, Validation loss: 0.14909\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23273, Validation loss: 0.34275\n",
      "[2] Training loss: 0.23274, Validation loss: 0.34284\n",
      "[3] Training loss: 0.23273, Validation loss: 0.34290\n",
      "[4] Training loss: 0.23271, Validation loss: 0.34295\n",
      "[5] Training loss: 0.23269, Validation loss: 0.34299\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22921, Validation loss: 0.08716\n",
      "[2] Training loss: 0.22919, Validation loss: 0.08719\n",
      "[3] Training loss: 0.22918, Validation loss: 0.08724\n",
      "[4] Training loss: 0.22916, Validation loss: 0.08730\n",
      "[5] Training loss: 0.22913, Validation loss: 0.08737\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10919, Validation loss: 0.58491\n",
      "[2] Training loss: 0.10919, Validation loss: 0.58511\n",
      "[3] Training loss: 0.10917, Validation loss: 0.58530\n",
      "[4] Training loss: 0.10912, Validation loss: 0.58549\n",
      "[5] Training loss: 0.10906, Validation loss: 0.58567\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21220, Validation loss: 0.47553\n",
      "[2] Training loss: 0.21219, Validation loss: 0.47555\n",
      "[3] Training loss: 0.21212, Validation loss: 0.47583\n",
      "[4] Training loss: 0.21202, Validation loss: 0.47631\n",
      "[5] Training loss: 0.21190, Validation loss: 0.47686\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22096, Validation loss: 0.14904\n",
      "[2] Training loss: 0.22102, Validation loss: 0.14896\n",
      "[3] Training loss: 0.22104, Validation loss: 0.14894\n",
      "[4] Training loss: 0.22103, Validation loss: 0.14896\n",
      "[5] Training loss: 0.22099, Validation loss: 0.14904\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23272, Validation loss: 0.34321\n",
      "[2] Training loss: 0.23273, Validation loss: 0.34335\n",
      "[3] Training loss: 0.23273, Validation loss: 0.34345\n",
      "[4] Training loss: 0.23271, Validation loss: 0.34354\n",
      "[5] Training loss: 0.23269, Validation loss: 0.34361\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22920, Validation loss: 0.08715\n",
      "[2] Training loss: 0.22918, Validation loss: 0.08720\n",
      "[3] Training loss: 0.22917, Validation loss: 0.08726\n",
      "[4] Training loss: 0.22915, Validation loss: 0.08733\n",
      "[5] Training loss: 0.22913, Validation loss: 0.08741\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10920, Validation loss: 0.58407\n",
      "[2] Training loss: 0.10920, Validation loss: 0.58425\n",
      "[3] Training loss: 0.10917, Validation loss: 0.58443\n",
      "[4] Training loss: 0.10912, Validation loss: 0.58461\n",
      "[5] Training loss: 0.10905, Validation loss: 0.58479\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21201, Validation loss: 0.47744\n",
      "[2] Training loss: 0.21201, Validation loss: 0.47705\n",
      "[3] Training loss: 0.21198, Validation loss: 0.47685\n",
      "[4] Training loss: 0.21192, Validation loss: 0.47681\n",
      "[5] Training loss: 0.21184, Validation loss: 0.47688\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22094, Validation loss: 0.14897\n",
      "[2] Training loss: 0.22099, Validation loss: 0.14890\n",
      "[3] Training loss: 0.22101, Validation loss: 0.14889\n",
      "[4] Training loss: 0.22099, Validation loss: 0.14892\n",
      "[5] Training loss: 0.22095, Validation loss: 0.14900\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23266, Validation loss: 0.34274\n",
      "[2] Training loss: 0.23266, Validation loss: 0.34281\n",
      "[3] Training loss: 0.23266, Validation loss: 0.34287\n",
      "[4] Training loss: 0.23264, Validation loss: 0.34292\n",
      "[5] Training loss: 0.23261, Validation loss: 0.34295\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22913, Validation loss: 0.08701\n",
      "[2] Training loss: 0.22911, Validation loss: 0.08704\n",
      "[3] Training loss: 0.22910, Validation loss: 0.08709\n",
      "[4] Training loss: 0.22908, Validation loss: 0.08716\n",
      "[5] Training loss: 0.22905, Validation loss: 0.08723\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10909, Validation loss: 0.58492\n",
      "[2] Training loss: 0.10909, Validation loss: 0.58511\n",
      "[3] Training loss: 0.10907, Validation loss: 0.58529\n",
      "[4] Training loss: 0.10902, Validation loss: 0.58547\n",
      "[5] Training loss: 0.10896, Validation loss: 0.58564\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21219, Validation loss: 0.47507\n",
      "[2] Training loss: 0.21218, Validation loss: 0.47512\n",
      "[3] Training loss: 0.21210, Validation loss: 0.47540\n",
      "[4] Training loss: 0.21199, Validation loss: 0.47588\n",
      "[5] Training loss: 0.21186, Validation loss: 0.47646\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22089, Validation loss: 0.14896\n",
      "[2] Training loss: 0.22095, Validation loss: 0.14888\n",
      "[3] Training loss: 0.22097, Validation loss: 0.14886\n",
      "[4] Training loss: 0.22096, Validation loss: 0.14888\n",
      "[5] Training loss: 0.22092, Validation loss: 0.14896\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23265, Validation loss: 0.34335\n",
      "[2] Training loss: 0.23266, Validation loss: 0.34349\n",
      "[3] Training loss: 0.23266, Validation loss: 0.34361\n",
      "[4] Training loss: 0.23265, Validation loss: 0.34371\n",
      "[5] Training loss: 0.23263, Validation loss: 0.34379\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22912, Validation loss: 0.08717\n",
      "[2] Training loss: 0.22911, Validation loss: 0.08721\n",
      "[3] Training loss: 0.22910, Validation loss: 0.08728\n",
      "[4] Training loss: 0.22908, Validation loss: 0.08735\n",
      "[5] Training loss: 0.22905, Validation loss: 0.08743\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10910, Validation loss: 0.58372\n",
      "[2] Training loss: 0.10910, Validation loss: 0.58389\n",
      "[3] Training loss: 0.10908, Validation loss: 0.58406\n",
      "[4] Training loss: 0.10903, Validation loss: 0.58424\n",
      "[5] Training loss: 0.10896, Validation loss: 0.58441\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21195, Validation loss: 0.47749\n",
      "[2] Training loss: 0.21195, Validation loss: 0.47713\n",
      "[3] Training loss: 0.21192, Validation loss: 0.47694\n",
      "[4] Training loss: 0.21186, Validation loss: 0.47690\n",
      "[5] Training loss: 0.21178, Validation loss: 0.47701\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22087, Validation loss: 0.14890\n",
      "[2] Training loss: 0.22092, Validation loss: 0.14883\n",
      "[3] Training loss: 0.22093, Validation loss: 0.14882\n",
      "[4] Training loss: 0.22092, Validation loss: 0.14885\n",
      "[5] Training loss: 0.22087, Validation loss: 0.14893\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23260, Validation loss: 0.34304\n",
      "[2] Training loss: 0.23260, Validation loss: 0.34312\n",
      "[3] Training loss: 0.23259, Validation loss: 0.34318\n",
      "[4] Training loss: 0.23258, Validation loss: 0.34323\n",
      "[5] Training loss: 0.23255, Validation loss: 0.34326\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22906, Validation loss: 0.08709\n",
      "[2] Training loss: 0.22904, Validation loss: 0.08713\n",
      "[3] Training loss: 0.22903, Validation loss: 0.08718\n",
      "[4] Training loss: 0.22901, Validation loss: 0.08724\n",
      "[5] Training loss: 0.22899, Validation loss: 0.08732\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10900, Validation loss: 0.58440\n",
      "[2] Training loss: 0.10901, Validation loss: 0.58459\n",
      "[3] Training loss: 0.10898, Validation loss: 0.58477\n",
      "[4] Training loss: 0.10893, Validation loss: 0.58495\n",
      "[5] Training loss: 0.10887, Validation loss: 0.58512\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21213, Validation loss: 0.47521\n",
      "[2] Training loss: 0.21212, Validation loss: 0.47526\n",
      "[3] Training loss: 0.21204, Validation loss: 0.47563\n",
      "[4] Training loss: 0.21193, Validation loss: 0.47617\n",
      "[5] Training loss: 0.21180, Validation loss: 0.47682\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22082, Validation loss: 0.14889\n",
      "[2] Training loss: 0.22087, Validation loss: 0.14881\n",
      "[3] Training loss: 0.22090, Validation loss: 0.14878\n",
      "[4] Training loss: 0.22089, Validation loss: 0.14881\n",
      "[5] Training loss: 0.22085, Validation loss: 0.14888\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23259, Validation loss: 0.34372\n",
      "[2] Training loss: 0.23260, Validation loss: 0.34386\n",
      "[3] Training loss: 0.23260, Validation loss: 0.34397\n",
      "[4] Training loss: 0.23259, Validation loss: 0.34407\n",
      "[5] Training loss: 0.23257, Validation loss: 0.34414\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22906, Validation loss: 0.08704\n",
      "[2] Training loss: 0.22904, Validation loss: 0.08708\n",
      "[3] Training loss: 0.22903, Validation loss: 0.08714\n",
      "[4] Training loss: 0.22901, Validation loss: 0.08722\n",
      "[5] Training loss: 0.22898, Validation loss: 0.08730\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10902, Validation loss: 0.58345\n",
      "[2] Training loss: 0.10902, Validation loss: 0.58362\n",
      "[3] Training loss: 0.10900, Validation loss: 0.58381\n",
      "[4] Training loss: 0.10895, Validation loss: 0.58399\n",
      "[5] Training loss: 0.10888, Validation loss: 0.58416\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21189, Validation loss: 0.47783\n",
      "[2] Training loss: 0.21189, Validation loss: 0.47741\n",
      "[3] Training loss: 0.21186, Validation loss: 0.47716\n",
      "[4] Training loss: 0.21180, Validation loss: 0.47706\n",
      "[5] Training loss: 0.21173, Validation loss: 0.47708\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22079, Validation loss: 0.14882\n",
      "[2] Training loss: 0.22084, Validation loss: 0.14875\n",
      "[3] Training loss: 0.22086, Validation loss: 0.14874\n",
      "[4] Training loss: 0.22084, Validation loss: 0.14878\n",
      "[5] Training loss: 0.22080, Validation loss: 0.14886\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23253, Validation loss: 0.34319\n",
      "[2] Training loss: 0.23253, Validation loss: 0.34326\n",
      "[3] Training loss: 0.23252, Validation loss: 0.34332\n",
      "[4] Training loss: 0.23250, Validation loss: 0.34336\n",
      "[5] Training loss: 0.23248, Validation loss: 0.34339\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22898, Validation loss: 0.08682\n",
      "[2] Training loss: 0.22896, Validation loss: 0.08685\n",
      "[3] Training loss: 0.22894, Validation loss: 0.08690\n",
      "[4] Training loss: 0.22893, Validation loss: 0.08696\n",
      "[5] Training loss: 0.22890, Validation loss: 0.08703\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10890, Validation loss: 0.58444\n",
      "[2] Training loss: 0.10890, Validation loss: 0.58463\n",
      "[3] Training loss: 0.10888, Validation loss: 0.58481\n",
      "[4] Training loss: 0.10883, Validation loss: 0.58498\n",
      "[5] Training loss: 0.10877, Validation loss: 0.58515\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21212, Validation loss: 0.47502\n",
      "[2] Training loss: 0.21211, Validation loss: 0.47508\n",
      "[3] Training loss: 0.21203, Validation loss: 0.47547\n",
      "[4] Training loss: 0.21192, Validation loss: 0.47607\n",
      "[5] Training loss: 0.21178, Validation loss: 0.47680\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22074, Validation loss: 0.14883\n",
      "[2] Training loss: 0.22079, Validation loss: 0.14875\n",
      "[3] Training loss: 0.22082, Validation loss: 0.14872\n",
      "[4] Training loss: 0.22081, Validation loss: 0.14875\n",
      "[5] Training loss: 0.22077, Validation loss: 0.14882\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23252, Validation loss: 0.34388\n",
      "[2] Training loss: 0.23253, Validation loss: 0.34403\n",
      "[3] Training loss: 0.23253, Validation loss: 0.34415\n",
      "[4] Training loss: 0.23252, Validation loss: 0.34424\n",
      "[5] Training loss: 0.23250, Validation loss: 0.34431\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22898, Validation loss: 0.08683\n",
      "[2] Training loss: 0.22897, Validation loss: 0.08688\n",
      "[3] Training loss: 0.22895, Validation loss: 0.08694\n",
      "[4] Training loss: 0.22893, Validation loss: 0.08702\n",
      "[5] Training loss: 0.22891, Validation loss: 0.08710\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10894, Validation loss: 0.58329\n",
      "[2] Training loss: 0.10894, Validation loss: 0.58347\n",
      "[3] Training loss: 0.10891, Validation loss: 0.58365\n",
      "[4] Training loss: 0.10886, Validation loss: 0.58383\n",
      "[5] Training loss: 0.10879, Validation loss: 0.58401\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21183, Validation loss: 0.47824\n",
      "[2] Training loss: 0.21183, Validation loss: 0.47782\n",
      "[3] Training loss: 0.21180, Validation loss: 0.47756\n",
      "[4] Training loss: 0.21174, Validation loss: 0.47747\n",
      "[5] Training loss: 0.21166, Validation loss: 0.47750\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22072, Validation loss: 0.14873\n",
      "[2] Training loss: 0.22078, Validation loss: 0.14866\n",
      "[3] Training loss: 0.22079, Validation loss: 0.14864\n",
      "[4] Training loss: 0.22078, Validation loss: 0.14868\n",
      "[5] Training loss: 0.22073, Validation loss: 0.14876\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23247, Validation loss: 0.34336\n",
      "[2] Training loss: 0.23247, Validation loss: 0.34343\n",
      "[3] Training loss: 0.23247, Validation loss: 0.34349\n",
      "[4] Training loss: 0.23245, Validation loss: 0.34353\n",
      "[5] Training loss: 0.23242, Validation loss: 0.34357\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22891, Validation loss: 0.08685\n",
      "[2] Training loss: 0.22890, Validation loss: 0.08689\n",
      "[3] Training loss: 0.22888, Validation loss: 0.08694\n",
      "[4] Training loss: 0.22886, Validation loss: 0.08701\n",
      "[5] Training loss: 0.22884, Validation loss: 0.08708\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10883, Validation loss: 0.58425\n",
      "[2] Training loss: 0.10883, Validation loss: 0.58443\n",
      "[3] Training loss: 0.10881, Validation loss: 0.58461\n",
      "[4] Training loss: 0.10876, Validation loss: 0.58479\n",
      "[5] Training loss: 0.10870, Validation loss: 0.58496\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21201, Validation loss: 0.47520\n",
      "[2] Training loss: 0.21201, Validation loss: 0.47522\n",
      "[3] Training loss: 0.21194, Validation loss: 0.47554\n",
      "[4] Training loss: 0.21184, Validation loss: 0.47610\n",
      "[5] Training loss: 0.21170, Validation loss: 0.47677\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22066, Validation loss: 0.14874\n",
      "[2] Training loss: 0.22072, Validation loss: 0.14866\n",
      "[3] Training loss: 0.22074, Validation loss: 0.14864\n",
      "[4] Training loss: 0.22073, Validation loss: 0.14866\n",
      "[5] Training loss: 0.22069, Validation loss: 0.14874\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23245, Validation loss: 0.34412\n",
      "[2] Training loss: 0.23246, Validation loss: 0.34428\n",
      "[3] Training loss: 0.23246, Validation loss: 0.34440\n",
      "[4] Training loss: 0.23245, Validation loss: 0.34450\n",
      "[5] Training loss: 0.23243, Validation loss: 0.34458\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22891, Validation loss: 0.08672\n",
      "[2] Training loss: 0.22890, Validation loss: 0.08676\n",
      "[3] Training loss: 0.22889, Validation loss: 0.08682\n",
      "[4] Training loss: 0.22887, Validation loss: 0.08689\n",
      "[5] Training loss: 0.22884, Validation loss: 0.08698\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10883, Validation loss: 0.58310\n",
      "[2] Training loss: 0.10883, Validation loss: 0.58327\n",
      "[3] Training loss: 0.10881, Validation loss: 0.58345\n",
      "[4] Training loss: 0.10876, Validation loss: 0.58363\n",
      "[5] Training loss: 0.10869, Validation loss: 0.58381\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21180, Validation loss: 0.47798\n",
      "[2] Training loss: 0.21180, Validation loss: 0.47761\n",
      "[3] Training loss: 0.21177, Validation loss: 0.47740\n",
      "[4] Training loss: 0.21170, Validation loss: 0.47736\n",
      "[5] Training loss: 0.21162, Validation loss: 0.47744\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22064, Validation loss: 0.14867\n",
      "[2] Training loss: 0.22069, Validation loss: 0.14860\n",
      "[3] Training loss: 0.22071, Validation loss: 0.14858\n",
      "[4] Training loss: 0.22069, Validation loss: 0.14862\n",
      "[5] Training loss: 0.22065, Validation loss: 0.14870\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23240, Validation loss: 0.34372\n",
      "[2] Training loss: 0.23241, Validation loss: 0.34380\n",
      "[3] Training loss: 0.23240, Validation loss: 0.34386\n",
      "[4] Training loss: 0.23238, Validation loss: 0.34391\n",
      "[5] Training loss: 0.23236, Validation loss: 0.34394\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22884, Validation loss: 0.08670\n",
      "[2] Training loss: 0.22883, Validation loss: 0.08673\n",
      "[3] Training loss: 0.22881, Validation loss: 0.08679\n",
      "[4] Training loss: 0.22879, Validation loss: 0.08685\n",
      "[5] Training loss: 0.22877, Validation loss: 0.08693\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10874, Validation loss: 0.58390\n",
      "[2] Training loss: 0.10874, Validation loss: 0.58409\n",
      "[3] Training loss: 0.10872, Validation loss: 0.58427\n",
      "[4] Training loss: 0.10867, Validation loss: 0.58445\n",
      "[5] Training loss: 0.10861, Validation loss: 0.58462\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21194, Validation loss: 0.47548\n",
      "[2] Training loss: 0.21193, Validation loss: 0.47551\n",
      "[3] Training loss: 0.21186, Validation loss: 0.47581\n",
      "[4] Training loss: 0.21176, Validation loss: 0.47631\n",
      "[5] Training loss: 0.21163, Validation loss: 0.47692\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22059, Validation loss: 0.14866\n",
      "[2] Training loss: 0.22065, Validation loss: 0.14858\n",
      "[3] Training loss: 0.22067, Validation loss: 0.14856\n",
      "[4] Training loss: 0.22066, Validation loss: 0.14859\n",
      "[5] Training loss: 0.22062, Validation loss: 0.14866\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23239, Validation loss: 0.34426\n",
      "[2] Training loss: 0.23240, Validation loss: 0.34441\n",
      "[3] Training loss: 0.23240, Validation loss: 0.34452\n",
      "[4] Training loss: 0.23238, Validation loss: 0.34461\n",
      "[5] Training loss: 0.23236, Validation loss: 0.34468\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22884, Validation loss: 0.08696\n",
      "[2] Training loss: 0.22883, Validation loss: 0.08701\n",
      "[3] Training loss: 0.22881, Validation loss: 0.08707\n",
      "[4] Training loss: 0.22879, Validation loss: 0.08715\n",
      "[5] Training loss: 0.22877, Validation loss: 0.08723\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10875, Validation loss: 0.58278\n",
      "[2] Training loss: 0.10875, Validation loss: 0.58296\n",
      "[3] Training loss: 0.10873, Validation loss: 0.58313\n",
      "[4] Training loss: 0.10868, Validation loss: 0.58331\n",
      "[5] Training loss: 0.10861, Validation loss: 0.58349\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21173, Validation loss: 0.47778\n",
      "[2] Training loss: 0.21173, Validation loss: 0.47737\n",
      "[3] Training loss: 0.21170, Validation loss: 0.47717\n",
      "[4] Training loss: 0.21164, Validation loss: 0.47714\n",
      "[5] Training loss: 0.21156, Validation loss: 0.47726\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22057, Validation loss: 0.14861\n",
      "[2] Training loss: 0.22062, Validation loss: 0.14854\n",
      "[3] Training loss: 0.22063, Validation loss: 0.14852\n",
      "[4] Training loss: 0.22062, Validation loss: 0.14856\n",
      "[5] Training loss: 0.22057, Validation loss: 0.14864\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23233, Validation loss: 0.34390\n",
      "[2] Training loss: 0.23233, Validation loss: 0.34398\n",
      "[3] Training loss: 0.23233, Validation loss: 0.34405\n",
      "[4] Training loss: 0.23231, Validation loss: 0.34409\n",
      "[5] Training loss: 0.23228, Validation loss: 0.34412\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22877, Validation loss: 0.08668\n",
      "[2] Training loss: 0.22876, Validation loss: 0.08671\n",
      "[3] Training loss: 0.22874, Validation loss: 0.08675\n",
      "[4] Training loss: 0.22872, Validation loss: 0.08681\n",
      "[5] Training loss: 0.22870, Validation loss: 0.08688\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10865, Validation loss: 0.58365\n",
      "[2] Training loss: 0.10865, Validation loss: 0.58384\n",
      "[3] Training loss: 0.10863, Validation loss: 0.58403\n",
      "[4] Training loss: 0.10858, Validation loss: 0.58421\n",
      "[5] Training loss: 0.10851, Validation loss: 0.58439\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21193, Validation loss: 0.47534\n",
      "[2] Training loss: 0.21192, Validation loss: 0.47537\n",
      "[3] Training loss: 0.21184, Validation loss: 0.47567\n",
      "[4] Training loss: 0.21173, Validation loss: 0.47622\n",
      "[5] Training loss: 0.21160, Validation loss: 0.47692\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22052, Validation loss: 0.14859\n",
      "[2] Training loss: 0.22058, Validation loss: 0.14851\n",
      "[3] Training loss: 0.22060, Validation loss: 0.14848\n",
      "[4] Training loss: 0.22059, Validation loss: 0.14851\n",
      "[5] Training loss: 0.22055, Validation loss: 0.14858\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23233, Validation loss: 0.34454\n",
      "[2] Training loss: 0.23234, Validation loss: 0.34469\n",
      "[3] Training loss: 0.23234, Validation loss: 0.34481\n",
      "[4] Training loss: 0.23233, Validation loss: 0.34491\n",
      "[5] Training loss: 0.23231, Validation loss: 0.34499\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22877, Validation loss: 0.08693\n",
      "[2] Training loss: 0.22876, Validation loss: 0.08698\n",
      "[3] Training loss: 0.22875, Validation loss: 0.08705\n",
      "[4] Training loss: 0.22873, Validation loss: 0.08712\n",
      "[5] Training loss: 0.22871, Validation loss: 0.08721\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10867, Validation loss: 0.58227\n",
      "[2] Training loss: 0.10867, Validation loss: 0.58244\n",
      "[3] Training loss: 0.10865, Validation loss: 0.58261\n",
      "[4] Training loss: 0.10860, Validation loss: 0.58279\n",
      "[5] Training loss: 0.10853, Validation loss: 0.58296\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21164, Validation loss: 0.47847\n",
      "[2] Training loss: 0.21164, Validation loss: 0.47806\n",
      "[3] Training loss: 0.21161, Validation loss: 0.47786\n",
      "[4] Training loss: 0.21156, Validation loss: 0.47778\n",
      "[5] Training loss: 0.21148, Validation loss: 0.47786\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22050, Validation loss: 0.14853\n",
      "[2] Training loss: 0.22055, Validation loss: 0.14846\n",
      "[3] Training loss: 0.22056, Validation loss: 0.14845\n",
      "[4] Training loss: 0.22054, Validation loss: 0.14849\n",
      "[5] Training loss: 0.22050, Validation loss: 0.14857\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23227, Validation loss: 0.34422\n",
      "[2] Training loss: 0.23227, Validation loss: 0.34430\n",
      "[3] Training loss: 0.23227, Validation loss: 0.34436\n",
      "[4] Training loss: 0.23225, Validation loss: 0.34440\n",
      "[5] Training loss: 0.23222, Validation loss: 0.34443\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22870, Validation loss: 0.08644\n",
      "[2] Training loss: 0.22869, Validation loss: 0.08647\n",
      "[3] Training loss: 0.22867, Validation loss: 0.08651\n",
      "[4] Training loss: 0.22865, Validation loss: 0.08656\n",
      "[5] Training loss: 0.22863, Validation loss: 0.08663\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10856, Validation loss: 0.58370\n",
      "[2] Training loss: 0.10856, Validation loss: 0.58390\n",
      "[3] Training loss: 0.10854, Validation loss: 0.58410\n",
      "[4] Training loss: 0.10849, Validation loss: 0.58429\n",
      "[5] Training loss: 0.10843, Validation loss: 0.58447\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21187, Validation loss: 0.47551\n",
      "[2] Training loss: 0.21186, Validation loss: 0.47554\n",
      "[3] Training loss: 0.21179, Validation loss: 0.47588\n",
      "[4] Training loss: 0.21167, Validation loss: 0.47647\n",
      "[5] Training loss: 0.21154, Validation loss: 0.47715\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22045, Validation loss: 0.14851\n",
      "[2] Training loss: 0.22050, Validation loss: 0.14842\n",
      "[3] Training loss: 0.22053, Validation loss: 0.14840\n",
      "[4] Training loss: 0.22052, Validation loss: 0.14842\n",
      "[5] Training loss: 0.22048, Validation loss: 0.14849\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23227, Validation loss: 0.34466\n",
      "[2] Training loss: 0.23228, Validation loss: 0.34480\n",
      "[3] Training loss: 0.23228, Validation loss: 0.34492\n",
      "[4] Training loss: 0.23227, Validation loss: 0.34501\n",
      "[5] Training loss: 0.23225, Validation loss: 0.34508\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22870, Validation loss: 0.08695\n",
      "[2] Training loss: 0.22869, Validation loss: 0.08701\n",
      "[3] Training loss: 0.22867, Validation loss: 0.08708\n",
      "[4] Training loss: 0.22865, Validation loss: 0.08717\n",
      "[5] Training loss: 0.22863, Validation loss: 0.08726\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10859, Validation loss: 0.58213\n",
      "[2] Training loss: 0.10859, Validation loss: 0.58229\n",
      "[3] Training loss: 0.10856, Validation loss: 0.58246\n",
      "[4] Training loss: 0.10852, Validation loss: 0.58262\n",
      "[5] Training loss: 0.10845, Validation loss: 0.58279\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21159, Validation loss: 0.47852\n",
      "[2] Training loss: 0.21159, Validation loss: 0.47815\n",
      "[3] Training loss: 0.21156, Validation loss: 0.47798\n",
      "[4] Training loss: 0.21150, Validation loss: 0.47794\n",
      "[5] Training loss: 0.21143, Validation loss: 0.47806\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22042, Validation loss: 0.14846\n",
      "[2] Training loss: 0.22047, Validation loss: 0.14839\n",
      "[3] Training loss: 0.22049, Validation loss: 0.14838\n",
      "[4] Training loss: 0.22047, Validation loss: 0.14842\n",
      "[5] Training loss: 0.22042, Validation loss: 0.14851\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23220, Validation loss: 0.34443\n",
      "[2] Training loss: 0.23221, Validation loss: 0.34452\n",
      "[3] Training loss: 0.23220, Validation loss: 0.34457\n",
      "[4] Training loss: 0.23218, Validation loss: 0.34462\n",
      "[5] Training loss: 0.23216, Validation loss: 0.34465\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22863, Validation loss: 0.08629\n",
      "[2] Training loss: 0.22861, Validation loss: 0.08631\n",
      "[3] Training loss: 0.22860, Validation loss: 0.08635\n",
      "[4] Training loss: 0.22858, Validation loss: 0.08640\n",
      "[5] Training loss: 0.22855, Validation loss: 0.08647\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10848, Validation loss: 0.58350\n",
      "[2] Training loss: 0.10848, Validation loss: 0.58370\n",
      "[3] Training loss: 0.10845, Validation loss: 0.58390\n",
      "[4] Training loss: 0.10840, Validation loss: 0.58409\n",
      "[5] Training loss: 0.10834, Validation loss: 0.58428\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21182, Validation loss: 0.47561\n",
      "[2] Training loss: 0.21182, Validation loss: 0.47558\n",
      "[3] Training loss: 0.21175, Validation loss: 0.47583\n",
      "[4] Training loss: 0.21165, Validation loss: 0.47633\n",
      "[5] Training loss: 0.21152, Validation loss: 0.47699\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22036, Validation loss: 0.14846\n",
      "[2] Training loss: 0.22042, Validation loss: 0.14837\n",
      "[3] Training loss: 0.22044, Validation loss: 0.14835\n",
      "[4] Training loss: 0.22043, Validation loss: 0.14837\n",
      "[5] Training loss: 0.22040, Validation loss: 0.14845\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23219, Validation loss: 0.34485\n",
      "[2] Training loss: 0.23220, Validation loss: 0.34500\n",
      "[3] Training loss: 0.23220, Validation loss: 0.34512\n",
      "[4] Training loss: 0.23219, Validation loss: 0.34521\n",
      "[5] Training loss: 0.23217, Validation loss: 0.34529\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22863, Validation loss: 0.08656\n",
      "[2] Training loss: 0.22861, Validation loss: 0.08662\n",
      "[3] Training loss: 0.22860, Validation loss: 0.08669\n",
      "[4] Training loss: 0.22858, Validation loss: 0.08677\n",
      "[5] Training loss: 0.22856, Validation loss: 0.08685\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10848, Validation loss: 0.58209\n",
      "[2] Training loss: 0.10848, Validation loss: 0.58226\n",
      "[3] Training loss: 0.10846, Validation loss: 0.58243\n",
      "[4] Training loss: 0.10841, Validation loss: 0.58259\n",
      "[5] Training loss: 0.10834, Validation loss: 0.58276\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21156, Validation loss: 0.47842\n",
      "[2] Training loss: 0.21156, Validation loss: 0.47805\n",
      "[3] Training loss: 0.21153, Validation loss: 0.47788\n",
      "[4] Training loss: 0.21147, Validation loss: 0.47790\n",
      "[5] Training loss: 0.21138, Validation loss: 0.47804\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22035, Validation loss: 0.14839\n",
      "[2] Training loss: 0.22039, Validation loss: 0.14833\n",
      "[3] Training loss: 0.22041, Validation loss: 0.14831\n",
      "[4] Training loss: 0.22039, Validation loss: 0.14835\n",
      "[5] Training loss: 0.22035, Validation loss: 0.14844\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23214, Validation loss: 0.34466\n",
      "[2] Training loss: 0.23214, Validation loss: 0.34474\n",
      "[3] Training loss: 0.23214, Validation loss: 0.34480\n",
      "[4] Training loss: 0.23212, Validation loss: 0.34485\n",
      "[5] Training loss: 0.23209, Validation loss: 0.34487\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22856, Validation loss: 0.08623\n",
      "[2] Training loss: 0.22855, Validation loss: 0.08627\n",
      "[3] Training loss: 0.22853, Validation loss: 0.08632\n",
      "[4] Training loss: 0.22851, Validation loss: 0.08639\n",
      "[5] Training loss: 0.22849, Validation loss: 0.08647\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10840, Validation loss: 0.58334\n",
      "[2] Training loss: 0.10840, Validation loss: 0.58354\n",
      "[3] Training loss: 0.10838, Validation loss: 0.58374\n",
      "[4] Training loss: 0.10833, Validation loss: 0.58393\n",
      "[5] Training loss: 0.10826, Validation loss: 0.58411\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21174, Validation loss: 0.47588\n",
      "[2] Training loss: 0.21173, Validation loss: 0.47588\n",
      "[3] Training loss: 0.21166, Validation loss: 0.47619\n",
      "[4] Training loss: 0.21154, Validation loss: 0.47676\n",
      "[5] Training loss: 0.21141, Validation loss: 0.47743\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22030, Validation loss: 0.14835\n",
      "[2] Training loss: 0.22036, Validation loss: 0.14826\n",
      "[3] Training loss: 0.22039, Validation loss: 0.14824\n",
      "[4] Training loss: 0.22038, Validation loss: 0.14826\n",
      "[5] Training loss: 0.22034, Validation loss: 0.14833\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23214, Validation loss: 0.34506\n",
      "[2] Training loss: 0.23216, Validation loss: 0.34520\n",
      "[3] Training loss: 0.23216, Validation loss: 0.34533\n",
      "[4] Training loss: 0.23214, Validation loss: 0.34542\n",
      "[5] Training loss: 0.23212, Validation loss: 0.34549\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22856, Validation loss: 0.08689\n",
      "[2] Training loss: 0.22854, Validation loss: 0.08695\n",
      "[3] Training loss: 0.22853, Validation loss: 0.08703\n",
      "[4] Training loss: 0.22851, Validation loss: 0.08712\n",
      "[5] Training loss: 0.22849, Validation loss: 0.08722\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10842, Validation loss: 0.58165\n",
      "[2] Training loss: 0.10842, Validation loss: 0.58181\n",
      "[3] Training loss: 0.10839, Validation loss: 0.58197\n",
      "[4] Training loss: 0.10834, Validation loss: 0.58213\n",
      "[5] Training loss: 0.10828, Validation loss: 0.58229\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21147, Validation loss: 0.47876\n",
      "[2] Training loss: 0.21147, Validation loss: 0.47835\n",
      "[3] Training loss: 0.21144, Validation loss: 0.47813\n",
      "[4] Training loss: 0.21139, Validation loss: 0.47808\n",
      "[5] Training loss: 0.21132, Validation loss: 0.47817\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22028, Validation loss: 0.14832\n",
      "[2] Training loss: 0.22032, Validation loss: 0.14825\n",
      "[3] Training loss: 0.22034, Validation loss: 0.14825\n",
      "[4] Training loss: 0.22032, Validation loss: 0.14829\n",
      "[5] Training loss: 0.22027, Validation loss: 0.14837\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23207, Validation loss: 0.34484\n",
      "[2] Training loss: 0.23207, Validation loss: 0.34492\n",
      "[3] Training loss: 0.23206, Validation loss: 0.34498\n",
      "[4] Training loss: 0.23204, Validation loss: 0.34502\n",
      "[5] Training loss: 0.23202, Validation loss: 0.34505\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22848, Validation loss: 0.08623\n",
      "[2] Training loss: 0.22847, Validation loss: 0.08625\n",
      "[3] Training loss: 0.22845, Validation loss: 0.08629\n",
      "[4] Training loss: 0.22843, Validation loss: 0.08634\n",
      "[5] Training loss: 0.22841, Validation loss: 0.08640\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10829, Validation loss: 0.58301\n",
      "[2] Training loss: 0.10830, Validation loss: 0.58321\n",
      "[3] Training loss: 0.10827, Validation loss: 0.58341\n",
      "[4] Training loss: 0.10822, Validation loss: 0.58360\n",
      "[5] Training loss: 0.10816, Validation loss: 0.58378\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21176, Validation loss: 0.47581\n",
      "[2] Training loss: 0.21174, Validation loss: 0.47596\n",
      "[3] Training loss: 0.21166, Validation loss: 0.47638\n",
      "[4] Training loss: 0.21154, Validation loss: 0.47697\n",
      "[5] Training loss: 0.21141, Validation loss: 0.47772\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22022, Validation loss: 0.14833\n",
      "[2] Training loss: 0.22027, Validation loss: 0.14824\n",
      "[3] Training loss: 0.22030, Validation loss: 0.14822\n",
      "[4] Training loss: 0.22029, Validation loss: 0.14824\n",
      "[5] Training loss: 0.22025, Validation loss: 0.14831\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23207, Validation loss: 0.34522\n",
      "[2] Training loss: 0.23208, Validation loss: 0.34537\n",
      "[3] Training loss: 0.23208, Validation loss: 0.34548\n",
      "[4] Training loss: 0.23207, Validation loss: 0.34556\n",
      "[5] Training loss: 0.23205, Validation loss: 0.34563\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22848, Validation loss: 0.08650\n",
      "[2] Training loss: 0.22847, Validation loss: 0.08656\n",
      "[3] Training loss: 0.22845, Validation loss: 0.08663\n",
      "[4] Training loss: 0.22843, Validation loss: 0.08671\n",
      "[5] Training loss: 0.22841, Validation loss: 0.08680\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10833, Validation loss: 0.58186\n",
      "[2] Training loss: 0.10834, Validation loss: 0.58203\n",
      "[3] Training loss: 0.10831, Validation loss: 0.58220\n",
      "[4] Training loss: 0.10826, Validation loss: 0.58238\n",
      "[5] Training loss: 0.10819, Validation loss: 0.58255\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21143, Validation loss: 0.47897\n",
      "[2] Training loss: 0.21143, Validation loss: 0.47858\n",
      "[3] Training loss: 0.21140, Validation loss: 0.47840\n",
      "[4] Training loss: 0.21134, Validation loss: 0.47841\n",
      "[5] Training loss: 0.21126, Validation loss: 0.47856\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22021, Validation loss: 0.14823\n",
      "[2] Training loss: 0.22026, Validation loss: 0.14816\n",
      "[3] Training loss: 0.22027, Validation loss: 0.14815\n",
      "[4] Training loss: 0.22026, Validation loss: 0.14819\n",
      "[5] Training loss: 0.22021, Validation loss: 0.14827\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23203, Validation loss: 0.34514\n",
      "[2] Training loss: 0.23203, Validation loss: 0.34523\n",
      "[3] Training loss: 0.23202, Validation loss: 0.34531\n",
      "[4] Training loss: 0.23200, Validation loss: 0.34537\n",
      "[5] Training loss: 0.23198, Validation loss: 0.34541\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22842, Validation loss: 0.08633\n",
      "[2] Training loss: 0.22840, Validation loss: 0.08637\n",
      "[3] Training loss: 0.22839, Validation loss: 0.08642\n",
      "[4] Training loss: 0.22837, Validation loss: 0.08649\n",
      "[5] Training loss: 0.22834, Validation loss: 0.08656\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10822, Validation loss: 0.58246\n",
      "[2] Training loss: 0.10822, Validation loss: 0.58265\n",
      "[3] Training loss: 0.10820, Validation loss: 0.58284\n",
      "[4] Training loss: 0.10815, Validation loss: 0.58302\n",
      "[5] Training loss: 0.10808, Validation loss: 0.58320\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21161, Validation loss: 0.47640\n",
      "[2] Training loss: 0.21161, Validation loss: 0.47631\n",
      "[3] Training loss: 0.21155, Validation loss: 0.47651\n",
      "[4] Training loss: 0.21146, Validation loss: 0.47690\n",
      "[5] Training loss: 0.21134, Validation loss: 0.47740\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22014, Validation loss: 0.14827\n",
      "[2] Training loss: 0.22019, Validation loss: 0.14819\n",
      "[3] Training loss: 0.22021, Validation loss: 0.14817\n",
      "[4] Training loss: 0.22020, Validation loss: 0.14820\n",
      "[5] Training loss: 0.22016, Validation loss: 0.14828\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23198, Validation loss: 0.34535\n",
      "[2] Training loss: 0.23199, Validation loss: 0.34548\n",
      "[3] Training loss: 0.23199, Validation loss: 0.34557\n",
      "[4] Training loss: 0.23197, Validation loss: 0.34565\n",
      "[5] Training loss: 0.23195, Validation loss: 0.34570\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22840, Validation loss: 0.08623\n",
      "[2] Training loss: 0.22839, Validation loss: 0.08628\n",
      "[3] Training loss: 0.22837, Validation loss: 0.08634\n",
      "[4] Training loss: 0.22835, Validation loss: 0.08642\n",
      "[5] Training loss: 0.22833, Validation loss: 0.08651\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10821, Validation loss: 0.58208\n",
      "[2] Training loss: 0.10821, Validation loss: 0.58226\n",
      "[3] Training loss: 0.10818, Validation loss: 0.58244\n",
      "[4] Training loss: 0.10814, Validation loss: 0.58262\n",
      "[5] Training loss: 0.10807, Validation loss: 0.58279\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21148, Validation loss: 0.47758\n",
      "[2] Training loss: 0.21148, Validation loss: 0.47727\n",
      "[3] Training loss: 0.21144, Validation loss: 0.47716\n",
      "[4] Training loss: 0.21137, Validation loss: 0.47723\n",
      "[5] Training loss: 0.21128, Validation loss: 0.47748\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22011, Validation loss: 0.14821\n",
      "[2] Training loss: 0.22016, Validation loss: 0.14814\n",
      "[3] Training loss: 0.22018, Validation loss: 0.14812\n",
      "[4] Training loss: 0.22016, Validation loss: 0.14816\n",
      "[5] Training loss: 0.22012, Validation loss: 0.14824\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23193, Validation loss: 0.34526\n",
      "[2] Training loss: 0.23194, Validation loss: 0.34537\n",
      "[3] Training loss: 0.23193, Validation loss: 0.34545\n",
      "[4] Training loss: 0.23192, Validation loss: 0.34551\n",
      "[5] Training loss: 0.23189, Validation loss: 0.34555\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22835, Validation loss: 0.08625\n",
      "[2] Training loss: 0.22833, Validation loss: 0.08630\n",
      "[3] Training loss: 0.22832, Validation loss: 0.08636\n",
      "[4] Training loss: 0.22830, Validation loss: 0.08643\n",
      "[5] Training loss: 0.22828, Validation loss: 0.08651\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10813, Validation loss: 0.58228\n",
      "[2] Training loss: 0.10813, Validation loss: 0.58246\n",
      "[3] Training loss: 0.10811, Validation loss: 0.58265\n",
      "[4] Training loss: 0.10806, Validation loss: 0.58282\n",
      "[5] Training loss: 0.10800, Validation loss: 0.58300\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21157, Validation loss: 0.47617\n",
      "[2] Training loss: 0.21157, Validation loss: 0.47609\n",
      "[3] Training loss: 0.21151, Validation loss: 0.47629\n",
      "[4] Training loss: 0.21141, Validation loss: 0.47674\n",
      "[5] Training loss: 0.21129, Validation loss: 0.47740\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22007, Validation loss: 0.14818\n",
      "[2] Training loss: 0.22013, Validation loss: 0.14810\n",
      "[3] Training loss: 0.22015, Validation loss: 0.14808\n",
      "[4] Training loss: 0.22014, Validation loss: 0.14811\n",
      "[5] Training loss: 0.22010, Validation loss: 0.14818\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23192, Validation loss: 0.34561\n",
      "[2] Training loss: 0.23193, Validation loss: 0.34575\n",
      "[3] Training loss: 0.23193, Validation loss: 0.34586\n",
      "[4] Training loss: 0.23192, Validation loss: 0.34595\n",
      "[5] Training loss: 0.23190, Validation loss: 0.34601\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22834, Validation loss: 0.08622\n",
      "[2] Training loss: 0.22833, Validation loss: 0.08627\n",
      "[3] Training loss: 0.22831, Validation loss: 0.08634\n",
      "[4] Training loss: 0.22829, Validation loss: 0.08641\n",
      "[5] Training loss: 0.22827, Validation loss: 0.08650\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10814, Validation loss: 0.58168\n",
      "[2] Training loss: 0.10814, Validation loss: 0.58186\n",
      "[3] Training loss: 0.10812, Validation loss: 0.58203\n",
      "[4] Training loss: 0.10807, Validation loss: 0.58221\n",
      "[5] Training loss: 0.10800, Validation loss: 0.58238\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21138, Validation loss: 0.47841\n",
      "[2] Training loss: 0.21137, Validation loss: 0.47804\n",
      "[3] Training loss: 0.21134, Validation loss: 0.47788\n",
      "[4] Training loss: 0.21127, Validation loss: 0.47780\n",
      "[5] Training loss: 0.21119, Validation loss: 0.47787\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22005, Validation loss: 0.14811\n",
      "[2] Training loss: 0.22010, Validation loss: 0.14804\n",
      "[3] Training loss: 0.22012, Validation loss: 0.14802\n",
      "[4] Training loss: 0.22010, Validation loss: 0.14806\n",
      "[5] Training loss: 0.22006, Validation loss: 0.14814\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23188, Validation loss: 0.34543\n",
      "[2] Training loss: 0.23188, Validation loss: 0.34553\n",
      "[3] Training loss: 0.23187, Validation loss: 0.34560\n",
      "[4] Training loss: 0.23186, Validation loss: 0.34566\n",
      "[5] Training loss: 0.23183, Validation loss: 0.34570\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22828, Validation loss: 0.08623\n",
      "[2] Training loss: 0.22827, Validation loss: 0.08628\n",
      "[3] Training loss: 0.22825, Validation loss: 0.08633\n",
      "[4] Training loss: 0.22823, Validation loss: 0.08640\n",
      "[5] Training loss: 0.22821, Validation loss: 0.08648\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10805, Validation loss: 0.58216\n",
      "[2] Training loss: 0.10805, Validation loss: 0.58235\n",
      "[3] Training loss: 0.10803, Validation loss: 0.58253\n",
      "[4] Training loss: 0.10798, Validation loss: 0.58271\n",
      "[5] Training loss: 0.10792, Validation loss: 0.58288\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21150, Validation loss: 0.47639\n",
      "[2] Training loss: 0.21150, Validation loss: 0.47637\n",
      "[3] Training loss: 0.21144, Validation loss: 0.47648\n",
      "[4] Training loss: 0.21134, Validation loss: 0.47674\n",
      "[5] Training loss: 0.21121, Validation loss: 0.47713\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.22000, Validation loss: 0.14810\n",
      "[2] Training loss: 0.22006, Validation loss: 0.14803\n",
      "[3] Training loss: 0.22008, Validation loss: 0.14801\n",
      "[4] Training loss: 0.22006, Validation loss: 0.14804\n",
      "[5] Training loss: 0.22002, Validation loss: 0.14811\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23185, Validation loss: 0.34575\n",
      "[2] Training loss: 0.23187, Validation loss: 0.34588\n",
      "[3] Training loss: 0.23186, Validation loss: 0.34598\n",
      "[4] Training loss: 0.23185, Validation loss: 0.34605\n",
      "[5] Training loss: 0.23183, Validation loss: 0.34611\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22827, Validation loss: 0.08623\n",
      "[2] Training loss: 0.22826, Validation loss: 0.08628\n",
      "[3] Training loss: 0.22824, Validation loss: 0.08634\n",
      "[4] Training loss: 0.22822, Validation loss: 0.08641\n",
      "[5] Training loss: 0.22820, Validation loss: 0.08649\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10806, Validation loss: 0.58158\n",
      "[2] Training loss: 0.10806, Validation loss: 0.58176\n",
      "[3] Training loss: 0.10804, Validation loss: 0.58195\n",
      "[4] Training loss: 0.10799, Validation loss: 0.58213\n",
      "[5] Training loss: 0.10792, Validation loss: 0.58230\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21131, Validation loss: 0.47787\n",
      "[2] Training loss: 0.21131, Validation loss: 0.47735\n",
      "[3] Training loss: 0.21127, Validation loss: 0.47706\n",
      "[4] Training loss: 0.21121, Validation loss: 0.47698\n",
      "[5] Training loss: 0.21112, Validation loss: 0.47707\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21998, Validation loss: 0.14804\n",
      "[2] Training loss: 0.22003, Validation loss: 0.14797\n",
      "[3] Training loss: 0.22004, Validation loss: 0.14795\n",
      "[4] Training loss: 0.22003, Validation loss: 0.14799\n",
      "[5] Training loss: 0.21999, Validation loss: 0.14807\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23181, Validation loss: 0.34549\n",
      "[2] Training loss: 0.23181, Validation loss: 0.34559\n",
      "[3] Training loss: 0.23181, Validation loss: 0.34567\n",
      "[4] Training loss: 0.23179, Validation loss: 0.34573\n",
      "[5] Training loss: 0.23177, Validation loss: 0.34576\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22821, Validation loss: 0.08618\n",
      "[2] Training loss: 0.22819, Validation loss: 0.08623\n",
      "[3] Training loss: 0.22818, Validation loss: 0.08628\n",
      "[4] Training loss: 0.22816, Validation loss: 0.08635\n",
      "[5] Training loss: 0.22813, Validation loss: 0.08643\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10797, Validation loss: 0.58197\n",
      "[2] Training loss: 0.10797, Validation loss: 0.58215\n",
      "[3] Training loss: 0.10794, Validation loss: 0.58233\n",
      "[4] Training loss: 0.10790, Validation loss: 0.58251\n",
      "[5] Training loss: 0.10783, Validation loss: 0.58268\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21148, Validation loss: 0.47613\n",
      "[2] Training loss: 0.21146, Validation loss: 0.47587\n",
      "[3] Training loss: 0.21139, Validation loss: 0.47590\n",
      "[4] Training loss: 0.21128, Validation loss: 0.47618\n",
      "[5] Training loss: 0.21115, Validation loss: 0.47665\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21993, Validation loss: 0.14802\n",
      "[2] Training loss: 0.21999, Validation loss: 0.14794\n",
      "[3] Training loss: 0.22001, Validation loss: 0.14792\n",
      "[4] Training loss: 0.22000, Validation loss: 0.14795\n",
      "[5] Training loss: 0.21996, Validation loss: 0.14803\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23181, Validation loss: 0.34601\n",
      "[2] Training loss: 0.23182, Validation loss: 0.34615\n",
      "[3] Training loss: 0.23182, Validation loss: 0.34626\n",
      "[4] Training loss: 0.23181, Validation loss: 0.34635\n",
      "[5] Training loss: 0.23179, Validation loss: 0.34641\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22820, Validation loss: 0.08633\n",
      "[2] Training loss: 0.22819, Validation loss: 0.08638\n",
      "[3] Training loss: 0.22818, Validation loss: 0.08644\n",
      "[4] Training loss: 0.22816, Validation loss: 0.08652\n",
      "[5] Training loss: 0.22813, Validation loss: 0.08660\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10799, Validation loss: 0.58096\n",
      "[2] Training loss: 0.10800, Validation loss: 0.58114\n",
      "[3] Training loss: 0.10797, Validation loss: 0.58132\n",
      "[4] Training loss: 0.10792, Validation loss: 0.58150\n",
      "[5] Training loss: 0.10785, Validation loss: 0.58167\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21122, Validation loss: 0.47776\n",
      "[2] Training loss: 0.21122, Validation loss: 0.47718\n",
      "[3] Training loss: 0.21119, Validation loss: 0.47681\n",
      "[4] Training loss: 0.21113, Validation loss: 0.47665\n",
      "[5] Training loss: 0.21106, Validation loss: 0.47665\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21991, Validation loss: 0.14796\n",
      "[2] Training loss: 0.21996, Validation loss: 0.14790\n",
      "[3] Training loss: 0.21997, Validation loss: 0.14789\n",
      "[4] Training loss: 0.21995, Validation loss: 0.14792\n",
      "[5] Training loss: 0.21991, Validation loss: 0.14801\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23174, Validation loss: 0.34566\n",
      "[2] Training loss: 0.23175, Validation loss: 0.34574\n",
      "[3] Training loss: 0.23174, Validation loss: 0.34582\n",
      "[4] Training loss: 0.23172, Validation loss: 0.34587\n",
      "[5] Training loss: 0.23169, Validation loss: 0.34590\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22813, Validation loss: 0.08609\n",
      "[2] Training loss: 0.22811, Validation loss: 0.08612\n",
      "[3] Training loss: 0.22809, Validation loss: 0.08617\n",
      "[4] Training loss: 0.22807, Validation loss: 0.08623\n",
      "[5] Training loss: 0.22805, Validation loss: 0.08630\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10787, Validation loss: 0.58193\n",
      "[2] Training loss: 0.10787, Validation loss: 0.58212\n",
      "[3] Training loss: 0.10784, Validation loss: 0.58231\n",
      "[4] Training loss: 0.10780, Validation loss: 0.58248\n",
      "[5] Training loss: 0.10773, Validation loss: 0.58266\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21148, Validation loss: 0.47531\n",
      "[2] Training loss: 0.21147, Validation loss: 0.47512\n",
      "[3] Training loss: 0.21138, Validation loss: 0.47521\n",
      "[4] Training loss: 0.21127, Validation loss: 0.47556\n",
      "[5] Training loss: 0.21113, Validation loss: 0.47601\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21985, Validation loss: 0.14798\n",
      "[2] Training loss: 0.21991, Validation loss: 0.14790\n",
      "[3] Training loss: 0.21993, Validation loss: 0.14788\n",
      "[4] Training loss: 0.21992, Validation loss: 0.14791\n",
      "[5] Training loss: 0.21988, Validation loss: 0.14799\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23174, Validation loss: 0.34624\n",
      "[2] Training loss: 0.23175, Validation loss: 0.34638\n",
      "[3] Training loss: 0.23175, Validation loss: 0.34649\n",
      "[4] Training loss: 0.23174, Validation loss: 0.34657\n",
      "[5] Training loss: 0.23172, Validation loss: 0.34662\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22813, Validation loss: 0.08609\n",
      "[2] Training loss: 0.22811, Validation loss: 0.08614\n",
      "[3] Training loss: 0.22810, Validation loss: 0.08621\n",
      "[4] Training loss: 0.22808, Validation loss: 0.08628\n",
      "[5] Training loss: 0.22806, Validation loss: 0.08637\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10792, Validation loss: 0.58112\n",
      "[2] Training loss: 0.10792, Validation loss: 0.58130\n",
      "[3] Training loss: 0.10789, Validation loss: 0.58149\n",
      "[4] Training loss: 0.10784, Validation loss: 0.58167\n",
      "[5] Training loss: 0.10777, Validation loss: 0.58186\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21116, Validation loss: 0.47685\n",
      "[2] Training loss: 0.21116, Validation loss: 0.47628\n",
      "[3] Training loss: 0.21113, Validation loss: 0.47593\n",
      "[4] Training loss: 0.21107, Validation loss: 0.47580\n",
      "[5] Training loss: 0.21099, Validation loss: 0.47581\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21984, Validation loss: 0.14789\n",
      "[2] Training loss: 0.21989, Validation loss: 0.14782\n",
      "[3] Training loss: 0.21991, Validation loss: 0.14781\n",
      "[4] Training loss: 0.21989, Validation loss: 0.14785\n",
      "[5] Training loss: 0.21984, Validation loss: 0.14793\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23169, Validation loss: 0.34584\n",
      "[2] Training loss: 0.23170, Validation loss: 0.34593\n",
      "[3] Training loss: 0.23169, Validation loss: 0.34600\n",
      "[4] Training loss: 0.23167, Validation loss: 0.34606\n",
      "[5] Training loss: 0.23165, Validation loss: 0.34609\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22805, Validation loss: 0.08588\n",
      "[2] Training loss: 0.22804, Validation loss: 0.08591\n",
      "[3] Training loss: 0.22802, Validation loss: 0.08596\n",
      "[4] Training loss: 0.22800, Validation loss: 0.08603\n",
      "[5] Training loss: 0.22798, Validation loss: 0.08610\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10780, Validation loss: 0.58188\n",
      "[2] Training loss: 0.10780, Validation loss: 0.58207\n",
      "[3] Training loss: 0.10778, Validation loss: 0.58225\n",
      "[4] Training loss: 0.10773, Validation loss: 0.58243\n",
      "[5] Training loss: 0.10766, Validation loss: 0.58261\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21137, Validation loss: 0.47432\n",
      "[2] Training loss: 0.21136, Validation loss: 0.47404\n",
      "[3] Training loss: 0.21131, Validation loss: 0.47404\n",
      "[4] Training loss: 0.21121, Validation loss: 0.47430\n",
      "[5] Training loss: 0.21108, Validation loss: 0.47473\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21977, Validation loss: 0.14791\n",
      "[2] Training loss: 0.21983, Validation loss: 0.14783\n",
      "[3] Training loss: 0.21985, Validation loss: 0.14781\n",
      "[4] Training loss: 0.21984, Validation loss: 0.14784\n",
      "[5] Training loss: 0.21980, Validation loss: 0.14792\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23167, Validation loss: 0.34634\n",
      "[2] Training loss: 0.23168, Validation loss: 0.34649\n",
      "[3] Training loss: 0.23167, Validation loss: 0.34661\n",
      "[4] Training loss: 0.23166, Validation loss: 0.34670\n",
      "[5] Training loss: 0.23164, Validation loss: 0.34676\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22805, Validation loss: 0.08596\n",
      "[2] Training loss: 0.22804, Validation loss: 0.08601\n",
      "[3] Training loss: 0.22802, Validation loss: 0.08607\n",
      "[4] Training loss: 0.22800, Validation loss: 0.08614\n",
      "[5] Training loss: 0.22798, Validation loss: 0.08623\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10780, Validation loss: 0.58088\n",
      "[2] Training loss: 0.10780, Validation loss: 0.58105\n",
      "[3] Training loss: 0.10778, Validation loss: 0.58123\n",
      "[4] Training loss: 0.10773, Validation loss: 0.58140\n",
      "[5] Training loss: 0.10766, Validation loss: 0.58157\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21117, Validation loss: 0.47615\n",
      "[2] Training loss: 0.21116, Validation loss: 0.47563\n",
      "[3] Training loss: 0.21113, Validation loss: 0.47534\n",
      "[4] Training loss: 0.21106, Validation loss: 0.47522\n",
      "[5] Training loss: 0.21097, Validation loss: 0.47528\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21975, Validation loss: 0.14785\n",
      "[2] Training loss: 0.21980, Validation loss: 0.14778\n",
      "[3] Training loss: 0.21982, Validation loss: 0.14776\n",
      "[4] Training loss: 0.21980, Validation loss: 0.14780\n",
      "[5] Training loss: 0.21976, Validation loss: 0.14788\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23162, Validation loss: 0.34627\n",
      "[2] Training loss: 0.23163, Validation loss: 0.34637\n",
      "[3] Training loss: 0.23162, Validation loss: 0.34645\n",
      "[4] Training loss: 0.23160, Validation loss: 0.34651\n",
      "[5] Training loss: 0.23158, Validation loss: 0.34655\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22799, Validation loss: 0.08615\n",
      "[2] Training loss: 0.22798, Validation loss: 0.08620\n",
      "[3] Training loss: 0.22796, Validation loss: 0.08626\n",
      "[4] Training loss: 0.22794, Validation loss: 0.08633\n",
      "[5] Training loss: 0.22792, Validation loss: 0.08641\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10772, Validation loss: 0.58131\n",
      "[2] Training loss: 0.10773, Validation loss: 0.58149\n",
      "[3] Training loss: 0.10770, Validation loss: 0.58167\n",
      "[4] Training loss: 0.10765, Validation loss: 0.58185\n",
      "[5] Training loss: 0.10759, Validation loss: 0.58203\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21128, Validation loss: 0.47415\n",
      "[2] Training loss: 0.21127, Validation loss: 0.47385\n",
      "[3] Training loss: 0.21122, Validation loss: 0.47384\n",
      "[4] Training loss: 0.21112, Validation loss: 0.47407\n",
      "[5] Training loss: 0.21100, Validation loss: 0.47441\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21970, Validation loss: 0.14784\n",
      "[2] Training loss: 0.21976, Validation loss: 0.14776\n",
      "[3] Training loss: 0.21978, Validation loss: 0.14774\n",
      "[4] Training loss: 0.21977, Validation loss: 0.14778\n",
      "[5] Training loss: 0.21973, Validation loss: 0.14785\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23160, Validation loss: 0.34652\n",
      "[2] Training loss: 0.23160, Validation loss: 0.34665\n",
      "[3] Training loss: 0.23160, Validation loss: 0.34675\n",
      "[4] Training loss: 0.23159, Validation loss: 0.34682\n",
      "[5] Training loss: 0.23157, Validation loss: 0.34687\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22798, Validation loss: 0.08602\n",
      "[2] Training loss: 0.22797, Validation loss: 0.08606\n",
      "[3] Training loss: 0.22795, Validation loss: 0.08612\n",
      "[4] Training loss: 0.22793, Validation loss: 0.08619\n",
      "[5] Training loss: 0.22791, Validation loss: 0.08626\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10772, Validation loss: 0.58088\n",
      "[2] Training loss: 0.10772, Validation loss: 0.58106\n",
      "[3] Training loss: 0.10769, Validation loss: 0.58124\n",
      "[4] Training loss: 0.10764, Validation loss: 0.58142\n",
      "[5] Training loss: 0.10758, Validation loss: 0.58160\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21111, Validation loss: 0.47519\n",
      "[2] Training loss: 0.21111, Validation loss: 0.47469\n",
      "[3] Training loss: 0.21108, Validation loss: 0.47442\n",
      "[4] Training loss: 0.21101, Validation loss: 0.47434\n",
      "[5] Training loss: 0.21092, Validation loss: 0.47442\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21968, Validation loss: 0.14779\n",
      "[2] Training loss: 0.21973, Validation loss: 0.14772\n",
      "[3] Training loss: 0.21974, Validation loss: 0.14770\n",
      "[4] Training loss: 0.21973, Validation loss: 0.14774\n",
      "[5] Training loss: 0.21969, Validation loss: 0.14782\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23154, Validation loss: 0.34632\n",
      "[2] Training loss: 0.23155, Validation loss: 0.34642\n",
      "[3] Training loss: 0.23154, Validation loss: 0.34650\n",
      "[4] Training loss: 0.23153, Validation loss: 0.34656\n",
      "[5] Training loss: 0.23150, Validation loss: 0.34659\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22792, Validation loss: 0.08594\n",
      "[2] Training loss: 0.22791, Validation loss: 0.08598\n",
      "[3] Training loss: 0.22789, Validation loss: 0.08604\n",
      "[4] Training loss: 0.22787, Validation loss: 0.08611\n",
      "[5] Training loss: 0.22785, Validation loss: 0.08619\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10763, Validation loss: 0.58126\n",
      "[2] Training loss: 0.10763, Validation loss: 0.58144\n",
      "[3] Training loss: 0.10761, Validation loss: 0.58162\n",
      "[4] Training loss: 0.10756, Validation loss: 0.58180\n",
      "[5] Training loss: 0.10749, Validation loss: 0.58198\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21128, Validation loss: 0.47356\n",
      "[2] Training loss: 0.21126, Validation loss: 0.47334\n",
      "[3] Training loss: 0.21118, Validation loss: 0.47338\n",
      "[4] Training loss: 0.21107, Validation loss: 0.47367\n",
      "[5] Training loss: 0.21094, Validation loss: 0.47408\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21964, Validation loss: 0.14776\n",
      "[2] Training loss: 0.21970, Validation loss: 0.14768\n",
      "[3] Training loss: 0.21972, Validation loss: 0.14766\n",
      "[4] Training loss: 0.21971, Validation loss: 0.14769\n",
      "[5] Training loss: 0.21967, Validation loss: 0.14776\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23155, Validation loss: 0.34674\n",
      "[2] Training loss: 0.23156, Validation loss: 0.34687\n",
      "[3] Training loss: 0.23156, Validation loss: 0.34698\n",
      "[4] Training loss: 0.23155, Validation loss: 0.34706\n",
      "[5] Training loss: 0.23153, Validation loss: 0.34712\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22791, Validation loss: 0.08620\n",
      "[2] Training loss: 0.22790, Validation loss: 0.08625\n",
      "[3] Training loss: 0.22788, Validation loss: 0.08632\n",
      "[4] Training loss: 0.22786, Validation loss: 0.08639\n",
      "[5] Training loss: 0.22784, Validation loss: 0.08648\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10766, Validation loss: 0.58026\n",
      "[2] Training loss: 0.10766, Validation loss: 0.58044\n",
      "[3] Training loss: 0.10764, Validation loss: 0.58061\n",
      "[4] Training loss: 0.10759, Validation loss: 0.58079\n",
      "[5] Training loss: 0.10752, Validation loss: 0.58096\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21101, Validation loss: 0.47500\n",
      "[2] Training loss: 0.21101, Validation loss: 0.47445\n",
      "[3] Training loss: 0.21098, Validation loss: 0.47410\n",
      "[4] Training loss: 0.21092, Validation loss: 0.47395\n",
      "[5] Training loss: 0.21085, Validation loss: 0.47393\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21961, Validation loss: 0.14771\n",
      "[2] Training loss: 0.21966, Validation loss: 0.14764\n",
      "[3] Training loss: 0.21968, Validation loss: 0.14763\n",
      "[4] Training loss: 0.21966, Validation loss: 0.14767\n",
      "[5] Training loss: 0.21961, Validation loss: 0.14776\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23148, Validation loss: 0.34635\n",
      "[2] Training loss: 0.23148, Validation loss: 0.34644\n",
      "[3] Training loss: 0.23147, Validation loss: 0.34651\n",
      "[4] Training loss: 0.23146, Validation loss: 0.34656\n",
      "[5] Training loss: 0.23143, Validation loss: 0.34660\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22783, Validation loss: 0.08588\n",
      "[2] Training loss: 0.22782, Validation loss: 0.08591\n",
      "[3] Training loss: 0.22780, Validation loss: 0.08596\n",
      "[4] Training loss: 0.22778, Validation loss: 0.08602\n",
      "[5] Training loss: 0.22776, Validation loss: 0.08609\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10753, Validation loss: 0.58130\n",
      "[2] Training loss: 0.10753, Validation loss: 0.58150\n",
      "[3] Training loss: 0.10751, Validation loss: 0.58168\n",
      "[4] Training loss: 0.10746, Validation loss: 0.58187\n",
      "[5] Training loss: 0.10740, Validation loss: 0.58204\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21128, Validation loss: 0.47250\n",
      "[2] Training loss: 0.21127, Validation loss: 0.47230\n",
      "[3] Training loss: 0.21119, Validation loss: 0.47236\n",
      "[4] Training loss: 0.21108, Validation loss: 0.47265\n",
      "[5] Training loss: 0.21094, Validation loss: 0.47311\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21955, Validation loss: 0.14772\n",
      "[2] Training loss: 0.21961, Validation loss: 0.14764\n",
      "[3] Training loss: 0.21963, Validation loss: 0.14762\n",
      "[4] Training loss: 0.21962, Validation loss: 0.14765\n",
      "[5] Training loss: 0.21958, Validation loss: 0.14772\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23147, Validation loss: 0.34690\n",
      "[2] Training loss: 0.23148, Validation loss: 0.34705\n",
      "[3] Training loss: 0.23149, Validation loss: 0.34716\n",
      "[4] Training loss: 0.23148, Validation loss: 0.34726\n",
      "[5] Training loss: 0.23146, Validation loss: 0.34732\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22784, Validation loss: 0.08586\n",
      "[2] Training loss: 0.22783, Validation loss: 0.08590\n",
      "[3] Training loss: 0.22781, Validation loss: 0.08596\n",
      "[4] Training loss: 0.22779, Validation loss: 0.08604\n",
      "[5] Training loss: 0.22777, Validation loss: 0.08612\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10758, Validation loss: 0.58041\n",
      "[2] Training loss: 0.10758, Validation loss: 0.58059\n",
      "[3] Training loss: 0.10755, Validation loss: 0.58077\n",
      "[4] Training loss: 0.10750, Validation loss: 0.58095\n",
      "[5] Training loss: 0.10743, Validation loss: 0.58113\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21096, Validation loss: 0.47440\n",
      "[2] Training loss: 0.21096, Validation loss: 0.47387\n",
      "[3] Training loss: 0.21092, Validation loss: 0.47357\n",
      "[4] Training loss: 0.21086, Validation loss: 0.47345\n",
      "[5] Training loss: 0.21078, Validation loss: 0.47347\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21955, Validation loss: 0.14763\n",
      "[2] Training loss: 0.21960, Validation loss: 0.14756\n",
      "[3] Training loss: 0.21961, Validation loss: 0.14755\n",
      "[4] Training loss: 0.21960, Validation loss: 0.14758\n",
      "[5] Training loss: 0.21955, Validation loss: 0.14767\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23144, Validation loss: 0.34668\n",
      "[2] Training loss: 0.23144, Validation loss: 0.34677\n",
      "[3] Training loss: 0.23143, Validation loss: 0.34685\n",
      "[4] Training loss: 0.23142, Validation loss: 0.34690\n",
      "[5] Training loss: 0.23139, Validation loss: 0.34694\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22778, Validation loss: 0.08577\n",
      "[2] Training loss: 0.22776, Validation loss: 0.08581\n",
      "[3] Training loss: 0.22774, Validation loss: 0.08586\n",
      "[4] Training loss: 0.22772, Validation loss: 0.08593\n",
      "[5] Training loss: 0.22770, Validation loss: 0.08601\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10747, Validation loss: 0.58099\n",
      "[2] Training loss: 0.10747, Validation loss: 0.58118\n",
      "[3] Training loss: 0.10745, Validation loss: 0.58137\n",
      "[4] Training loss: 0.10740, Validation loss: 0.58155\n",
      "[5] Training loss: 0.10734, Validation loss: 0.58173\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21112, Validation loss: 0.47211\n",
      "[2] Training loss: 0.21111, Validation loss: 0.47180\n",
      "[3] Training loss: 0.21106, Validation loss: 0.47179\n",
      "[4] Training loss: 0.21097, Validation loss: 0.47196\n",
      "[5] Training loss: 0.21085, Validation loss: 0.47225\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21948, Validation loss: 0.14765\n",
      "[2] Training loss: 0.21953, Validation loss: 0.14758\n",
      "[3] Training loss: 0.21955, Validation loss: 0.14756\n",
      "[4] Training loss: 0.21954, Validation loss: 0.14759\n",
      "[5] Training loss: 0.21950, Validation loss: 0.14767\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23139, Validation loss: 0.34684\n",
      "[2] Training loss: 0.23140, Validation loss: 0.34697\n",
      "[3] Training loss: 0.23140, Validation loss: 0.34707\n",
      "[4] Training loss: 0.23139, Validation loss: 0.34715\n",
      "[5] Training loss: 0.23137, Validation loss: 0.34721\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22776, Validation loss: 0.08578\n",
      "[2] Training loss: 0.22774, Validation loss: 0.08584\n",
      "[3] Training loss: 0.22773, Validation loss: 0.08591\n",
      "[4] Training loss: 0.22771, Validation loss: 0.08599\n",
      "[5] Training loss: 0.22769, Validation loss: 0.08608\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10746, Validation loss: 0.58065\n",
      "[2] Training loss: 0.10746, Validation loss: 0.58083\n",
      "[3] Training loss: 0.10743, Validation loss: 0.58101\n",
      "[4] Training loss: 0.10739, Validation loss: 0.58119\n",
      "[5] Training loss: 0.10732, Validation loss: 0.58137\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21101, Validation loss: 0.47291\n",
      "[2] Training loss: 0.21100, Validation loss: 0.47247\n",
      "[3] Training loss: 0.21096, Validation loss: 0.47227\n",
      "[4] Training loss: 0.21088, Validation loss: 0.47223\n",
      "[5] Training loss: 0.21078, Validation loss: 0.47234\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21946, Validation loss: 0.14759\n",
      "[2] Training loss: 0.21951, Validation loss: 0.14752\n",
      "[3] Training loss: 0.21953, Validation loss: 0.14750\n",
      "[4] Training loss: 0.21952, Validation loss: 0.14754\n",
      "[5] Training loss: 0.21947, Validation loss: 0.14762\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23136, Validation loss: 0.34683\n",
      "[2] Training loss: 0.23137, Validation loss: 0.34694\n",
      "[3] Training loss: 0.23136, Validation loss: 0.34702\n",
      "[4] Training loss: 0.23135, Validation loss: 0.34709\n",
      "[5] Training loss: 0.23132, Validation loss: 0.34714\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22770, Validation loss: 0.08588\n",
      "[2] Training loss: 0.22769, Validation loss: 0.08593\n",
      "[3] Training loss: 0.22767, Validation loss: 0.08600\n",
      "[4] Training loss: 0.22765, Validation loss: 0.08607\n",
      "[5] Training loss: 0.22763, Validation loss: 0.08616\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10740, Validation loss: 0.58067\n",
      "[2] Training loss: 0.10740, Validation loss: 0.58085\n",
      "[3] Training loss: 0.10738, Validation loss: 0.58103\n",
      "[4] Training loss: 0.10733, Validation loss: 0.58121\n",
      "[5] Training loss: 0.10726, Validation loss: 0.58138\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21106, Validation loss: 0.47159\n",
      "[2] Training loss: 0.21106, Validation loss: 0.47128\n",
      "[3] Training loss: 0.21100, Validation loss: 0.47125\n",
      "[4] Training loss: 0.21090, Validation loss: 0.47141\n",
      "[5] Training loss: 0.21079, Validation loss: 0.47174\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21942, Validation loss: 0.14756\n",
      "[2] Training loss: 0.21948, Validation loss: 0.14749\n",
      "[3] Training loss: 0.21950, Validation loss: 0.14747\n",
      "[4] Training loss: 0.21948, Validation loss: 0.14750\n",
      "[5] Training loss: 0.21944, Validation loss: 0.14758\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23134, Validation loss: 0.34714\n",
      "[2] Training loss: 0.23135, Validation loss: 0.34727\n",
      "[3] Training loss: 0.23135, Validation loss: 0.34738\n",
      "[4] Training loss: 0.23134, Validation loss: 0.34746\n",
      "[5] Training loss: 0.23131, Validation loss: 0.34753\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22768, Validation loss: 0.08590\n",
      "[2] Training loss: 0.22767, Validation loss: 0.08594\n",
      "[3] Training loss: 0.22766, Validation loss: 0.08600\n",
      "[4] Training loss: 0.22764, Validation loss: 0.08607\n",
      "[5] Training loss: 0.22762, Validation loss: 0.08615\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10738, Validation loss: 0.58014\n",
      "[2] Training loss: 0.10738, Validation loss: 0.58031\n",
      "[3] Training loss: 0.10735, Validation loss: 0.58049\n",
      "[4] Training loss: 0.10731, Validation loss: 0.58066\n",
      "[5] Training loss: 0.10724, Validation loss: 0.58083\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21092, Validation loss: 0.47261\n",
      "[2] Training loss: 0.21092, Validation loss: 0.47214\n",
      "[3] Training loss: 0.21088, Validation loss: 0.47186\n",
      "[4] Training loss: 0.21082, Validation loss: 0.47175\n",
      "[5] Training loss: 0.21074, Validation loss: 0.47178\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21938, Validation loss: 0.14754\n",
      "[2] Training loss: 0.21943, Validation loss: 0.14747\n",
      "[3] Training loss: 0.21945, Validation loss: 0.14746\n",
      "[4] Training loss: 0.21943, Validation loss: 0.14750\n",
      "[5] Training loss: 0.21939, Validation loss: 0.14759\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23128, Validation loss: 0.34701\n",
      "[2] Training loss: 0.23129, Validation loss: 0.34711\n",
      "[3] Training loss: 0.23128, Validation loss: 0.34719\n",
      "[4] Training loss: 0.23127, Validation loss: 0.34725\n",
      "[5] Training loss: 0.23124, Validation loss: 0.34729\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22763, Validation loss: 0.08566\n",
      "[2] Training loss: 0.22762, Validation loss: 0.08570\n",
      "[3] Training loss: 0.22760, Validation loss: 0.08575\n",
      "[4] Training loss: 0.22758, Validation loss: 0.08581\n",
      "[5] Training loss: 0.22756, Validation loss: 0.08589\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10730, Validation loss: 0.58072\n",
      "[2] Training loss: 0.10730, Validation loss: 0.58091\n",
      "[3] Training loss: 0.10728, Validation loss: 0.58110\n",
      "[4] Training loss: 0.10723, Validation loss: 0.58129\n",
      "[5] Training loss: 0.10717, Validation loss: 0.58147\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21105, Validation loss: 0.47066\n",
      "[2] Training loss: 0.21104, Validation loss: 0.47044\n",
      "[3] Training loss: 0.21097, Validation loss: 0.47052\n",
      "[4] Training loss: 0.21086, Validation loss: 0.47080\n",
      "[5] Training loss: 0.21073, Validation loss: 0.47120\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21934, Validation loss: 0.14750\n",
      "[2] Training loss: 0.21940, Validation loss: 0.14742\n",
      "[3] Training loss: 0.21942, Validation loss: 0.14740\n",
      "[4] Training loss: 0.21941, Validation loss: 0.14743\n",
      "[5] Training loss: 0.21938, Validation loss: 0.14750\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23128, Validation loss: 0.34733\n",
      "[2] Training loss: 0.23129, Validation loss: 0.34746\n",
      "[3] Training loss: 0.23129, Validation loss: 0.34757\n",
      "[4] Training loss: 0.23128, Validation loss: 0.34766\n",
      "[5] Training loss: 0.23126, Validation loss: 0.34772\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22762, Validation loss: 0.08610\n",
      "[2] Training loss: 0.22761, Validation loss: 0.08616\n",
      "[3] Training loss: 0.22759, Validation loss: 0.08623\n",
      "[4] Training loss: 0.22757, Validation loss: 0.08631\n",
      "[5] Training loss: 0.22755, Validation loss: 0.08640\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10732, Validation loss: 0.57965\n",
      "[2] Training loss: 0.10732, Validation loss: 0.57983\n",
      "[3] Training loss: 0.10730, Validation loss: 0.58000\n",
      "[4] Training loss: 0.10725, Validation loss: 0.58017\n",
      "[5] Training loss: 0.10718, Validation loss: 0.58034\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21082, Validation loss: 0.47233\n",
      "[2] Training loss: 0.21082, Validation loss: 0.47181\n",
      "[3] Training loss: 0.21079, Validation loss: 0.47149\n",
      "[4] Training loss: 0.21073, Validation loss: 0.47135\n",
      "[5] Training loss: 0.21065, Validation loss: 0.47134\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21932, Validation loss: 0.14746\n",
      "[2] Training loss: 0.21937, Validation loss: 0.14739\n",
      "[3] Training loss: 0.21938, Validation loss: 0.14738\n",
      "[4] Training loss: 0.21937, Validation loss: 0.14742\n",
      "[5] Training loss: 0.21932, Validation loss: 0.14751\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23122, Validation loss: 0.34702\n",
      "[2] Training loss: 0.23123, Validation loss: 0.34711\n",
      "[3] Training loss: 0.23122, Validation loss: 0.34718\n",
      "[4] Training loss: 0.23120, Validation loss: 0.34723\n",
      "[5] Training loss: 0.23118, Validation loss: 0.34726\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22755, Validation loss: 0.08552\n",
      "[2] Training loss: 0.22754, Validation loss: 0.08555\n",
      "[3] Training loss: 0.22752, Validation loss: 0.08559\n",
      "[4] Training loss: 0.22750, Validation loss: 0.08565\n",
      "[5] Training loss: 0.22748, Validation loss: 0.08571\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10722, Validation loss: 0.58079\n",
      "[2] Training loss: 0.10722, Validation loss: 0.58099\n",
      "[3] Training loss: 0.10719, Validation loss: 0.58118\n",
      "[4] Training loss: 0.10714, Validation loss: 0.58137\n",
      "[5] Training loss: 0.10708, Validation loss: 0.58155\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21104, Validation loss: 0.47012\n",
      "[2] Training loss: 0.21103, Validation loss: 0.46993\n",
      "[3] Training loss: 0.21096, Validation loss: 0.47002\n",
      "[4] Training loss: 0.21085, Validation loss: 0.47032\n",
      "[5] Training loss: 0.21072, Validation loss: 0.47079\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21926, Validation loss: 0.14745\n",
      "[2] Training loss: 0.21932, Validation loss: 0.14736\n",
      "[3] Training loss: 0.21935, Validation loss: 0.14734\n",
      "[4] Training loss: 0.21934, Validation loss: 0.14737\n",
      "[5] Training loss: 0.21930, Validation loss: 0.14744\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23122, Validation loss: 0.34754\n",
      "[2] Training loss: 0.23123, Validation loss: 0.34769\n",
      "[3] Training loss: 0.23123, Validation loss: 0.34781\n",
      "[4] Training loss: 0.23122, Validation loss: 0.34791\n",
      "[5] Training loss: 0.23120, Validation loss: 0.34798\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22755, Validation loss: 0.08592\n",
      "[2] Training loss: 0.22754, Validation loss: 0.08598\n",
      "[3] Training loss: 0.22752, Validation loss: 0.08605\n",
      "[4] Training loss: 0.22751, Validation loss: 0.08614\n",
      "[5] Training loss: 0.22748, Validation loss: 0.08623\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10723, Validation loss: 0.57942\n",
      "[2] Training loss: 0.10724, Validation loss: 0.57959\n",
      "[3] Training loss: 0.10721, Validation loss: 0.57976\n",
      "[4] Training loss: 0.10716, Validation loss: 0.57993\n",
      "[5] Training loss: 0.10710, Validation loss: 0.58010\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21075, Validation loss: 0.47247\n",
      "[2] Training loss: 0.21075, Validation loss: 0.47193\n",
      "[3] Training loss: 0.21072, Validation loss: 0.47159\n",
      "[4] Training loss: 0.21067, Validation loss: 0.47139\n",
      "[5] Training loss: 0.21059, Validation loss: 0.47133\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21925, Validation loss: 0.14738\n",
      "[2] Training loss: 0.21929, Validation loss: 0.14732\n",
      "[3] Training loss: 0.21931, Validation loss: 0.14731\n",
      "[4] Training loss: 0.21929, Validation loss: 0.14735\n",
      "[5] Training loss: 0.21924, Validation loss: 0.14744\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23115, Validation loss: 0.34718\n",
      "[2] Training loss: 0.23116, Validation loss: 0.34727\n",
      "[3] Training loss: 0.23115, Validation loss: 0.34733\n",
      "[4] Training loss: 0.23113, Validation loss: 0.34738\n",
      "[5] Training loss: 0.23111, Validation loss: 0.34741\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22748, Validation loss: 0.08538\n",
      "[2] Training loss: 0.22747, Validation loss: 0.08541\n",
      "[3] Training loss: 0.22745, Validation loss: 0.08545\n",
      "[4] Training loss: 0.22743, Validation loss: 0.08551\n",
      "[5] Training loss: 0.22741, Validation loss: 0.08557\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10713, Validation loss: 0.58093\n",
      "[2] Training loss: 0.10713, Validation loss: 0.58113\n",
      "[3] Training loss: 0.10711, Validation loss: 0.58132\n",
      "[4] Training loss: 0.10706, Validation loss: 0.58151\n",
      "[5] Training loss: 0.10699, Validation loss: 0.58170\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21102, Validation loss: 0.46986\n",
      "[2] Training loss: 0.21101, Validation loss: 0.46970\n",
      "[3] Training loss: 0.21093, Validation loss: 0.46981\n",
      "[4] Training loss: 0.21081, Validation loss: 0.47018\n",
      "[5] Training loss: 0.21066, Validation loss: 0.47069\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21919, Validation loss: 0.14737\n",
      "[2] Training loss: 0.21925, Validation loss: 0.14729\n",
      "[3] Training loss: 0.21928, Validation loss: 0.14726\n",
      "[4] Training loss: 0.21927, Validation loss: 0.14729\n",
      "[5] Training loss: 0.21923, Validation loss: 0.14736\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23116, Validation loss: 0.34780\n",
      "[2] Training loss: 0.23117, Validation loss: 0.34796\n",
      "[3] Training loss: 0.23117, Validation loss: 0.34809\n",
      "[4] Training loss: 0.23116, Validation loss: 0.34819\n",
      "[5] Training loss: 0.23114, Validation loss: 0.34826\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22749, Validation loss: 0.08601\n",
      "[2] Training loss: 0.22747, Validation loss: 0.08608\n",
      "[3] Training loss: 0.22746, Validation loss: 0.08616\n",
      "[4] Training loss: 0.22744, Validation loss: 0.08625\n",
      "[5] Training loss: 0.22742, Validation loss: 0.08634\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10716, Validation loss: 0.57914\n",
      "[2] Training loss: 0.10716, Validation loss: 0.57931\n",
      "[3] Training loss: 0.10714, Validation loss: 0.57947\n",
      "[4] Training loss: 0.10709, Validation loss: 0.57963\n",
      "[5] Training loss: 0.10702, Validation loss: 0.57979\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21068, Validation loss: 0.47295\n",
      "[2] Training loss: 0.21068, Validation loss: 0.47247\n",
      "[3] Training loss: 0.21065, Validation loss: 0.47216\n",
      "[4] Training loss: 0.21060, Validation loss: 0.47201\n",
      "[5] Training loss: 0.21053, Validation loss: 0.47200\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21917, Validation loss: 0.14731\n",
      "[2] Training loss: 0.21922, Validation loss: 0.14725\n",
      "[3] Training loss: 0.21923, Validation loss: 0.14724\n",
      "[4] Training loss: 0.21922, Validation loss: 0.14729\n",
      "[5] Training loss: 0.21917, Validation loss: 0.14738\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23108, Validation loss: 0.34754\n",
      "[2] Training loss: 0.23109, Validation loss: 0.34761\n",
      "[3] Training loss: 0.23108, Validation loss: 0.34767\n",
      "[4] Training loss: 0.23106, Validation loss: 0.34772\n",
      "[5] Training loss: 0.23103, Validation loss: 0.34774\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22741, Validation loss: 0.08539\n",
      "[2] Training loss: 0.22740, Validation loss: 0.08541\n",
      "[3] Training loss: 0.22738, Validation loss: 0.08545\n",
      "[4] Training loss: 0.22736, Validation loss: 0.08551\n",
      "[5] Training loss: 0.22734, Validation loss: 0.08557\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10703, Validation loss: 0.58057\n",
      "[2] Training loss: 0.10703, Validation loss: 0.58078\n",
      "[3] Training loss: 0.10701, Validation loss: 0.58098\n",
      "[4] Training loss: 0.10696, Validation loss: 0.58117\n",
      "[5] Training loss: 0.10689, Validation loss: 0.58136\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21100, Validation loss: 0.47057\n",
      "[2] Training loss: 0.21098, Validation loss: 0.47038\n",
      "[3] Training loss: 0.21090, Validation loss: 0.47043\n",
      "[4] Training loss: 0.21078, Validation loss: 0.47071\n",
      "[5] Training loss: 0.21064, Validation loss: 0.47113\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21912, Validation loss: 0.14730\n",
      "[2] Training loss: 0.21918, Validation loss: 0.14721\n",
      "[3] Training loss: 0.21920, Validation loss: 0.14719\n",
      "[4] Training loss: 0.21919, Validation loss: 0.14721\n",
      "[5] Training loss: 0.21916, Validation loss: 0.14728\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23109, Validation loss: 0.34795\n",
      "[2] Training loss: 0.23111, Validation loss: 0.34811\n",
      "[3] Training loss: 0.23111, Validation loss: 0.34824\n",
      "[4] Training loss: 0.23110, Validation loss: 0.34834\n",
      "[5] Training loss: 0.23108, Validation loss: 0.34843\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22741, Validation loss: 0.08598\n",
      "[2] Training loss: 0.22740, Validation loss: 0.08604\n",
      "[3] Training loss: 0.22738, Validation loss: 0.08612\n",
      "[4] Training loss: 0.22736, Validation loss: 0.08621\n",
      "[5] Training loss: 0.22734, Validation loss: 0.08630\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10707, Validation loss: 0.57894\n",
      "[2] Training loss: 0.10708, Validation loss: 0.57910\n",
      "[3] Training loss: 0.10705, Validation loss: 0.57926\n",
      "[4] Training loss: 0.10701, Validation loss: 0.57942\n",
      "[5] Training loss: 0.10694, Validation loss: 0.57958\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21063, Validation loss: 0.47283\n",
      "[2] Training loss: 0.21063, Validation loss: 0.47235\n",
      "[3] Training loss: 0.21060, Validation loss: 0.47201\n",
      "[4] Training loss: 0.21055, Validation loss: 0.47185\n",
      "[5] Training loss: 0.21048, Validation loss: 0.47180\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21910, Validation loss: 0.14726\n",
      "[2] Training loss: 0.21914, Validation loss: 0.14720\n",
      "[3] Training loss: 0.21916, Validation loss: 0.14719\n",
      "[4] Training loss: 0.21914, Validation loss: 0.14723\n",
      "[5] Training loss: 0.21909, Validation loss: 0.14732\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23102, Validation loss: 0.34774\n",
      "[2] Training loss: 0.23102, Validation loss: 0.34782\n",
      "[3] Training loss: 0.23101, Validation loss: 0.34788\n",
      "[4] Training loss: 0.23099, Validation loss: 0.34792\n",
      "[5] Training loss: 0.23097, Validation loss: 0.34794\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22734, Validation loss: 0.08532\n",
      "[2] Training loss: 0.22732, Validation loss: 0.08534\n",
      "[3] Training loss: 0.22731, Validation loss: 0.08538\n",
      "[4] Training loss: 0.22729, Validation loss: 0.08543\n",
      "[5] Training loss: 0.22726, Validation loss: 0.08549\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10695, Validation loss: 0.58063\n",
      "[2] Training loss: 0.10695, Validation loss: 0.58084\n",
      "[3] Training loss: 0.10693, Validation loss: 0.58104\n",
      "[4] Training loss: 0.10688, Validation loss: 0.58124\n",
      "[5] Training loss: 0.10681, Validation loss: 0.58143\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21096, Validation loss: 0.47021\n",
      "[2] Training loss: 0.21094, Validation loss: 0.47000\n",
      "[3] Training loss: 0.21086, Validation loss: 0.47006\n",
      "[4] Training loss: 0.21074, Validation loss: 0.47035\n",
      "[5] Training loss: 0.21060, Validation loss: 0.47085\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21904, Validation loss: 0.14723\n",
      "[2] Training loss: 0.21910, Validation loss: 0.14714\n",
      "[3] Training loss: 0.21913, Validation loss: 0.14712\n",
      "[4] Training loss: 0.21912, Validation loss: 0.14714\n",
      "[5] Training loss: 0.21908, Validation loss: 0.14721\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23103, Validation loss: 0.34826\n",
      "[2] Training loss: 0.23104, Validation loss: 0.34843\n",
      "[3] Training loss: 0.23104, Validation loss: 0.34857\n",
      "[4] Training loss: 0.23103, Validation loss: 0.34869\n",
      "[5] Training loss: 0.23101, Validation loss: 0.34878\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22734, Validation loss: 0.08596\n",
      "[2] Training loss: 0.22733, Validation loss: 0.08602\n",
      "[3] Training loss: 0.22732, Validation loss: 0.08610\n",
      "[4] Training loss: 0.22730, Validation loss: 0.08619\n",
      "[5] Training loss: 0.22728, Validation loss: 0.08628\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10698, Validation loss: 0.57862\n",
      "[2] Training loss: 0.10698, Validation loss: 0.57877\n",
      "[3] Training loss: 0.10696, Validation loss: 0.57893\n",
      "[4] Training loss: 0.10691, Validation loss: 0.57908\n",
      "[5] Training loss: 0.10685, Validation loss: 0.57924\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21058, Validation loss: 0.47307\n",
      "[2] Training loss: 0.21058, Validation loss: 0.47257\n",
      "[3] Training loss: 0.21055, Validation loss: 0.47224\n",
      "[4] Training loss: 0.21050, Validation loss: 0.47207\n",
      "[5] Training loss: 0.21043, Validation loss: 0.47203\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21902, Validation loss: 0.14719\n",
      "[2] Training loss: 0.21907, Validation loss: 0.14713\n",
      "[3] Training loss: 0.21908, Validation loss: 0.14712\n",
      "[4] Training loss: 0.21907, Validation loss: 0.14717\n",
      "[5] Training loss: 0.21902, Validation loss: 0.14726\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23096, Validation loss: 0.34811\n",
      "[2] Training loss: 0.23096, Validation loss: 0.34819\n",
      "[3] Training loss: 0.23095, Validation loss: 0.34825\n",
      "[4] Training loss: 0.23093, Validation loss: 0.34829\n",
      "[5] Training loss: 0.23091, Validation loss: 0.34831\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22727, Validation loss: 0.08533\n",
      "[2] Training loss: 0.22726, Validation loss: 0.08535\n",
      "[3] Training loss: 0.22724, Validation loss: 0.08539\n",
      "[4] Training loss: 0.22722, Validation loss: 0.08545\n",
      "[5] Training loss: 0.22720, Validation loss: 0.08552\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10687, Validation loss: 0.58022\n",
      "[2] Training loss: 0.10687, Validation loss: 0.58043\n",
      "[3] Training loss: 0.10684, Validation loss: 0.58063\n",
      "[4] Training loss: 0.10679, Validation loss: 0.58083\n",
      "[5] Training loss: 0.10673, Validation loss: 0.58102\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21089, Validation loss: 0.47040\n",
      "[2] Training loss: 0.21087, Validation loss: 0.47022\n",
      "[3] Training loss: 0.21078, Validation loss: 0.47030\n",
      "[4] Training loss: 0.21066, Validation loss: 0.47059\n",
      "[5] Training loss: 0.21051, Validation loss: 0.47103\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21897, Validation loss: 0.14717\n",
      "[2] Training loss: 0.21903, Validation loss: 0.14708\n",
      "[3] Training loss: 0.21906, Validation loss: 0.14706\n",
      "[4] Training loss: 0.21905, Validation loss: 0.14708\n",
      "[5] Training loss: 0.21901, Validation loss: 0.14715\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23097, Validation loss: 0.34849\n",
      "[2] Training loss: 0.23099, Validation loss: 0.34864\n",
      "[3] Training loss: 0.23099, Validation loss: 0.34878\n",
      "[4] Training loss: 0.23098, Validation loss: 0.34888\n",
      "[5] Training loss: 0.23096, Validation loss: 0.34896\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22727, Validation loss: 0.08595\n",
      "[2] Training loss: 0.22726, Validation loss: 0.08601\n",
      "[3] Training loss: 0.22724, Validation loss: 0.08609\n",
      "[4] Training loss: 0.22722, Validation loss: 0.08618\n",
      "[5] Training loss: 0.22720, Validation loss: 0.08627\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10692, Validation loss: 0.57860\n",
      "[2] Training loss: 0.10692, Validation loss: 0.57876\n",
      "[3] Training loss: 0.10689, Validation loss: 0.57892\n",
      "[4] Training loss: 0.10684, Validation loss: 0.57909\n",
      "[5] Training loss: 0.10678, Validation loss: 0.57926\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21051, Validation loss: 0.47272\n",
      "[2] Training loss: 0.21051, Validation loss: 0.47220\n",
      "[3] Training loss: 0.21048, Validation loss: 0.47185\n",
      "[4] Training loss: 0.21043, Validation loss: 0.47165\n",
      "[5] Training loss: 0.21036, Validation loss: 0.47160\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21896, Validation loss: 0.14711\n",
      "[2] Training loss: 0.21900, Validation loss: 0.14705\n",
      "[3] Training loss: 0.21902, Validation loss: 0.14704\n",
      "[4] Training loss: 0.21900, Validation loss: 0.14709\n",
      "[5] Training loss: 0.21895, Validation loss: 0.14718\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23091, Validation loss: 0.34821\n",
      "[2] Training loss: 0.23091, Validation loss: 0.34829\n",
      "[3] Training loss: 0.23090, Validation loss: 0.34835\n",
      "[4] Training loss: 0.23088, Validation loss: 0.34839\n",
      "[5] Training loss: 0.23086, Validation loss: 0.34842\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22720, Validation loss: 0.08523\n",
      "[2] Training loss: 0.22718, Validation loss: 0.08525\n",
      "[3] Training loss: 0.22716, Validation loss: 0.08529\n",
      "[4] Training loss: 0.22714, Validation loss: 0.08535\n",
      "[5] Training loss: 0.22712, Validation loss: 0.08541\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10678, Validation loss: 0.58005\n",
      "[2] Training loss: 0.10678, Validation loss: 0.58025\n",
      "[3] Training loss: 0.10675, Validation loss: 0.58045\n",
      "[4] Training loss: 0.10671, Validation loss: 0.58065\n",
      "[5] Training loss: 0.10664, Validation loss: 0.58084\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21082, Validation loss: 0.47001\n",
      "[2] Training loss: 0.21081, Validation loss: 0.46976\n",
      "[3] Training loss: 0.21073, Validation loss: 0.46977\n",
      "[4] Training loss: 0.21062, Validation loss: 0.47002\n",
      "[5] Training loss: 0.21048, Validation loss: 0.47043\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21890, Validation loss: 0.14711\n",
      "[2] Training loss: 0.21895, Validation loss: 0.14703\n",
      "[3] Training loss: 0.21898, Validation loss: 0.14700\n",
      "[4] Training loss: 0.21897, Validation loss: 0.14703\n",
      "[5] Training loss: 0.21893, Validation loss: 0.14710\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23090, Validation loss: 0.34864\n",
      "[2] Training loss: 0.23091, Validation loss: 0.34881\n",
      "[3] Training loss: 0.23091, Validation loss: 0.34895\n",
      "[4] Training loss: 0.23090, Validation loss: 0.34907\n",
      "[5] Training loss: 0.23088, Validation loss: 0.34916\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22719, Validation loss: 0.08585\n",
      "[2] Training loss: 0.22718, Validation loss: 0.08591\n",
      "[3] Training loss: 0.22716, Validation loss: 0.08599\n",
      "[4] Training loss: 0.22715, Validation loss: 0.08608\n",
      "[5] Training loss: 0.22712, Validation loss: 0.08617\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10681, Validation loss: 0.57840\n",
      "[2] Training loss: 0.10681, Validation loss: 0.57856\n",
      "[3] Training loss: 0.10679, Validation loss: 0.57871\n",
      "[4] Training loss: 0.10674, Validation loss: 0.57887\n",
      "[5] Training loss: 0.10667, Validation loss: 0.57903\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21050, Validation loss: 0.47229\n",
      "[2] Training loss: 0.21050, Validation loss: 0.47179\n",
      "[3] Training loss: 0.21047, Validation loss: 0.47148\n",
      "[4] Training loss: 0.21042, Validation loss: 0.47130\n",
      "[5] Training loss: 0.21035, Validation loss: 0.47127\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21887, Validation loss: 0.14707\n",
      "[2] Training loss: 0.21892, Validation loss: 0.14701\n",
      "[3] Training loss: 0.21893, Validation loss: 0.14700\n",
      "[4] Training loss: 0.21892, Validation loss: 0.14705\n",
      "[5] Training loss: 0.21887, Validation loss: 0.14714\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23083, Validation loss: 0.34845\n",
      "[2] Training loss: 0.23083, Validation loss: 0.34854\n",
      "[3] Training loss: 0.23082, Validation loss: 0.34860\n",
      "[4] Training loss: 0.23081, Validation loss: 0.34864\n",
      "[5] Training loss: 0.23078, Validation loss: 0.34867\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22712, Validation loss: 0.08544\n",
      "[2] Training loss: 0.22711, Validation loss: 0.08547\n",
      "[3] Training loss: 0.22709, Validation loss: 0.08551\n",
      "[4] Training loss: 0.22707, Validation loss: 0.08557\n",
      "[5] Training loss: 0.22705, Validation loss: 0.08564\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10670, Validation loss: 0.57968\n",
      "[2] Training loss: 0.10670, Validation loss: 0.57988\n",
      "[3] Training loss: 0.10667, Validation loss: 0.58007\n",
      "[4] Training loss: 0.10663, Validation loss: 0.58027\n",
      "[5] Training loss: 0.10656, Validation loss: 0.58045\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21078, Validation loss: 0.46988\n",
      "[2] Training loss: 0.21077, Validation loss: 0.46967\n",
      "[3] Training loss: 0.21069, Validation loss: 0.46970\n",
      "[4] Training loss: 0.21058, Validation loss: 0.46995\n",
      "[5] Training loss: 0.21044, Validation loss: 0.47039\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21882, Validation loss: 0.14707\n",
      "[2] Training loss: 0.21887, Validation loss: 0.14699\n",
      "[3] Training loss: 0.21890, Validation loss: 0.14696\n",
      "[4] Training loss: 0.21889, Validation loss: 0.14699\n",
      "[5] Training loss: 0.21885, Validation loss: 0.14707\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23083, Validation loss: 0.34884\n",
      "[2] Training loss: 0.23084, Validation loss: 0.34899\n",
      "[3] Training loss: 0.23084, Validation loss: 0.34912\n",
      "[4] Training loss: 0.23083, Validation loss: 0.34923\n",
      "[5] Training loss: 0.23081, Validation loss: 0.34931\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22713, Validation loss: 0.08553\n",
      "[2] Training loss: 0.22711, Validation loss: 0.08558\n",
      "[3] Training loss: 0.22710, Validation loss: 0.08564\n",
      "[4] Training loss: 0.22708, Validation loss: 0.08572\n",
      "[5] Training loss: 0.22706, Validation loss: 0.08580\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10674, Validation loss: 0.57853\n",
      "[2] Training loss: 0.10674, Validation loss: 0.57871\n",
      "[3] Training loss: 0.10671, Validation loss: 0.57888\n",
      "[4] Training loss: 0.10666, Validation loss: 0.57906\n",
      "[5] Training loss: 0.10659, Validation loss: 0.57923\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21043, Validation loss: 0.47218\n",
      "[2] Training loss: 0.21043, Validation loss: 0.47167\n",
      "[3] Training loss: 0.21040, Validation loss: 0.47135\n",
      "[4] Training loss: 0.21034, Validation loss: 0.47117\n",
      "[5] Training loss: 0.21027, Validation loss: 0.47114\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21881, Validation loss: 0.14699\n",
      "[2] Training loss: 0.21886, Validation loss: 0.14692\n",
      "[3] Training loss: 0.21887, Validation loss: 0.14691\n",
      "[4] Training loss: 0.21885, Validation loss: 0.14695\n",
      "[5] Training loss: 0.21881, Validation loss: 0.14704\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23078, Validation loss: 0.34855\n",
      "[2] Training loss: 0.23078, Validation loss: 0.34864\n",
      "[3] Training loss: 0.23077, Validation loss: 0.34871\n",
      "[4] Training loss: 0.23076, Validation loss: 0.34876\n",
      "[5] Training loss: 0.23073, Validation loss: 0.34880\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22706, Validation loss: 0.08507\n",
      "[2] Training loss: 0.22705, Validation loss: 0.08510\n",
      "[3] Training loss: 0.22703, Validation loss: 0.08515\n",
      "[4] Training loss: 0.22701, Validation loss: 0.08521\n",
      "[5] Training loss: 0.22699, Validation loss: 0.08528\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10663, Validation loss: 0.57990\n",
      "[2] Training loss: 0.10663, Validation loss: 0.58011\n",
      "[3] Training loss: 0.10661, Validation loss: 0.58030\n",
      "[4] Training loss: 0.10656, Validation loss: 0.58050\n",
      "[5] Training loss: 0.10649, Validation loss: 0.58068\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21066, Validation loss: 0.46964\n",
      "[2] Training loss: 0.21065, Validation loss: 0.46941\n",
      "[3] Training loss: 0.21059, Validation loss: 0.46946\n",
      "[4] Training loss: 0.21048, Validation loss: 0.46974\n",
      "[5] Training loss: 0.21035, Validation loss: 0.47018\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21875, Validation loss: 0.14696\n",
      "[2] Training loss: 0.21881, Validation loss: 0.14688\n",
      "[3] Training loss: 0.21884, Validation loss: 0.14686\n",
      "[4] Training loss: 0.21882, Validation loss: 0.14689\n",
      "[5] Training loss: 0.21878, Validation loss: 0.14696\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23077, Validation loss: 0.34893\n",
      "[2] Training loss: 0.23078, Validation loss: 0.34909\n",
      "[3] Training loss: 0.23078, Validation loss: 0.34923\n",
      "[4] Training loss: 0.23077, Validation loss: 0.34934\n",
      "[5] Training loss: 0.23075, Validation loss: 0.34942\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22705, Validation loss: 0.08558\n",
      "[2] Training loss: 0.22704, Validation loss: 0.08564\n",
      "[3] Training loss: 0.22703, Validation loss: 0.08572\n",
      "[4] Training loss: 0.22701, Validation loss: 0.08580\n",
      "[5] Training loss: 0.22699, Validation loss: 0.08590\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10664, Validation loss: 0.57829\n",
      "[2] Training loss: 0.10664, Validation loss: 0.57845\n",
      "[3] Training loss: 0.10662, Validation loss: 0.57861\n",
      "[4] Training loss: 0.10657, Validation loss: 0.57877\n",
      "[5] Training loss: 0.10650, Validation loss: 0.57894\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21040, Validation loss: 0.47216\n",
      "[2] Training loss: 0.21040, Validation loss: 0.47170\n",
      "[3] Training loss: 0.21036, Validation loss: 0.47141\n",
      "[4] Training loss: 0.21030, Validation loss: 0.47126\n",
      "[5] Training loss: 0.21023, Validation loss: 0.47125\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21873, Validation loss: 0.14693\n",
      "[2] Training loss: 0.21878, Validation loss: 0.14687\n",
      "[3] Training loss: 0.21879, Validation loss: 0.14686\n",
      "[4] Training loss: 0.21877, Validation loss: 0.14690\n",
      "[5] Training loss: 0.21873, Validation loss: 0.14699\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23071, Validation loss: 0.34881\n",
      "[2] Training loss: 0.23071, Validation loss: 0.34890\n",
      "[3] Training loss: 0.23071, Validation loss: 0.34897\n",
      "[4] Training loss: 0.23069, Validation loss: 0.34902\n",
      "[5] Training loss: 0.23066, Validation loss: 0.34905\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22699, Validation loss: 0.08518\n",
      "[2] Training loss: 0.22698, Validation loss: 0.08521\n",
      "[3] Training loss: 0.22696, Validation loss: 0.08526\n",
      "[4] Training loss: 0.22694, Validation loss: 0.08532\n",
      "[5] Training loss: 0.22692, Validation loss: 0.08539\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10655, Validation loss: 0.57935\n",
      "[2] Training loss: 0.10655, Validation loss: 0.57955\n",
      "[3] Training loss: 0.10653, Validation loss: 0.57975\n",
      "[4] Training loss: 0.10648, Validation loss: 0.57995\n",
      "[5] Training loss: 0.10641, Validation loss: 0.58014\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21061, Validation loss: 0.46995\n",
      "[2] Training loss: 0.21060, Validation loss: 0.46972\n",
      "[3] Training loss: 0.21053, Validation loss: 0.46976\n",
      "[4] Training loss: 0.21042, Validation loss: 0.47003\n",
      "[5] Training loss: 0.21029, Validation loss: 0.47044\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21868, Validation loss: 0.14690\n",
      "[2] Training loss: 0.21874, Validation loss: 0.14682\n",
      "[3] Training loss: 0.21876, Validation loss: 0.14680\n",
      "[4] Training loss: 0.21875, Validation loss: 0.14683\n",
      "[5] Training loss: 0.21871, Validation loss: 0.14690\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23071, Validation loss: 0.34911\n",
      "[2] Training loss: 0.23072, Validation loss: 0.34927\n",
      "[3] Training loss: 0.23072, Validation loss: 0.34940\n",
      "[4] Training loss: 0.23071, Validation loss: 0.34951\n",
      "[5] Training loss: 0.23069, Validation loss: 0.34959\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22698, Validation loss: 0.08571\n",
      "[2] Training loss: 0.22697, Validation loss: 0.08577\n",
      "[3] Training loss: 0.22696, Validation loss: 0.08585\n",
      "[4] Training loss: 0.22694, Validation loss: 0.08593\n",
      "[5] Training loss: 0.22692, Validation loss: 0.08602\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10657, Validation loss: 0.57793\n",
      "[2] Training loss: 0.10657, Validation loss: 0.57809\n",
      "[3] Training loss: 0.10655, Validation loss: 0.57826\n",
      "[4] Training loss: 0.10650, Validation loss: 0.57843\n",
      "[5] Training loss: 0.10643, Validation loss: 0.57859\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21034, Validation loss: 0.47220\n",
      "[2] Training loss: 0.21033, Validation loss: 0.47171\n",
      "[3] Training loss: 0.21030, Validation loss: 0.47139\n",
      "[4] Training loss: 0.21025, Validation loss: 0.47122\n",
      "[5] Training loss: 0.21018, Validation loss: 0.47118\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21866, Validation loss: 0.14687\n",
      "[2] Training loss: 0.21870, Validation loss: 0.14681\n",
      "[3] Training loss: 0.21872, Validation loss: 0.14680\n",
      "[4] Training loss: 0.21870, Validation loss: 0.14684\n",
      "[5] Training loss: 0.21865, Validation loss: 0.14693\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23064, Validation loss: 0.34888\n",
      "[2] Training loss: 0.23064, Validation loss: 0.34896\n",
      "[3] Training loss: 0.23064, Validation loss: 0.34903\n",
      "[4] Training loss: 0.23062, Validation loss: 0.34908\n",
      "[5] Training loss: 0.23059, Validation loss: 0.34911\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22691, Validation loss: 0.08502\n",
      "[2] Training loss: 0.22690, Validation loss: 0.08504\n",
      "[3] Training loss: 0.22688, Validation loss: 0.08508\n",
      "[4] Training loss: 0.22686, Validation loss: 0.08513\n",
      "[5] Training loss: 0.22684, Validation loss: 0.08520\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10646, Validation loss: 0.57929\n",
      "[2] Training loss: 0.10646, Validation loss: 0.57949\n",
      "[3] Training loss: 0.10644, Validation loss: 0.57970\n",
      "[4] Training loss: 0.10639, Validation loss: 0.57989\n",
      "[5] Training loss: 0.10632, Validation loss: 0.58009\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21060, Validation loss: 0.46977\n",
      "[2] Training loss: 0.21058, Validation loss: 0.46958\n",
      "[3] Training loss: 0.21050, Validation loss: 0.46964\n",
      "[4] Training loss: 0.21038, Validation loss: 0.46993\n",
      "[5] Training loss: 0.21024, Validation loss: 0.47036\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21861, Validation loss: 0.14684\n",
      "[2] Training loss: 0.21867, Validation loss: 0.14675\n",
      "[3] Training loss: 0.21869, Validation loss: 0.14673\n",
      "[4] Training loss: 0.21869, Validation loss: 0.14676\n",
      "[5] Training loss: 0.21865, Validation loss: 0.14683\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23065, Validation loss: 0.34922\n",
      "[2] Training loss: 0.23067, Validation loss: 0.34937\n",
      "[3] Training loss: 0.23067, Validation loss: 0.34951\n",
      "[4] Training loss: 0.23066, Validation loss: 0.34961\n",
      "[5] Training loss: 0.23064, Validation loss: 0.34969\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22691, Validation loss: 0.08573\n",
      "[2] Training loss: 0.22690, Validation loss: 0.08580\n",
      "[3] Training loss: 0.22688, Validation loss: 0.08588\n",
      "[4] Training loss: 0.22687, Validation loss: 0.08597\n",
      "[5] Training loss: 0.22684, Validation loss: 0.08606\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10652, Validation loss: 0.57774\n",
      "[2] Training loss: 0.10652, Validation loss: 0.57790\n",
      "[3] Training loss: 0.10649, Validation loss: 0.57807\n",
      "[4] Training loss: 0.10644, Validation loss: 0.57824\n",
      "[5] Training loss: 0.10638, Validation loss: 0.57841\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21026, Validation loss: 0.47204\n",
      "[2] Training loss: 0.21026, Validation loss: 0.47154\n",
      "[3] Training loss: 0.21023, Validation loss: 0.47121\n",
      "[4] Training loss: 0.21018, Validation loss: 0.47103\n",
      "[5] Training loss: 0.21011, Validation loss: 0.47098\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21859, Validation loss: 0.14677\n",
      "[2] Training loss: 0.21864, Validation loss: 0.14671\n",
      "[3] Training loss: 0.21865, Validation loss: 0.14671\n",
      "[4] Training loss: 0.21864, Validation loss: 0.14675\n",
      "[5] Training loss: 0.21859, Validation loss: 0.14684\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23059, Validation loss: 0.34890\n",
      "[2] Training loss: 0.23059, Validation loss: 0.34899\n",
      "[3] Training loss: 0.23058, Validation loss: 0.34906\n",
      "[4] Training loss: 0.23056, Validation loss: 0.34911\n",
      "[5] Training loss: 0.23053, Validation loss: 0.34915\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22684, Validation loss: 0.08513\n",
      "[2] Training loss: 0.22682, Validation loss: 0.08515\n",
      "[3] Training loss: 0.22681, Validation loss: 0.08519\n",
      "[4] Training loss: 0.22679, Validation loss: 0.08525\n",
      "[5] Training loss: 0.22676, Validation loss: 0.08531\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10639, Validation loss: 0.57914\n",
      "[2] Training loss: 0.10639, Validation loss: 0.57935\n",
      "[3] Training loss: 0.10636, Validation loss: 0.57955\n",
      "[4] Training loss: 0.10632, Validation loss: 0.57975\n",
      "[5] Training loss: 0.10625, Validation loss: 0.57994\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21055, Validation loss: 0.46950\n",
      "[2] Training loss: 0.21054, Validation loss: 0.46928\n",
      "[3] Training loss: 0.21046, Validation loss: 0.46930\n",
      "[4] Training loss: 0.21035, Validation loss: 0.46953\n",
      "[5] Training loss: 0.21022, Validation loss: 0.46989\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21853, Validation loss: 0.14678\n",
      "[2] Training loss: 0.21859, Validation loss: 0.14669\n",
      "[3] Training loss: 0.21861, Validation loss: 0.14667\n",
      "[4] Training loss: 0.21860, Validation loss: 0.14670\n",
      "[5] Training loss: 0.21856, Validation loss: 0.14678\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23057, Validation loss: 0.34924\n",
      "[2] Training loss: 0.23059, Validation loss: 0.34941\n",
      "[3] Training loss: 0.23059, Validation loss: 0.34955\n",
      "[4] Training loss: 0.23058, Validation loss: 0.34966\n",
      "[5] Training loss: 0.23056, Validation loss: 0.34976\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22683, Validation loss: 0.08566\n",
      "[2] Training loss: 0.22682, Validation loss: 0.08572\n",
      "[3] Training loss: 0.22681, Validation loss: 0.08579\n",
      "[4] Training loss: 0.22679, Validation loss: 0.08588\n",
      "[5] Training loss: 0.22677, Validation loss: 0.08597\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10642, Validation loss: 0.57769\n",
      "[2] Training loss: 0.10642, Validation loss: 0.57784\n",
      "[3] Training loss: 0.10639, Validation loss: 0.57801\n",
      "[4] Training loss: 0.10635, Validation loss: 0.57818\n",
      "[5] Training loss: 0.10628, Validation loss: 0.57834\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21024, Validation loss: 0.47153\n",
      "[2] Training loss: 0.21024, Validation loss: 0.47102\n",
      "[3] Training loss: 0.21021, Validation loss: 0.47070\n",
      "[4] Training loss: 0.21016, Validation loss: 0.47051\n",
      "[5] Training loss: 0.21009, Validation loss: 0.47046\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21851, Validation loss: 0.14675\n",
      "[2] Training loss: 0.21855, Validation loss: 0.14669\n",
      "[3] Training loss: 0.21857, Validation loss: 0.14668\n",
      "[4] Training loss: 0.21855, Validation loss: 0.14673\n",
      "[5] Training loss: 0.21850, Validation loss: 0.14682\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23050, Validation loss: 0.34905\n",
      "[2] Training loss: 0.23050, Validation loss: 0.34914\n",
      "[3] Training loss: 0.23049, Validation loss: 0.34921\n",
      "[4] Training loss: 0.23048, Validation loss: 0.34926\n",
      "[5] Training loss: 0.23045, Validation loss: 0.34930\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22676, Validation loss: 0.08486\n",
      "[2] Training loss: 0.22675, Validation loss: 0.08488\n",
      "[3] Training loss: 0.22673, Validation loss: 0.08492\n",
      "[4] Training loss: 0.22671, Validation loss: 0.08497\n",
      "[5] Training loss: 0.22669, Validation loss: 0.08503\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10630, Validation loss: 0.57922\n",
      "[2] Training loss: 0.10630, Validation loss: 0.57942\n",
      "[3] Training loss: 0.10628, Validation loss: 0.57963\n",
      "[4] Training loss: 0.10623, Validation loss: 0.57983\n",
      "[5] Training loss: 0.10616, Validation loss: 0.58002\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21055, Validation loss: 0.46903\n",
      "[2] Training loss: 0.21053, Validation loss: 0.46884\n",
      "[3] Training loss: 0.21043, Validation loss: 0.46892\n",
      "[4] Training loss: 0.21031, Validation loss: 0.46919\n",
      "[5] Training loss: 0.21016, Validation loss: 0.46963\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21846, Validation loss: 0.14671\n",
      "[2] Training loss: 0.21852, Validation loss: 0.14662\n",
      "[3] Training loss: 0.21855, Validation loss: 0.14659\n",
      "[4] Training loss: 0.21854, Validation loss: 0.14662\n",
      "[5] Training loss: 0.21850, Validation loss: 0.14669\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23052, Validation loss: 0.34941\n",
      "[2] Training loss: 0.23053, Validation loss: 0.34958\n",
      "[3] Training loss: 0.23054, Validation loss: 0.34972\n",
      "[4] Training loss: 0.23053, Validation loss: 0.34983\n",
      "[5] Training loss: 0.23051, Validation loss: 0.34992\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22677, Validation loss: 0.08576\n",
      "[2] Training loss: 0.22675, Validation loss: 0.08583\n",
      "[3] Training loss: 0.22674, Validation loss: 0.08592\n",
      "[4] Training loss: 0.22672, Validation loss: 0.08601\n",
      "[5] Training loss: 0.22670, Validation loss: 0.08611\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10636, Validation loss: 0.57731\n",
      "[2] Training loss: 0.10636, Validation loss: 0.57747\n",
      "[3] Training loss: 0.10633, Validation loss: 0.57763\n",
      "[4] Training loss: 0.10629, Validation loss: 0.57779\n",
      "[5] Training loss: 0.10622, Validation loss: 0.57796\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21016, Validation loss: 0.47150\n",
      "[2] Training loss: 0.21016, Validation loss: 0.47099\n",
      "[3] Training loss: 0.21013, Validation loss: 0.47067\n",
      "[4] Training loss: 0.21008, Validation loss: 0.47047\n",
      "[5] Training loss: 0.21001, Validation loss: 0.47041\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21845, Validation loss: 0.14665\n",
      "[2] Training loss: 0.21850, Validation loss: 0.14658\n",
      "[3] Training loss: 0.21851, Validation loss: 0.14658\n",
      "[4] Training loss: 0.21849, Validation loss: 0.14663\n",
      "[5] Training loss: 0.21844, Validation loss: 0.14672\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23045, Validation loss: 0.34917\n",
      "[2] Training loss: 0.23046, Validation loss: 0.34926\n",
      "[3] Training loss: 0.23045, Validation loss: 0.34933\n",
      "[4] Training loss: 0.23043, Validation loss: 0.34939\n",
      "[5] Training loss: 0.23040, Validation loss: 0.34943\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22669, Validation loss: 0.08516\n",
      "[2] Training loss: 0.22668, Validation loss: 0.08518\n",
      "[3] Training loss: 0.22666, Validation loss: 0.08522\n",
      "[4] Training loss: 0.22664, Validation loss: 0.08527\n",
      "[5] Training loss: 0.22662, Validation loss: 0.08534\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10624, Validation loss: 0.57880\n",
      "[2] Training loss: 0.10624, Validation loss: 0.57900\n",
      "[3] Training loss: 0.10621, Validation loss: 0.57920\n",
      "[4] Training loss: 0.10616, Validation loss: 0.57940\n",
      "[5] Training loss: 0.10610, Validation loss: 0.57960\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21044, Validation loss: 0.46882\n",
      "[2] Training loss: 0.21044, Validation loss: 0.46857\n",
      "[3] Training loss: 0.21037, Validation loss: 0.46858\n",
      "[4] Training loss: 0.21027, Validation loss: 0.46881\n",
      "[5] Training loss: 0.21013, Validation loss: 0.46917\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21838, Validation loss: 0.14667\n",
      "[2] Training loss: 0.21844, Validation loss: 0.14659\n",
      "[3] Training loss: 0.21846, Validation loss: 0.14657\n",
      "[4] Training loss: 0.21845, Validation loss: 0.14660\n",
      "[5] Training loss: 0.21841, Validation loss: 0.14667\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23043, Validation loss: 0.34945\n",
      "[2] Training loss: 0.23044, Validation loss: 0.34960\n",
      "[3] Training loss: 0.23044, Validation loss: 0.34974\n",
      "[4] Training loss: 0.23043, Validation loss: 0.34985\n",
      "[5] Training loss: 0.23041, Validation loss: 0.34994\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22669, Validation loss: 0.08536\n",
      "[2] Training loss: 0.22668, Validation loss: 0.08542\n",
      "[3] Training loss: 0.22666, Validation loss: 0.08548\n",
      "[4] Training loss: 0.22664, Validation loss: 0.08556\n",
      "[5] Training loss: 0.22662, Validation loss: 0.08564\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10625, Validation loss: 0.57767\n",
      "[2] Training loss: 0.10625, Validation loss: 0.57784\n",
      "[3] Training loss: 0.10623, Validation loss: 0.57801\n",
      "[4] Training loss: 0.10618, Validation loss: 0.57818\n",
      "[5] Training loss: 0.10611, Validation loss: 0.57835\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21017, Validation loss: 0.47075\n",
      "[2] Training loss: 0.21016, Validation loss: 0.47027\n",
      "[3] Training loss: 0.21013, Validation loss: 0.46995\n",
      "[4] Training loss: 0.21007, Validation loss: 0.46979\n",
      "[5] Training loss: 0.20999, Validation loss: 0.46975\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21836, Validation loss: 0.14661\n",
      "[2] Training loss: 0.21841, Validation loss: 0.14654\n",
      "[3] Training loss: 0.21843, Validation loss: 0.14654\n",
      "[4] Training loss: 0.21841, Validation loss: 0.14658\n",
      "[5] Training loss: 0.21837, Validation loss: 0.14667\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23039, Validation loss: 0.34926\n",
      "[2] Training loss: 0.23039, Validation loss: 0.34936\n",
      "[3] Training loss: 0.23038, Validation loss: 0.34944\n",
      "[4] Training loss: 0.23037, Validation loss: 0.34950\n",
      "[5] Training loss: 0.23034, Validation loss: 0.34953\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22663, Validation loss: 0.08500\n",
      "[2] Training loss: 0.22662, Validation loss: 0.08503\n",
      "[3] Training loss: 0.22660, Validation loss: 0.08508\n",
      "[4] Training loss: 0.22658, Validation loss: 0.08513\n",
      "[5] Training loss: 0.22656, Validation loss: 0.08520\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10617, Validation loss: 0.57870\n",
      "[2] Training loss: 0.10617, Validation loss: 0.57890\n",
      "[3] Training loss: 0.10614, Validation loss: 0.57910\n",
      "[4] Training loss: 0.10609, Validation loss: 0.57930\n",
      "[5] Training loss: 0.10603, Validation loss: 0.57949\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21035, Validation loss: 0.46839\n",
      "[2] Training loss: 0.21034, Validation loss: 0.46817\n",
      "[3] Training loss: 0.21026, Validation loss: 0.46821\n",
      "[4] Training loss: 0.21015, Validation loss: 0.46848\n",
      "[5] Training loss: 0.21001, Validation loss: 0.46889\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21832, Validation loss: 0.14657\n",
      "[2] Training loss: 0.21838, Validation loss: 0.14649\n",
      "[3] Training loss: 0.21841, Validation loss: 0.14646\n",
      "[4] Training loss: 0.21840, Validation loss: 0.14649\n",
      "[5] Training loss: 0.21836, Validation loss: 0.14657\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23039, Validation loss: 0.34966\n",
      "[2] Training loss: 0.23040, Validation loss: 0.34982\n",
      "[3] Training loss: 0.23040, Validation loss: 0.34996\n",
      "[4] Training loss: 0.23039, Validation loss: 0.35007\n",
      "[5] Training loss: 0.23037, Validation loss: 0.35016\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22662, Validation loss: 0.08561\n",
      "[2] Training loss: 0.22661, Validation loss: 0.08568\n",
      "[3] Training loss: 0.22660, Validation loss: 0.08575\n",
      "[4] Training loss: 0.22658, Validation loss: 0.08584\n",
      "[5] Training loss: 0.22656, Validation loss: 0.08593\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10619, Validation loss: 0.57717\n",
      "[2] Training loss: 0.10619, Validation loss: 0.57733\n",
      "[3] Training loss: 0.10616, Validation loss: 0.57749\n",
      "[4] Training loss: 0.10612, Validation loss: 0.57766\n",
      "[5] Training loss: 0.10605, Validation loss: 0.57783\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21008, Validation loss: 0.47069\n",
      "[2] Training loss: 0.21008, Validation loss: 0.47022\n",
      "[3] Training loss: 0.21005, Validation loss: 0.46992\n",
      "[4] Training loss: 0.21000, Validation loss: 0.46976\n",
      "[5] Training loss: 0.20993, Validation loss: 0.46973\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21830, Validation loss: 0.14654\n",
      "[2] Training loss: 0.21834, Validation loss: 0.14647\n",
      "[3] Training loss: 0.21836, Validation loss: 0.14647\n",
      "[4] Training loss: 0.21834, Validation loss: 0.14652\n",
      "[5] Training loss: 0.21829, Validation loss: 0.14661\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23032, Validation loss: 0.34949\n",
      "[2] Training loss: 0.23032, Validation loss: 0.34958\n",
      "[3] Training loss: 0.23031, Validation loss: 0.34965\n",
      "[4] Training loss: 0.23029, Validation loss: 0.34971\n",
      "[5] Training loss: 0.23027, Validation loss: 0.34974\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22655, Validation loss: 0.08495\n",
      "[2] Training loss: 0.22654, Validation loss: 0.08498\n",
      "[3] Training loss: 0.22652, Validation loss: 0.08502\n",
      "[4] Training loss: 0.22650, Validation loss: 0.08507\n",
      "[5] Training loss: 0.22648, Validation loss: 0.08513\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10607, Validation loss: 0.57841\n",
      "[2] Training loss: 0.10607, Validation loss: 0.57862\n",
      "[3] Training loss: 0.10604, Validation loss: 0.57882\n",
      "[4] Training loss: 0.10600, Validation loss: 0.57902\n",
      "[5] Training loss: 0.10593, Validation loss: 0.57921\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21037, Validation loss: 0.46843\n",
      "[2] Training loss: 0.21035, Validation loss: 0.46819\n",
      "[3] Training loss: 0.21027, Validation loss: 0.46819\n",
      "[4] Training loss: 0.21015, Validation loss: 0.46841\n",
      "[5] Training loss: 0.21001, Validation loss: 0.46876\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21824, Validation loss: 0.14653\n",
      "[2] Training loss: 0.21830, Validation loss: 0.14645\n",
      "[3] Training loss: 0.21832, Validation loss: 0.14642\n",
      "[4] Training loss: 0.21832, Validation loss: 0.14645\n",
      "[5] Training loss: 0.21828, Validation loss: 0.14653\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23032, Validation loss: 0.34970\n",
      "[2] Training loss: 0.23034, Validation loss: 0.34985\n",
      "[3] Training loss: 0.23034, Validation loss: 0.34998\n",
      "[4] Training loss: 0.23033, Validation loss: 0.35009\n",
      "[5] Training loss: 0.23031, Validation loss: 0.35016\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22655, Validation loss: 0.08534\n",
      "[2] Training loss: 0.22654, Validation loss: 0.08540\n",
      "[3] Training loss: 0.22652, Validation loss: 0.08547\n",
      "[4] Training loss: 0.22650, Validation loss: 0.08555\n",
      "[5] Training loss: 0.22648, Validation loss: 0.08564\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10613, Validation loss: 0.57736\n",
      "[2] Training loss: 0.10613, Validation loss: 0.57753\n",
      "[3] Training loss: 0.10611, Validation loss: 0.57771\n",
      "[4] Training loss: 0.10606, Validation loss: 0.57789\n",
      "[5] Training loss: 0.10599, Validation loss: 0.57806\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21003, Validation loss: 0.47000\n",
      "[2] Training loss: 0.21003, Validation loss: 0.46952\n",
      "[3] Training loss: 0.21000, Validation loss: 0.46922\n",
      "[4] Training loss: 0.20994, Validation loss: 0.46908\n",
      "[5] Training loss: 0.20987, Validation loss: 0.46906\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21823, Validation loss: 0.14645\n",
      "[2] Training loss: 0.21828, Validation loss: 0.14638\n",
      "[3] Training loss: 0.21829, Validation loss: 0.14638\n",
      "[4] Training loss: 0.21828, Validation loss: 0.14642\n",
      "[5] Training loss: 0.21823, Validation loss: 0.14651\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23026, Validation loss: 0.34948\n",
      "[2] Training loss: 0.23027, Validation loss: 0.34958\n",
      "[3] Training loss: 0.23026, Validation loss: 0.34967\n",
      "[4] Training loss: 0.23024, Validation loss: 0.34973\n",
      "[5] Training loss: 0.23021, Validation loss: 0.34979\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22648, Validation loss: 0.08512\n",
      "[2] Training loss: 0.22646, Validation loss: 0.08515\n",
      "[3] Training loss: 0.22645, Validation loss: 0.08520\n",
      "[4] Training loss: 0.22643, Validation loss: 0.08527\n",
      "[5] Training loss: 0.22641, Validation loss: 0.08534\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10600, Validation loss: 0.57817\n",
      "[2] Training loss: 0.10600, Validation loss: 0.57836\n",
      "[3] Training loss: 0.10597, Validation loss: 0.57855\n",
      "[4] Training loss: 0.10593, Validation loss: 0.57874\n",
      "[5] Training loss: 0.10586, Validation loss: 0.57892\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21028, Validation loss: 0.46803\n",
      "[2] Training loss: 0.21028, Validation loss: 0.46778\n",
      "[3] Training loss: 0.21022, Validation loss: 0.46775\n",
      "[4] Training loss: 0.21012, Validation loss: 0.46794\n",
      "[5] Training loss: 0.20999, Validation loss: 0.46824\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21816, Validation loss: 0.14649\n",
      "[2] Training loss: 0.21821, Validation loss: 0.14642\n",
      "[3] Training loss: 0.21823, Validation loss: 0.14640\n",
      "[4] Training loss: 0.21822, Validation loss: 0.14644\n",
      "[5] Training loss: 0.21818, Validation loss: 0.14652\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23022, Validation loss: 0.34982\n",
      "[2] Training loss: 0.23023, Validation loss: 0.34996\n",
      "[3] Training loss: 0.23023, Validation loss: 0.35007\n",
      "[4] Training loss: 0.23022, Validation loss: 0.35017\n",
      "[5] Training loss: 0.23020, Validation loss: 0.35024\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22648, Validation loss: 0.08485\n",
      "[2] Training loss: 0.22646, Validation loss: 0.08489\n",
      "[3] Training loss: 0.22645, Validation loss: 0.08494\n",
      "[4] Training loss: 0.22643, Validation loss: 0.08501\n",
      "[5] Training loss: 0.22641, Validation loss: 0.08509\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10601, Validation loss: 0.57770\n",
      "[2] Training loss: 0.10601, Validation loss: 0.57788\n",
      "[3] Training loss: 0.10599, Validation loss: 0.57807\n",
      "[4] Training loss: 0.10594, Validation loss: 0.57826\n",
      "[5] Training loss: 0.10587, Validation loss: 0.57845\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21006, Validation loss: 0.46935\n",
      "[2] Training loss: 0.21006, Validation loss: 0.46892\n",
      "[3] Training loss: 0.21002, Validation loss: 0.46865\n",
      "[4] Training loss: 0.20995, Validation loss: 0.46854\n",
      "[5] Training loss: 0.20986, Validation loss: 0.46858\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21814, Validation loss: 0.14641\n",
      "[2] Training loss: 0.21819, Validation loss: 0.14634\n",
      "[3] Training loss: 0.21821, Validation loss: 0.14633\n",
      "[4] Training loss: 0.21820, Validation loss: 0.14637\n",
      "[5] Training loss: 0.21815, Validation loss: 0.14645\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23019, Validation loss: 0.34952\n",
      "[2] Training loss: 0.23020, Validation loss: 0.34964\n",
      "[3] Training loss: 0.23019, Validation loss: 0.34973\n",
      "[4] Training loss: 0.23017, Validation loss: 0.34980\n",
      "[5] Training loss: 0.23015, Validation loss: 0.34985\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22642, Validation loss: 0.08504\n",
      "[2] Training loss: 0.22641, Validation loss: 0.08508\n",
      "[3] Training loss: 0.22639, Validation loss: 0.08515\n",
      "[4] Training loss: 0.22637, Validation loss: 0.08522\n",
      "[5] Training loss: 0.22635, Validation loss: 0.08530\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10593, Validation loss: 0.57795\n",
      "[2] Training loss: 0.10593, Validation loss: 0.57814\n",
      "[3] Training loss: 0.10591, Validation loss: 0.57832\n",
      "[4] Training loss: 0.10586, Validation loss: 0.57850\n",
      "[5] Training loss: 0.10580, Validation loss: 0.57868\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21016, Validation loss: 0.46787\n",
      "[2] Training loss: 0.21015, Validation loss: 0.46760\n",
      "[3] Training loss: 0.21010, Validation loss: 0.46756\n",
      "[4] Training loss: 0.21000, Validation loss: 0.46774\n",
      "[5] Training loss: 0.20988, Validation loss: 0.46805\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21810, Validation loss: 0.14639\n",
      "[2] Training loss: 0.21816, Validation loss: 0.14632\n",
      "[3] Training loss: 0.21818, Validation loss: 0.14630\n",
      "[4] Training loss: 0.21816, Validation loss: 0.14633\n",
      "[5] Training loss: 0.21812, Validation loss: 0.14642\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23017, Validation loss: 0.34985\n",
      "[2] Training loss: 0.23018, Validation loss: 0.35000\n",
      "[3] Training loss: 0.23018, Validation loss: 0.35012\n",
      "[4] Training loss: 0.23017, Validation loss: 0.35022\n",
      "[5] Training loss: 0.23015, Validation loss: 0.35029\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22641, Validation loss: 0.08495\n",
      "[2] Training loss: 0.22640, Validation loss: 0.08499\n",
      "[3] Training loss: 0.22639, Validation loss: 0.08504\n",
      "[4] Training loss: 0.22637, Validation loss: 0.08511\n",
      "[5] Training loss: 0.22635, Validation loss: 0.08519\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10595, Validation loss: 0.57748\n",
      "[2] Training loss: 0.10595, Validation loss: 0.57766\n",
      "[3] Training loss: 0.10592, Validation loss: 0.57784\n",
      "[4] Training loss: 0.10587, Validation loss: 0.57803\n",
      "[5] Training loss: 0.10580, Validation loss: 0.57821\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20998, Validation loss: 0.46922\n",
      "[2] Training loss: 0.20998, Validation loss: 0.46881\n",
      "[3] Training loss: 0.20994, Validation loss: 0.46856\n",
      "[4] Training loss: 0.20987, Validation loss: 0.46847\n",
      "[5] Training loss: 0.20979, Validation loss: 0.46850\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21807, Validation loss: 0.14634\n",
      "[2] Training loss: 0.21813, Validation loss: 0.14627\n",
      "[3] Training loss: 0.21814, Validation loss: 0.14626\n",
      "[4] Training loss: 0.21813, Validation loss: 0.14630\n",
      "[5] Training loss: 0.21808, Validation loss: 0.14638\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23013, Validation loss: 0.34968\n",
      "[2] Training loss: 0.23013, Validation loss: 0.34980\n",
      "[3] Training loss: 0.23012, Validation loss: 0.34988\n",
      "[4] Training loss: 0.23011, Validation loss: 0.34995\n",
      "[5] Training loss: 0.23008, Validation loss: 0.35000\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22635, Validation loss: 0.08487\n",
      "[2] Training loss: 0.22634, Validation loss: 0.08491\n",
      "[3] Training loss: 0.22632, Validation loss: 0.08497\n",
      "[4] Training loss: 0.22630, Validation loss: 0.08504\n",
      "[5] Training loss: 0.22628, Validation loss: 0.08511\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10585, Validation loss: 0.57786\n",
      "[2] Training loss: 0.10585, Validation loss: 0.57805\n",
      "[3] Training loss: 0.10583, Validation loss: 0.57824\n",
      "[4] Training loss: 0.10578, Validation loss: 0.57843\n",
      "[5] Training loss: 0.10572, Validation loss: 0.57861\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21013, Validation loss: 0.46781\n",
      "[2] Training loss: 0.21011, Validation loss: 0.46758\n",
      "[3] Training loss: 0.21003, Validation loss: 0.46758\n",
      "[4] Training loss: 0.20993, Validation loss: 0.46778\n",
      "[5] Training loss: 0.20980, Validation loss: 0.46810\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21804, Validation loss: 0.14631\n",
      "[2] Training loss: 0.21809, Validation loss: 0.14623\n",
      "[3] Training loss: 0.21812, Validation loss: 0.14621\n",
      "[4] Training loss: 0.21811, Validation loss: 0.14625\n",
      "[5] Training loss: 0.21806, Validation loss: 0.14633\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23013, Validation loss: 0.35004\n",
      "[2] Training loss: 0.23014, Validation loss: 0.35018\n",
      "[3] Training loss: 0.23014, Validation loss: 0.35030\n",
      "[4] Training loss: 0.23013, Validation loss: 0.35040\n",
      "[5] Training loss: 0.23011, Validation loss: 0.35047\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22635, Validation loss: 0.08508\n",
      "[2] Training loss: 0.22634, Validation loss: 0.08513\n",
      "[3] Training loss: 0.22632, Validation loss: 0.08520\n",
      "[4] Training loss: 0.22630, Validation loss: 0.08527\n",
      "[5] Training loss: 0.22628, Validation loss: 0.08535\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10590, Validation loss: 0.57710\n",
      "[2] Training loss: 0.10590, Validation loss: 0.57729\n",
      "[3] Training loss: 0.10587, Validation loss: 0.57747\n",
      "[4] Training loss: 0.10582, Validation loss: 0.57766\n",
      "[5] Training loss: 0.10575, Validation loss: 0.57785\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20988, Validation loss: 0.46907\n",
      "[2] Training loss: 0.20987, Validation loss: 0.46860\n",
      "[3] Training loss: 0.20984, Validation loss: 0.46831\n",
      "[4] Training loss: 0.20979, Validation loss: 0.46817\n",
      "[5] Training loss: 0.20971, Validation loss: 0.46816\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21801, Validation loss: 0.14625\n",
      "[2] Training loss: 0.21806, Validation loss: 0.14618\n",
      "[3] Training loss: 0.21808, Validation loss: 0.14617\n",
      "[4] Training loss: 0.21806, Validation loss: 0.14622\n",
      "[5] Training loss: 0.21802, Validation loss: 0.14630\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23007, Validation loss: 0.34963\n",
      "[2] Training loss: 0.23007, Validation loss: 0.34973\n",
      "[3] Training loss: 0.23006, Validation loss: 0.34982\n",
      "[4] Training loss: 0.23004, Validation loss: 0.34989\n",
      "[5] Training loss: 0.23002, Validation loss: 0.34994\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22627, Validation loss: 0.08486\n",
      "[2] Training loss: 0.22626, Validation loss: 0.08490\n",
      "[3] Training loss: 0.22624, Validation loss: 0.08495\n",
      "[4] Training loss: 0.22622, Validation loss: 0.08501\n",
      "[5] Training loss: 0.22620, Validation loss: 0.08508\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10576, Validation loss: 0.57781\n",
      "[2] Training loss: 0.10576, Validation loss: 0.57800\n",
      "[3] Training loss: 0.10574, Validation loss: 0.57818\n",
      "[4] Training loss: 0.10569, Validation loss: 0.57837\n",
      "[5] Training loss: 0.10563, Validation loss: 0.57855\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21014, Validation loss: 0.46727\n",
      "[2] Training loss: 0.21012, Validation loss: 0.46703\n",
      "[3] Training loss: 0.21005, Validation loss: 0.46701\n",
      "[4] Training loss: 0.20994, Validation loss: 0.46719\n",
      "[5] Training loss: 0.20980, Validation loss: 0.46750\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21795, Validation loss: 0.14628\n",
      "[2] Training loss: 0.21801, Validation loss: 0.14620\n",
      "[3] Training loss: 0.21803, Validation loss: 0.14618\n",
      "[4] Training loss: 0.21802, Validation loss: 0.14622\n",
      "[5] Training loss: 0.21798, Validation loss: 0.14630\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23005, Validation loss: 0.35016\n",
      "[2] Training loss: 0.23006, Validation loss: 0.35032\n",
      "[3] Training loss: 0.23006, Validation loss: 0.35044\n",
      "[4] Training loss: 0.23005, Validation loss: 0.35054\n",
      "[5] Training loss: 0.23003, Validation loss: 0.35062\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22627, Validation loss: 0.08486\n",
      "[2] Training loss: 0.22626, Validation loss: 0.08491\n",
      "[3] Training loss: 0.22624, Validation loss: 0.08497\n",
      "[4] Training loss: 0.22623, Validation loss: 0.08504\n",
      "[5] Training loss: 0.22621, Validation loss: 0.08512\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10582, Validation loss: 0.57710\n",
      "[2] Training loss: 0.10582, Validation loss: 0.57728\n",
      "[3] Training loss: 0.10579, Validation loss: 0.57748\n",
      "[4] Training loss: 0.10574, Validation loss: 0.57767\n",
      "[5] Training loss: 0.10567, Validation loss: 0.57787\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20984, Validation loss: 0.46839\n",
      "[2] Training loss: 0.20984, Validation loss: 0.46792\n",
      "[3] Training loss: 0.20981, Validation loss: 0.46763\n",
      "[4] Training loss: 0.20975, Validation loss: 0.46750\n",
      "[5] Training loss: 0.20967, Validation loss: 0.46749\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21794, Validation loss: 0.14619\n",
      "[2] Training loss: 0.21799, Validation loss: 0.14613\n",
      "[3] Training loss: 0.21801, Validation loss: 0.14612\n",
      "[4] Training loss: 0.21799, Validation loss: 0.14616\n",
      "[5] Training loss: 0.21794, Validation loss: 0.14624\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.23000, Validation loss: 0.34968\n",
      "[2] Training loss: 0.23000, Validation loss: 0.34979\n",
      "[3] Training loss: 0.22999, Validation loss: 0.34987\n",
      "[4] Training loss: 0.22997, Validation loss: 0.34994\n",
      "[5] Training loss: 0.22995, Validation loss: 0.34999\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22619, Validation loss: 0.08485\n",
      "[2] Training loss: 0.22618, Validation loss: 0.08489\n",
      "[3] Training loss: 0.22616, Validation loss: 0.08494\n",
      "[4] Training loss: 0.22614, Validation loss: 0.08500\n",
      "[5] Training loss: 0.22612, Validation loss: 0.08508\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10569, Validation loss: 0.57778\n",
      "[2] Training loss: 0.10569, Validation loss: 0.57796\n",
      "[3] Training loss: 0.10566, Validation loss: 0.57814\n",
      "[4] Training loss: 0.10562, Validation loss: 0.57832\n",
      "[5] Training loss: 0.10555, Validation loss: 0.57850\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21011, Validation loss: 0.46670\n",
      "[2] Training loss: 0.21010, Validation loss: 0.46647\n",
      "[3] Training loss: 0.21002, Validation loss: 0.46646\n",
      "[4] Training loss: 0.20991, Validation loss: 0.46665\n",
      "[5] Training loss: 0.20977, Validation loss: 0.46697\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21788, Validation loss: 0.14621\n",
      "[2] Training loss: 0.21794, Validation loss: 0.14614\n",
      "[3] Training loss: 0.21796, Validation loss: 0.14612\n",
      "[4] Training loss: 0.21795, Validation loss: 0.14615\n",
      "[5] Training loss: 0.21791, Validation loss: 0.14623\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22998, Validation loss: 0.35019\n",
      "[2] Training loss: 0.22999, Validation loss: 0.35034\n",
      "[3] Training loss: 0.22999, Validation loss: 0.35048\n",
      "[4] Training loss: 0.22998, Validation loss: 0.35057\n",
      "[5] Training loss: 0.22996, Validation loss: 0.35065\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22620, Validation loss: 0.08503\n",
      "[2] Training loss: 0.22619, Validation loss: 0.08508\n",
      "[3] Training loss: 0.22617, Validation loss: 0.08514\n",
      "[4] Training loss: 0.22615, Validation loss: 0.08522\n",
      "[5] Training loss: 0.22613, Validation loss: 0.08530\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10573, Validation loss: 0.57680\n",
      "[2] Training loss: 0.10573, Validation loss: 0.57698\n",
      "[3] Training loss: 0.10571, Validation loss: 0.57716\n",
      "[4] Training loss: 0.10566, Validation loss: 0.57735\n",
      "[5] Training loss: 0.10559, Validation loss: 0.57754\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20981, Validation loss: 0.46814\n",
      "[2] Training loss: 0.20981, Validation loss: 0.46771\n",
      "[3] Training loss: 0.20977, Validation loss: 0.46743\n",
      "[4] Training loss: 0.20972, Validation loss: 0.46729\n",
      "[5] Training loss: 0.20964, Validation loss: 0.46729\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21786, Validation loss: 0.14614\n",
      "[2] Training loss: 0.21791, Validation loss: 0.14608\n",
      "[3] Training loss: 0.21793, Validation loss: 0.14607\n",
      "[4] Training loss: 0.21791, Validation loss: 0.14611\n",
      "[5] Training loss: 0.21787, Validation loss: 0.14620\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22993, Validation loss: 0.34988\n",
      "[2] Training loss: 0.22993, Validation loss: 0.34998\n",
      "[3] Training loss: 0.22992, Validation loss: 0.35007\n",
      "[4] Training loss: 0.22991, Validation loss: 0.35014\n",
      "[5] Training loss: 0.22988, Validation loss: 0.35019\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22613, Validation loss: 0.08484\n",
      "[2] Training loss: 0.22611, Validation loss: 0.08487\n",
      "[3] Training loss: 0.22610, Validation loss: 0.08492\n",
      "[4] Training loss: 0.22608, Validation loss: 0.08498\n",
      "[5] Training loss: 0.22606, Validation loss: 0.08505\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10560, Validation loss: 0.57744\n",
      "[2] Training loss: 0.10560, Validation loss: 0.57763\n",
      "[3] Training loss: 0.10558, Validation loss: 0.57781\n",
      "[4] Training loss: 0.10553, Validation loss: 0.57800\n",
      "[5] Training loss: 0.10547, Validation loss: 0.57818\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21005, Validation loss: 0.46649\n",
      "[2] Training loss: 0.21004, Validation loss: 0.46626\n",
      "[3] Training loss: 0.20996, Validation loss: 0.46625\n",
      "[4] Training loss: 0.20984, Validation loss: 0.46643\n",
      "[5] Training loss: 0.20971, Validation loss: 0.46674\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21781, Validation loss: 0.14616\n",
      "[2] Training loss: 0.21787, Validation loss: 0.14608\n",
      "[3] Training loss: 0.21789, Validation loss: 0.14607\n",
      "[4] Training loss: 0.21788, Validation loss: 0.14610\n",
      "[5] Training loss: 0.21784, Validation loss: 0.14618\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22993, Validation loss: 0.35045\n",
      "[2] Training loss: 0.22994, Validation loss: 0.35060\n",
      "[3] Training loss: 0.22994, Validation loss: 0.35073\n",
      "[4] Training loss: 0.22993, Validation loss: 0.35083\n",
      "[5] Training loss: 0.22991, Validation loss: 0.35089\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22613, Validation loss: 0.08485\n",
      "[2] Training loss: 0.22612, Validation loss: 0.08490\n",
      "[3] Training loss: 0.22610, Validation loss: 0.08496\n",
      "[4] Training loss: 0.22609, Validation loss: 0.08504\n",
      "[5] Training loss: 0.22607, Validation loss: 0.08512\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10568, Validation loss: 0.57669\n",
      "[2] Training loss: 0.10568, Validation loss: 0.57688\n",
      "[3] Training loss: 0.10565, Validation loss: 0.57708\n",
      "[4] Training loss: 0.10560, Validation loss: 0.57728\n",
      "[5] Training loss: 0.10553, Validation loss: 0.57748\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20973, Validation loss: 0.46747\n",
      "[2] Training loss: 0.20972, Validation loss: 0.46699\n",
      "[3] Training loss: 0.20969, Validation loss: 0.46668\n",
      "[4] Training loss: 0.20964, Validation loss: 0.46652\n",
      "[5] Training loss: 0.20957, Validation loss: 0.46647\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21780, Validation loss: 0.14606\n",
      "[2] Training loss: 0.21785, Validation loss: 0.14600\n",
      "[3] Training loss: 0.21787, Validation loss: 0.14599\n",
      "[4] Training loss: 0.21785, Validation loss: 0.14603\n",
      "[5] Training loss: 0.21780, Validation loss: 0.14612\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22987, Validation loss: 0.34982\n",
      "[2] Training loss: 0.22987, Validation loss: 0.34992\n",
      "[3] Training loss: 0.22987, Validation loss: 0.35000\n",
      "[4] Training loss: 0.22985, Validation loss: 0.35006\n",
      "[5] Training loss: 0.22982, Validation loss: 0.35011\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22605, Validation loss: 0.08477\n",
      "[2] Training loss: 0.22603, Validation loss: 0.08480\n",
      "[3] Training loss: 0.22602, Validation loss: 0.08485\n",
      "[4] Training loss: 0.22600, Validation loss: 0.08491\n",
      "[5] Training loss: 0.22598, Validation loss: 0.08498\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10553, Validation loss: 0.57765\n",
      "[2] Training loss: 0.10553, Validation loss: 0.57784\n",
      "[3] Training loss: 0.10551, Validation loss: 0.57802\n",
      "[4] Training loss: 0.10546, Validation loss: 0.57820\n",
      "[5] Training loss: 0.10540, Validation loss: 0.57837\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.21000, Validation loss: 0.46539\n",
      "[2] Training loss: 0.21000, Validation loss: 0.46513\n",
      "[3] Training loss: 0.20994, Validation loss: 0.46509\n",
      "[4] Training loss: 0.20984, Validation loss: 0.46524\n",
      "[5] Training loss: 0.20970, Validation loss: 0.46558\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21773, Validation loss: 0.14610\n",
      "[2] Training loss: 0.21779, Validation loss: 0.14603\n",
      "[3] Training loss: 0.21781, Validation loss: 0.14601\n",
      "[4] Training loss: 0.21780, Validation loss: 0.14604\n",
      "[5] Training loss: 0.21775, Validation loss: 0.14612\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22984, Validation loss: 0.35029\n",
      "[2] Training loss: 0.22985, Validation loss: 0.35046\n",
      "[3] Training loss: 0.22985, Validation loss: 0.35060\n",
      "[4] Training loss: 0.22984, Validation loss: 0.35071\n",
      "[5] Training loss: 0.22982, Validation loss: 0.35079\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22606, Validation loss: 0.08466\n",
      "[2] Training loss: 0.22604, Validation loss: 0.08471\n",
      "[3] Training loss: 0.22603, Validation loss: 0.08476\n",
      "[4] Training loss: 0.22601, Validation loss: 0.08483\n",
      "[5] Training loss: 0.22599, Validation loss: 0.08491\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10556, Validation loss: 0.57678\n",
      "[2] Training loss: 0.10556, Validation loss: 0.57696\n",
      "[3] Training loss: 0.10554, Validation loss: 0.57714\n",
      "[4] Training loss: 0.10549, Validation loss: 0.57732\n",
      "[5] Training loss: 0.10542, Validation loss: 0.57750\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20973, Validation loss: 0.46705\n",
      "[2] Training loss: 0.20973, Validation loss: 0.46663\n",
      "[3] Training loss: 0.20970, Validation loss: 0.46636\n",
      "[4] Training loss: 0.20963, Validation loss: 0.46624\n",
      "[5] Training loss: 0.20955, Validation loss: 0.46624\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21772, Validation loss: 0.14602\n",
      "[2] Training loss: 0.21777, Validation loss: 0.14595\n",
      "[3] Training loss: 0.21779, Validation loss: 0.14594\n",
      "[4] Training loss: 0.21777, Validation loss: 0.14598\n",
      "[5] Training loss: 0.21773, Validation loss: 0.14607\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22980, Validation loss: 0.35003\n",
      "[2] Training loss: 0.22980, Validation loss: 0.35013\n",
      "[3] Training loss: 0.22980, Validation loss: 0.35021\n",
      "[4] Training loss: 0.22978, Validation loss: 0.35028\n",
      "[5] Training loss: 0.22976, Validation loss: 0.35033\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22599, Validation loss: 0.08471\n",
      "[2] Training loss: 0.22598, Validation loss: 0.08475\n",
      "[3] Training loss: 0.22596, Validation loss: 0.08481\n",
      "[4] Training loss: 0.22594, Validation loss: 0.08487\n",
      "[5] Training loss: 0.22592, Validation loss: 0.08495\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10547, Validation loss: 0.57737\n",
      "[2] Training loss: 0.10547, Validation loss: 0.57756\n",
      "[3] Training loss: 0.10544, Validation loss: 0.57774\n",
      "[4] Training loss: 0.10540, Validation loss: 0.57793\n",
      "[5] Training loss: 0.10533, Validation loss: 0.57811\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20990, Validation loss: 0.46548\n",
      "[2] Training loss: 0.20988, Validation loss: 0.46527\n",
      "[3] Training loss: 0.20981, Validation loss: 0.46526\n",
      "[4] Training loss: 0.20970, Validation loss: 0.46543\n",
      "[5] Training loss: 0.20957, Validation loss: 0.46573\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21768, Validation loss: 0.14601\n",
      "[2] Training loss: 0.21773, Validation loss: 0.14594\n",
      "[3] Training loss: 0.21776, Validation loss: 0.14592\n",
      "[4] Training loss: 0.21774, Validation loss: 0.14595\n",
      "[5] Training loss: 0.21770, Validation loss: 0.14603\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22980, Validation loss: 0.35052\n",
      "[2] Training loss: 0.22981, Validation loss: 0.35067\n",
      "[3] Training loss: 0.22981, Validation loss: 0.35079\n",
      "[4] Training loss: 0.22980, Validation loss: 0.35089\n",
      "[5] Training loss: 0.22978, Validation loss: 0.35095\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22600, Validation loss: 0.08476\n",
      "[2] Training loss: 0.22598, Validation loss: 0.08481\n",
      "[3] Training loss: 0.22597, Validation loss: 0.08487\n",
      "[4] Training loss: 0.22595, Validation loss: 0.08494\n",
      "[5] Training loss: 0.22593, Validation loss: 0.08502\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10552, Validation loss: 0.57663\n",
      "[2] Training loss: 0.10552, Validation loss: 0.57682\n",
      "[3] Training loss: 0.10549, Validation loss: 0.57701\n",
      "[4] Training loss: 0.10544, Validation loss: 0.57721\n",
      "[5] Training loss: 0.10537, Validation loss: 0.57741\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20963, Validation loss: 0.46653\n",
      "[2] Training loss: 0.20963, Validation loss: 0.46609\n",
      "[3] Training loss: 0.20960, Validation loss: 0.46581\n",
      "[4] Training loss: 0.20954, Validation loss: 0.46567\n",
      "[5] Training loss: 0.20947, Validation loss: 0.46567\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21766, Validation loss: 0.14593\n",
      "[2] Training loss: 0.21771, Validation loss: 0.14587\n",
      "[3] Training loss: 0.21772, Validation loss: 0.14586\n",
      "[4] Training loss: 0.21771, Validation loss: 0.14590\n",
      "[5] Training loss: 0.21766, Validation loss: 0.14599\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22974, Validation loss: 0.35001\n",
      "[2] Training loss: 0.22974, Validation loss: 0.35012\n",
      "[3] Training loss: 0.22974, Validation loss: 0.35020\n",
      "[4] Training loss: 0.22972, Validation loss: 0.35027\n",
      "[5] Training loss: 0.22969, Validation loss: 0.35033\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22591, Validation loss: 0.08468\n",
      "[2] Training loss: 0.22590, Validation loss: 0.08472\n",
      "[3] Training loss: 0.22588, Validation loss: 0.08477\n",
      "[4] Training loss: 0.22586, Validation loss: 0.08483\n",
      "[5] Training loss: 0.22584, Validation loss: 0.08491\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10538, Validation loss: 0.57729\n",
      "[2] Training loss: 0.10538, Validation loss: 0.57747\n",
      "[3] Training loss: 0.10536, Validation loss: 0.57765\n",
      "[4] Training loss: 0.10531, Validation loss: 0.57783\n",
      "[5] Training loss: 0.10524, Validation loss: 0.57800\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20990, Validation loss: 0.46493\n",
      "[2] Training loss: 0.20989, Validation loss: 0.46470\n",
      "[3] Training loss: 0.20982, Validation loss: 0.46467\n",
      "[4] Training loss: 0.20971, Validation loss: 0.46485\n",
      "[5] Training loss: 0.20957, Validation loss: 0.46514\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21759, Validation loss: 0.14597\n",
      "[2] Training loss: 0.21765, Validation loss: 0.14589\n",
      "[3] Training loss: 0.21767, Validation loss: 0.14587\n",
      "[4] Training loss: 0.21766, Validation loss: 0.14591\n",
      "[5] Training loss: 0.21762, Validation loss: 0.14599\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22972, Validation loss: 0.35052\n",
      "[2] Training loss: 0.22974, Validation loss: 0.35068\n",
      "[3] Training loss: 0.22973, Validation loss: 0.35081\n",
      "[4] Training loss: 0.22972, Validation loss: 0.35091\n",
      "[5] Training loss: 0.22970, Validation loss: 0.35099\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22592, Validation loss: 0.08480\n",
      "[2] Training loss: 0.22591, Validation loss: 0.08485\n",
      "[3] Training loss: 0.22589, Validation loss: 0.08491\n",
      "[4] Training loss: 0.22587, Validation loss: 0.08498\n",
      "[5] Training loss: 0.22585, Validation loss: 0.08506\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10543, Validation loss: 0.57639\n",
      "[2] Training loss: 0.10543, Validation loss: 0.57657\n",
      "[3] Training loss: 0.10541, Validation loss: 0.57676\n",
      "[4] Training loss: 0.10536, Validation loss: 0.57695\n",
      "[5] Training loss: 0.10529, Validation loss: 0.57714\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20961, Validation loss: 0.46616\n",
      "[2] Training loss: 0.20960, Validation loss: 0.46572\n",
      "[3] Training loss: 0.20957, Validation loss: 0.46545\n",
      "[4] Training loss: 0.20952, Validation loss: 0.46531\n",
      "[5] Training loss: 0.20944, Validation loss: 0.46530\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21758, Validation loss: 0.14590\n",
      "[2] Training loss: 0.21763, Validation loss: 0.14583\n",
      "[3] Training loss: 0.21764, Validation loss: 0.14582\n",
      "[4] Training loss: 0.21763, Validation loss: 0.14586\n",
      "[5] Training loss: 0.21758, Validation loss: 0.14595\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22967, Validation loss: 0.35013\n",
      "[2] Training loss: 0.22967, Validation loss: 0.35023\n",
      "[3] Training loss: 0.22966, Validation loss: 0.35032\n",
      "[4] Training loss: 0.22964, Validation loss: 0.35039\n",
      "[5] Training loss: 0.22962, Validation loss: 0.35044\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22584, Validation loss: 0.08454\n",
      "[2] Training loss: 0.22583, Validation loss: 0.08457\n",
      "[3] Training loss: 0.22581, Validation loss: 0.08462\n",
      "[4] Training loss: 0.22579, Validation loss: 0.08468\n",
      "[5] Training loss: 0.22577, Validation loss: 0.08474\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10530, Validation loss: 0.57721\n",
      "[2] Training loss: 0.10530, Validation loss: 0.57739\n",
      "[3] Training loss: 0.10528, Validation loss: 0.57758\n",
      "[4] Training loss: 0.10523, Validation loss: 0.57776\n",
      "[5] Training loss: 0.10516, Validation loss: 0.57794\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20988, Validation loss: 0.46453\n",
      "[2] Training loss: 0.20986, Validation loss: 0.46432\n",
      "[3] Training loss: 0.20977, Validation loss: 0.46431\n",
      "[4] Training loss: 0.20965, Validation loss: 0.46450\n",
      "[5] Training loss: 0.20951, Validation loss: 0.46479\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21753, Validation loss: 0.14589\n",
      "[2] Training loss: 0.21759, Validation loss: 0.14582\n",
      "[3] Training loss: 0.21761, Validation loss: 0.14579\n",
      "[4] Training loss: 0.21760, Validation loss: 0.14583\n",
      "[5] Training loss: 0.21756, Validation loss: 0.14590\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22967, Validation loss: 0.35066\n",
      "[2] Training loss: 0.22969, Validation loss: 0.35083\n",
      "[3] Training loss: 0.22969, Validation loss: 0.35096\n",
      "[4] Training loss: 0.22968, Validation loss: 0.35107\n",
      "[5] Training loss: 0.22966, Validation loss: 0.35115\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22585, Validation loss: 0.08488\n",
      "[2] Training loss: 0.22584, Validation loss: 0.08494\n",
      "[3] Training loss: 0.22582, Validation loss: 0.08501\n",
      "[4] Training loss: 0.22580, Validation loss: 0.08508\n",
      "[5] Training loss: 0.22578, Validation loss: 0.08517\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10537, Validation loss: 0.57606\n",
      "[2] Training loss: 0.10537, Validation loss: 0.57624\n",
      "[3] Training loss: 0.10535, Validation loss: 0.57642\n",
      "[4] Training loss: 0.10530, Validation loss: 0.57661\n",
      "[5] Training loss: 0.10523, Validation loss: 0.57680\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20952, Validation loss: 0.46576\n",
      "[2] Training loss: 0.20951, Validation loss: 0.46531\n",
      "[3] Training loss: 0.20948, Validation loss: 0.46503\n",
      "[4] Training loss: 0.20943, Validation loss: 0.46487\n",
      "[5] Training loss: 0.20936, Validation loss: 0.46481\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21752, Validation loss: 0.14582\n",
      "[2] Training loss: 0.21757, Validation loss: 0.14576\n",
      "[3] Training loss: 0.21758, Validation loss: 0.14575\n",
      "[4] Training loss: 0.21756, Validation loss: 0.14580\n",
      "[5] Training loss: 0.21752, Validation loss: 0.14589\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22961, Validation loss: 0.35028\n",
      "[2] Training loss: 0.22962, Validation loss: 0.35037\n",
      "[3] Training loss: 0.22961, Validation loss: 0.35045\n",
      "[4] Training loss: 0.22959, Validation loss: 0.35051\n",
      "[5] Training loss: 0.22956, Validation loss: 0.35056\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22577, Validation loss: 0.08450\n",
      "[2] Training loss: 0.22576, Validation loss: 0.08453\n",
      "[3] Training loss: 0.22574, Validation loss: 0.08457\n",
      "[4] Training loss: 0.22572, Validation loss: 0.08462\n",
      "[5] Training loss: 0.22570, Validation loss: 0.08469\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10524, Validation loss: 0.57726\n",
      "[2] Training loss: 0.10524, Validation loss: 0.57745\n",
      "[3] Training loss: 0.10521, Validation loss: 0.57764\n",
      "[4] Training loss: 0.10516, Validation loss: 0.57783\n",
      "[5] Training loss: 0.10510, Validation loss: 0.57802\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20980, Validation loss: 0.46366\n",
      "[2] Training loss: 0.20979, Validation loss: 0.46341\n",
      "[3] Training loss: 0.20973, Validation loss: 0.46338\n",
      "[4] Training loss: 0.20963, Validation loss: 0.46354\n",
      "[5] Training loss: 0.20950, Validation loss: 0.46386\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21745, Validation loss: 0.14585\n",
      "[2] Training loss: 0.21750, Validation loss: 0.14578\n",
      "[3] Training loss: 0.21753, Validation loss: 0.14576\n",
      "[4] Training loss: 0.21751, Validation loss: 0.14579\n",
      "[5] Training loss: 0.21747, Validation loss: 0.14587\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22958, Validation loss: 0.35057\n",
      "[2] Training loss: 0.22960, Validation loss: 0.35072\n",
      "[3] Training loss: 0.22959, Validation loss: 0.35086\n",
      "[4] Training loss: 0.22958, Validation loss: 0.35096\n",
      "[5] Training loss: 0.22956, Validation loss: 0.35104\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22577, Validation loss: 0.08445\n",
      "[2] Training loss: 0.22576, Validation loss: 0.08450\n",
      "[3] Training loss: 0.22575, Validation loss: 0.08456\n",
      "[4] Training loss: 0.22573, Validation loss: 0.08463\n",
      "[5] Training loss: 0.22571, Validation loss: 0.08472\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10526, Validation loss: 0.57637\n",
      "[2] Training loss: 0.10526, Validation loss: 0.57655\n",
      "[3] Training loss: 0.10523, Validation loss: 0.57673\n",
      "[4] Training loss: 0.10519, Validation loss: 0.57691\n",
      "[5] Training loss: 0.10512, Validation loss: 0.57709\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20954, Validation loss: 0.46534\n",
      "[2] Training loss: 0.20954, Validation loss: 0.46492\n",
      "[3] Training loss: 0.20950, Validation loss: 0.46465\n",
      "[4] Training loss: 0.20944, Validation loss: 0.46452\n",
      "[5] Training loss: 0.20936, Validation loss: 0.46452\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21743, Validation loss: 0.14578\n",
      "[2] Training loss: 0.21749, Validation loss: 0.14571\n",
      "[3] Training loss: 0.21750, Validation loss: 0.14570\n",
      "[4] Training loss: 0.21749, Validation loss: 0.14574\n",
      "[5] Training loss: 0.21744, Validation loss: 0.14583\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22954, Validation loss: 0.35034\n",
      "[2] Training loss: 0.22954, Validation loss: 0.35044\n",
      "[3] Training loss: 0.22954, Validation loss: 0.35053\n",
      "[4] Training loss: 0.22952, Validation loss: 0.35059\n",
      "[5] Training loss: 0.22949, Validation loss: 0.35063\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22571, Validation loss: 0.08445\n",
      "[2] Training loss: 0.22570, Validation loss: 0.08449\n",
      "[3] Training loss: 0.22568, Validation loss: 0.08455\n",
      "[4] Training loss: 0.22566, Validation loss: 0.08462\n",
      "[5] Training loss: 0.22564, Validation loss: 0.08469\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10517, Validation loss: 0.57705\n",
      "[2] Training loss: 0.10517, Validation loss: 0.57723\n",
      "[3] Training loss: 0.10514, Validation loss: 0.57742\n",
      "[4] Training loss: 0.10510, Validation loss: 0.57761\n",
      "[5] Training loss: 0.10503, Validation loss: 0.57780\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20971, Validation loss: 0.46373\n",
      "[2] Training loss: 0.20969, Validation loss: 0.46352\n",
      "[3] Training loss: 0.20962, Validation loss: 0.46351\n",
      "[4] Training loss: 0.20951, Validation loss: 0.46369\n",
      "[5] Training loss: 0.20937, Validation loss: 0.46398\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21739, Validation loss: 0.14575\n",
      "[2] Training loss: 0.21745, Validation loss: 0.14568\n",
      "[3] Training loss: 0.21747, Validation loss: 0.14566\n",
      "[4] Training loss: 0.21746, Validation loss: 0.14569\n",
      "[5] Training loss: 0.21742, Validation loss: 0.14577\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22954, Validation loss: 0.35075\n",
      "[2] Training loss: 0.22955, Validation loss: 0.35090\n",
      "[3] Training loss: 0.22955, Validation loss: 0.35102\n",
      "[4] Training loss: 0.22954, Validation loss: 0.35111\n",
      "[5] Training loss: 0.22952, Validation loss: 0.35118\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22571, Validation loss: 0.08467\n",
      "[2] Training loss: 0.22570, Validation loss: 0.08472\n",
      "[3] Training loss: 0.22569, Validation loss: 0.08478\n",
      "[4] Training loss: 0.22567, Validation loss: 0.08486\n",
      "[5] Training loss: 0.22565, Validation loss: 0.08494\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10522, Validation loss: 0.57614\n",
      "[2] Training loss: 0.10522, Validation loss: 0.57632\n",
      "[3] Training loss: 0.10519, Validation loss: 0.57651\n",
      "[4] Training loss: 0.10514, Validation loss: 0.57670\n",
      "[5] Training loss: 0.10507, Validation loss: 0.57689\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20943, Validation loss: 0.46493\n",
      "[2] Training loss: 0.20943, Validation loss: 0.46451\n",
      "[3] Training loss: 0.20940, Validation loss: 0.46423\n",
      "[4] Training loss: 0.20934, Validation loss: 0.46410\n",
      "[5] Training loss: 0.20927, Validation loss: 0.46408\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21738, Validation loss: 0.14568\n",
      "[2] Training loss: 0.21743, Validation loss: 0.14561\n",
      "[3] Training loss: 0.21744, Validation loss: 0.14561\n",
      "[4] Training loss: 0.21743, Validation loss: 0.14565\n",
      "[5] Training loss: 0.21738, Validation loss: 0.14574\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22948, Validation loss: 0.35030\n",
      "[2] Training loss: 0.22949, Validation loss: 0.35041\n",
      "[3] Training loss: 0.22948, Validation loss: 0.35049\n",
      "[4] Training loss: 0.22946, Validation loss: 0.35056\n",
      "[5] Training loss: 0.22943, Validation loss: 0.35061\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22563, Validation loss: 0.08460\n",
      "[2] Training loss: 0.22562, Validation loss: 0.08463\n",
      "[3] Training loss: 0.22560, Validation loss: 0.08469\n",
      "[4] Training loss: 0.22558, Validation loss: 0.08475\n",
      "[5] Training loss: 0.22556, Validation loss: 0.08482\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10508, Validation loss: 0.57676\n",
      "[2] Training loss: 0.10508, Validation loss: 0.57694\n",
      "[3] Training loss: 0.10506, Validation loss: 0.57713\n",
      "[4] Training loss: 0.10501, Validation loss: 0.57731\n",
      "[5] Training loss: 0.10495, Validation loss: 0.57749\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20969, Validation loss: 0.46329\n",
      "[2] Training loss: 0.20969, Validation loss: 0.46305\n",
      "[3] Training loss: 0.20963, Validation loss: 0.46302\n",
      "[4] Training loss: 0.20952, Validation loss: 0.46318\n",
      "[5] Training loss: 0.20939, Validation loss: 0.46348\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21731, Validation loss: 0.14573\n",
      "[2] Training loss: 0.21736, Validation loss: 0.14566\n",
      "[3] Training loss: 0.21739, Validation loss: 0.14564\n",
      "[4] Training loss: 0.21737, Validation loss: 0.14568\n",
      "[5] Training loss: 0.21733, Validation loss: 0.14576\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22946, Validation loss: 0.35084\n",
      "[2] Training loss: 0.22947, Validation loss: 0.35099\n",
      "[3] Training loss: 0.22947, Validation loss: 0.35111\n",
      "[4] Training loss: 0.22946, Validation loss: 0.35120\n",
      "[5] Training loss: 0.22944, Validation loss: 0.35127\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22564, Validation loss: 0.08440\n",
      "[2] Training loss: 0.22563, Validation loss: 0.08444\n",
      "[3] Training loss: 0.22561, Validation loss: 0.08450\n",
      "[4] Training loss: 0.22560, Validation loss: 0.08457\n",
      "[5] Training loss: 0.22558, Validation loss: 0.08464\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10513, Validation loss: 0.57607\n",
      "[2] Training loss: 0.10512, Validation loss: 0.57626\n",
      "[3] Training loss: 0.10510, Validation loss: 0.57645\n",
      "[4] Training loss: 0.10505, Validation loss: 0.57665\n",
      "[5] Training loss: 0.10498, Validation loss: 0.57684\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20942, Validation loss: 0.46468\n",
      "[2] Training loss: 0.20941, Validation loss: 0.46425\n",
      "[3] Training loss: 0.20938, Validation loss: 0.46399\n",
      "[4] Training loss: 0.20931, Validation loss: 0.46385\n",
      "[5] Training loss: 0.20923, Validation loss: 0.46383\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21730, Validation loss: 0.14564\n",
      "[2] Training loss: 0.21735, Validation loss: 0.14557\n",
      "[3] Training loss: 0.21737, Validation loss: 0.14556\n",
      "[4] Training loss: 0.21735, Validation loss: 0.14560\n",
      "[5] Training loss: 0.21731, Validation loss: 0.14568\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22942, Validation loss: 0.35047\n",
      "[2] Training loss: 0.22943, Validation loss: 0.35057\n",
      "[3] Training loss: 0.22942, Validation loss: 0.35066\n",
      "[4] Training loss: 0.22940, Validation loss: 0.35072\n",
      "[5] Training loss: 0.22938, Validation loss: 0.35076\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22557, Validation loss: 0.08445\n",
      "[2] Training loss: 0.22556, Validation loss: 0.08449\n",
      "[3] Training loss: 0.22554, Validation loss: 0.08455\n",
      "[4] Training loss: 0.22553, Validation loss: 0.08462\n",
      "[5] Training loss: 0.22550, Validation loss: 0.08469\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10503, Validation loss: 0.57672\n",
      "[2] Training loss: 0.10503, Validation loss: 0.57690\n",
      "[3] Training loss: 0.10500, Validation loss: 0.57709\n",
      "[4] Training loss: 0.10496, Validation loss: 0.57727\n",
      "[5] Training loss: 0.10489, Validation loss: 0.57746\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20957, Validation loss: 0.46293\n",
      "[2] Training loss: 0.20957, Validation loss: 0.46270\n",
      "[3] Training loss: 0.20950, Validation loss: 0.46267\n",
      "[4] Training loss: 0.20940, Validation loss: 0.46283\n",
      "[5] Training loss: 0.20927, Validation loss: 0.46310\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21725, Validation loss: 0.14563\n",
      "[2] Training loss: 0.21731, Validation loss: 0.14556\n",
      "[3] Training loss: 0.21733, Validation loss: 0.14554\n",
      "[4] Training loss: 0.21732, Validation loss: 0.14558\n",
      "[5] Training loss: 0.21727, Validation loss: 0.14566\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22941, Validation loss: 0.35082\n",
      "[2] Training loss: 0.22942, Validation loss: 0.35097\n",
      "[3] Training loss: 0.22941, Validation loss: 0.35109\n",
      "[4] Training loss: 0.22940, Validation loss: 0.35119\n",
      "[5] Training loss: 0.22938, Validation loss: 0.35126\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22557, Validation loss: 0.08446\n",
      "[2] Training loss: 0.22556, Validation loss: 0.08451\n",
      "[3] Training loss: 0.22554, Validation loss: 0.08457\n",
      "[4] Training loss: 0.22553, Validation loss: 0.08464\n",
      "[5] Training loss: 0.22551, Validation loss: 0.08472\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10505, Validation loss: 0.57606\n",
      "[2] Training loss: 0.10505, Validation loss: 0.57624\n",
      "[3] Training loss: 0.10502, Validation loss: 0.57643\n",
      "[4] Training loss: 0.10497, Validation loss: 0.57662\n",
      "[5] Training loss: 0.10490, Validation loss: 0.57680\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20936, Validation loss: 0.46414\n",
      "[2] Training loss: 0.20936, Validation loss: 0.46373\n",
      "[3] Training loss: 0.20933, Validation loss: 0.46346\n",
      "[4] Training loss: 0.20927, Validation loss: 0.46334\n",
      "[5] Training loss: 0.20920, Validation loss: 0.46333\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21722, Validation loss: 0.14559\n",
      "[2] Training loss: 0.21728, Validation loss: 0.14552\n",
      "[3] Training loss: 0.21729, Validation loss: 0.14551\n",
      "[4] Training loss: 0.21727, Validation loss: 0.14556\n",
      "[5] Training loss: 0.21723, Validation loss: 0.14565\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22934, Validation loss: 0.35046\n",
      "[2] Training loss: 0.22934, Validation loss: 0.35057\n",
      "[3] Training loss: 0.22934, Validation loss: 0.35065\n",
      "[4] Training loss: 0.22932, Validation loss: 0.35072\n",
      "[5] Training loss: 0.22929, Validation loss: 0.35077\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22550, Validation loss: 0.08432\n",
      "[2] Training loss: 0.22548, Validation loss: 0.08435\n",
      "[3] Training loss: 0.22547, Validation loss: 0.08440\n",
      "[4] Training loss: 0.22545, Validation loss: 0.08447\n",
      "[5] Training loss: 0.22543, Validation loss: 0.08454\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10492, Validation loss: 0.57671\n",
      "[2] Training loss: 0.10492, Validation loss: 0.57689\n",
      "[3] Training loss: 0.10490, Validation loss: 0.57707\n",
      "[4] Training loss: 0.10485, Validation loss: 0.57726\n",
      "[5] Training loss: 0.10479, Validation loss: 0.57744\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20964, Validation loss: 0.46264\n",
      "[2] Training loss: 0.20962, Validation loss: 0.46243\n",
      "[3] Training loss: 0.20952, Validation loss: 0.46244\n",
      "[4] Training loss: 0.20940, Validation loss: 0.46261\n",
      "[5] Training loss: 0.20925, Validation loss: 0.46289\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21718, Validation loss: 0.14558\n",
      "[2] Training loss: 0.21724, Validation loss: 0.14550\n",
      "[3] Training loss: 0.21727, Validation loss: 0.14548\n",
      "[4] Training loss: 0.21726, Validation loss: 0.14551\n",
      "[5] Training loss: 0.21722, Validation loss: 0.14559\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22936, Validation loss: 0.35102\n",
      "[2] Training loss: 0.22937, Validation loss: 0.35118\n",
      "[3] Training loss: 0.22937, Validation loss: 0.35131\n",
      "[4] Training loss: 0.22936, Validation loss: 0.35141\n",
      "[5] Training loss: 0.22934, Validation loss: 0.35147\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22551, Validation loss: 0.08475\n",
      "[2] Training loss: 0.22549, Validation loss: 0.08481\n",
      "[3] Training loss: 0.22548, Validation loss: 0.08489\n",
      "[4] Training loss: 0.22546, Validation loss: 0.08497\n",
      "[5] Training loss: 0.22544, Validation loss: 0.08505\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10501, Validation loss: 0.57550\n",
      "[2] Training loss: 0.10501, Validation loss: 0.57568\n",
      "[3] Training loss: 0.10499, Validation loss: 0.57587\n",
      "[4] Training loss: 0.10493, Validation loss: 0.57606\n",
      "[5] Training loss: 0.10486, Validation loss: 0.57626\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20925, Validation loss: 0.46373\n",
      "[2] Training loss: 0.20925, Validation loss: 0.46331\n",
      "[3] Training loss: 0.20922, Validation loss: 0.46304\n",
      "[4] Training loss: 0.20916, Validation loss: 0.46290\n",
      "[5] Training loss: 0.20909, Validation loss: 0.46286\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21718, Validation loss: 0.14549\n",
      "[2] Training loss: 0.21723, Validation loss: 0.14542\n",
      "[3] Training loss: 0.21724, Validation loss: 0.14541\n",
      "[4] Training loss: 0.21722, Validation loss: 0.14546\n",
      "[5] Training loss: 0.21718, Validation loss: 0.14555\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22931, Validation loss: 0.35055\n",
      "[2] Training loss: 0.22931, Validation loss: 0.35064\n",
      "[3] Training loss: 0.22930, Validation loss: 0.35073\n",
      "[4] Training loss: 0.22928, Validation loss: 0.35078\n",
      "[5] Training loss: 0.22925, Validation loss: 0.35083\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22542, Validation loss: 0.08448\n",
      "[2] Training loss: 0.22541, Validation loss: 0.08451\n",
      "[3] Training loss: 0.22539, Validation loss: 0.08456\n",
      "[4] Training loss: 0.22537, Validation loss: 0.08461\n",
      "[5] Training loss: 0.22535, Validation loss: 0.08468\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10487, Validation loss: 0.57644\n",
      "[2] Training loss: 0.10487, Validation loss: 0.57663\n",
      "[3] Training loss: 0.10484, Validation loss: 0.57682\n",
      "[4] Training loss: 0.10479, Validation loss: 0.57701\n",
      "[5] Training loss: 0.10473, Validation loss: 0.57719\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20952, Validation loss: 0.46181\n",
      "[2] Training loss: 0.20951, Validation loss: 0.46154\n",
      "[3] Training loss: 0.20946, Validation loss: 0.46149\n",
      "[4] Training loss: 0.20936, Validation loss: 0.46162\n",
      "[5] Training loss: 0.20923, Validation loss: 0.46187\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21710, Validation loss: 0.14555\n",
      "[2] Training loss: 0.21715, Validation loss: 0.14548\n",
      "[3] Training loss: 0.21717, Validation loss: 0.14547\n",
      "[4] Training loss: 0.21716, Validation loss: 0.14550\n",
      "[5] Training loss: 0.21712, Validation loss: 0.14559\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22926, Validation loss: 0.35091\n",
      "[2] Training loss: 0.22927, Validation loss: 0.35106\n",
      "[3] Training loss: 0.22927, Validation loss: 0.35119\n",
      "[4] Training loss: 0.22926, Validation loss: 0.35129\n",
      "[5] Training loss: 0.22924, Validation loss: 0.35136\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22542, Validation loss: 0.08411\n",
      "[2] Training loss: 0.22541, Validation loss: 0.08415\n",
      "[3] Training loss: 0.22540, Validation loss: 0.08420\n",
      "[4] Training loss: 0.22538, Validation loss: 0.08427\n",
      "[5] Training loss: 0.22536, Validation loss: 0.08435\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10488, Validation loss: 0.57606\n",
      "[2] Training loss: 0.10488, Validation loss: 0.57625\n",
      "[3] Training loss: 0.10486, Validation loss: 0.57644\n",
      "[4] Training loss: 0.10481, Validation loss: 0.57663\n",
      "[5] Training loss: 0.10474, Validation loss: 0.57682\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20932, Validation loss: 0.46290\n",
      "[2] Training loss: 0.20932, Validation loss: 0.46252\n",
      "[3] Training loss: 0.20928, Validation loss: 0.46230\n",
      "[4] Training loss: 0.20921, Validation loss: 0.46219\n",
      "[5] Training loss: 0.20912, Validation loss: 0.46220\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21708, Validation loss: 0.14547\n",
      "[2] Training loss: 0.21714, Validation loss: 0.14540\n",
      "[3] Training loss: 0.21716, Validation loss: 0.14538\n",
      "[4] Training loss: 0.21714, Validation loss: 0.14542\n",
      "[5] Training loss: 0.21710, Validation loss: 0.14551\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22922, Validation loss: 0.35053\n",
      "[2] Training loss: 0.22923, Validation loss: 0.35063\n",
      "[3] Training loss: 0.22922, Validation loss: 0.35072\n",
      "[4] Training loss: 0.22921, Validation loss: 0.35079\n",
      "[5] Training loss: 0.22918, Validation loss: 0.35084\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22536, Validation loss: 0.08463\n",
      "[2] Training loss: 0.22534, Validation loss: 0.08469\n",
      "[3] Training loss: 0.22533, Validation loss: 0.08475\n",
      "[4] Training loss: 0.22531, Validation loss: 0.08483\n",
      "[5] Training loss: 0.22529, Validation loss: 0.08491\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10480, Validation loss: 0.57613\n",
      "[2] Training loss: 0.10480, Validation loss: 0.57631\n",
      "[3] Training loss: 0.10477, Validation loss: 0.57648\n",
      "[4] Training loss: 0.10473, Validation loss: 0.57665\n",
      "[5] Training loss: 0.10466, Validation loss: 0.57682\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20943, Validation loss: 0.46161\n",
      "[2] Training loss: 0.20943, Validation loss: 0.46137\n",
      "[3] Training loss: 0.20937, Validation loss: 0.46133\n",
      "[4] Training loss: 0.20928, Validation loss: 0.46145\n",
      "[5] Training loss: 0.20915, Validation loss: 0.46170\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21703, Validation loss: 0.14549\n",
      "[2] Training loss: 0.21709, Validation loss: 0.14542\n",
      "[3] Training loss: 0.21711, Validation loss: 0.14541\n",
      "[4] Training loss: 0.21709, Validation loss: 0.14545\n",
      "[5] Training loss: 0.21705, Validation loss: 0.14553\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22920, Validation loss: 0.35117\n",
      "[2] Training loss: 0.22921, Validation loss: 0.35133\n",
      "[3] Training loss: 0.22920, Validation loss: 0.35146\n",
      "[4] Training loss: 0.22919, Validation loss: 0.35155\n",
      "[5] Training loss: 0.22917, Validation loss: 0.35161\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22536, Validation loss: 0.08403\n",
      "[2] Training loss: 0.22535, Validation loss: 0.08406\n",
      "[3] Training loss: 0.22534, Validation loss: 0.08410\n",
      "[4] Training loss: 0.22532, Validation loss: 0.08416\n",
      "[5] Training loss: 0.22530, Validation loss: 0.08423\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10482, Validation loss: 0.57596\n",
      "[2] Training loss: 0.10482, Validation loss: 0.57616\n",
      "[3] Training loss: 0.10479, Validation loss: 0.57637\n",
      "[4] Training loss: 0.10474, Validation loss: 0.57658\n",
      "[5] Training loss: 0.10467, Validation loss: 0.57678\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20925, Validation loss: 0.46242\n",
      "[2] Training loss: 0.20925, Validation loss: 0.46201\n",
      "[3] Training loss: 0.20921, Validation loss: 0.46177\n",
      "[4] Training loss: 0.20915, Validation loss: 0.46166\n",
      "[5] Training loss: 0.20906, Validation loss: 0.46167\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21702, Validation loss: 0.14539\n",
      "[2] Training loss: 0.21707, Validation loss: 0.14532\n",
      "[3] Training loss: 0.21709, Validation loss: 0.14531\n",
      "[4] Training loss: 0.21708, Validation loss: 0.14535\n",
      "[5] Training loss: 0.21703, Validation loss: 0.14543\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22916, Validation loss: 0.35045\n",
      "[2] Training loss: 0.22916, Validation loss: 0.35055\n",
      "[3] Training loss: 0.22916, Validation loss: 0.35064\n",
      "[4] Training loss: 0.22914, Validation loss: 0.35070\n",
      "[5] Training loss: 0.22911, Validation loss: 0.35075\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22529, Validation loss: 0.08454\n",
      "[2] Training loss: 0.22527, Validation loss: 0.08460\n",
      "[3] Training loss: 0.22526, Validation loss: 0.08466\n",
      "[4] Training loss: 0.22524, Validation loss: 0.08474\n",
      "[5] Training loss: 0.22522, Validation loss: 0.08482\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10471, Validation loss: 0.57610\n",
      "[2] Training loss: 0.10471, Validation loss: 0.57626\n",
      "[3] Training loss: 0.10469, Validation loss: 0.57643\n",
      "[4] Training loss: 0.10464, Validation loss: 0.57660\n",
      "[5] Training loss: 0.10458, Validation loss: 0.57677\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20941, Validation loss: 0.46123\n",
      "[2] Training loss: 0.20941, Validation loss: 0.46101\n",
      "[3] Training loss: 0.20934, Validation loss: 0.46100\n",
      "[4] Training loss: 0.20923, Validation loss: 0.46116\n",
      "[5] Training loss: 0.20909, Validation loss: 0.46143\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21697, Validation loss: 0.14542\n",
      "[2] Training loss: 0.21703, Validation loss: 0.14534\n",
      "[3] Training loss: 0.21705, Validation loss: 0.14533\n",
      "[4] Training loss: 0.21703, Validation loss: 0.14537\n",
      "[5] Training loss: 0.21699, Validation loss: 0.14545\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22914, Validation loss: 0.35127\n",
      "[2] Training loss: 0.22915, Validation loss: 0.35144\n",
      "[3] Training loss: 0.22915, Validation loss: 0.35157\n",
      "[4] Training loss: 0.22914, Validation loss: 0.35166\n",
      "[5] Training loss: 0.22912, Validation loss: 0.35173\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22530, Validation loss: 0.08415\n",
      "[2] Training loss: 0.22529, Validation loss: 0.08418\n",
      "[3] Training loss: 0.22527, Validation loss: 0.08423\n",
      "[4] Training loss: 0.22526, Validation loss: 0.08430\n",
      "[5] Training loss: 0.22523, Validation loss: 0.08437\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10476, Validation loss: 0.57559\n",
      "[2] Training loss: 0.10476, Validation loss: 0.57579\n",
      "[3] Training loss: 0.10473, Validation loss: 0.57600\n",
      "[4] Training loss: 0.10468, Validation loss: 0.57620\n",
      "[5] Training loss: 0.10461, Validation loss: 0.57641\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20916, Validation loss: 0.46229\n",
      "[2] Training loss: 0.20915, Validation loss: 0.46188\n",
      "[3] Training loss: 0.20912, Validation loss: 0.46163\n",
      "[4] Training loss: 0.20906, Validation loss: 0.46151\n",
      "[5] Training loss: 0.20899, Validation loss: 0.46148\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21696, Validation loss: 0.14532\n",
      "[2] Training loss: 0.21701, Validation loss: 0.14525\n",
      "[3] Training loss: 0.21703, Validation loss: 0.14524\n",
      "[4] Training loss: 0.21701, Validation loss: 0.14528\n",
      "[5] Training loss: 0.21696, Validation loss: 0.14537\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22909, Validation loss: 0.35049\n",
      "[2] Training loss: 0.22910, Validation loss: 0.35059\n",
      "[3] Training loss: 0.22909, Validation loss: 0.35067\n",
      "[4] Training loss: 0.22907, Validation loss: 0.35073\n",
      "[5] Training loss: 0.22905, Validation loss: 0.35078\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22521, Validation loss: 0.08442\n",
      "[2] Training loss: 0.22520, Validation loss: 0.08446\n",
      "[3] Training loss: 0.22518, Validation loss: 0.08452\n",
      "[4] Training loss: 0.22516, Validation loss: 0.08458\n",
      "[5] Training loss: 0.22514, Validation loss: 0.08466\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10462, Validation loss: 0.57615\n",
      "[2] Training loss: 0.10462, Validation loss: 0.57632\n",
      "[3] Training loss: 0.10460, Validation loss: 0.57649\n",
      "[4] Training loss: 0.10456, Validation loss: 0.57666\n",
      "[5] Training loss: 0.10449, Validation loss: 0.57682\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20942, Validation loss: 0.46078\n",
      "[2] Training loss: 0.20940, Validation loss: 0.46057\n",
      "[3] Training loss: 0.20932, Validation loss: 0.46056\n",
      "[4] Training loss: 0.20920, Validation loss: 0.46072\n",
      "[5] Training loss: 0.20906, Validation loss: 0.46099\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21690, Validation loss: 0.14535\n",
      "[2] Training loss: 0.21696, Validation loss: 0.14528\n",
      "[3] Training loss: 0.21698, Validation loss: 0.14526\n",
      "[4] Training loss: 0.21697, Validation loss: 0.14530\n",
      "[5] Training loss: 0.21693, Validation loss: 0.14538\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22909, Validation loss: 0.35138\n",
      "[2] Training loss: 0.22910, Validation loss: 0.35155\n",
      "[3] Training loss: 0.22910, Validation loss: 0.35169\n",
      "[4] Training loss: 0.22909, Validation loss: 0.35179\n",
      "[5] Training loss: 0.22907, Validation loss: 0.35186\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22523, Validation loss: 0.08434\n",
      "[2] Training loss: 0.22522, Validation loss: 0.08439\n",
      "[3] Training loss: 0.22520, Validation loss: 0.08445\n",
      "[4] Training loss: 0.22518, Validation loss: 0.08452\n",
      "[5] Training loss: 0.22516, Validation loss: 0.08460\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10471, Validation loss: 0.57521\n",
      "[2] Training loss: 0.10471, Validation loss: 0.57540\n",
      "[3] Training loss: 0.10468, Validation loss: 0.57560\n",
      "[4] Training loss: 0.10463, Validation loss: 0.57581\n",
      "[5] Training loss: 0.10456, Validation loss: 0.57601\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20908, Validation loss: 0.46182\n",
      "[2] Training loss: 0.20907, Validation loss: 0.46140\n",
      "[3] Training loss: 0.20904, Validation loss: 0.46111\n",
      "[4] Training loss: 0.20899, Validation loss: 0.46096\n",
      "[5] Training loss: 0.20892, Validation loss: 0.46090\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21689, Validation loss: 0.14525\n",
      "[2] Training loss: 0.21695, Validation loss: 0.14519\n",
      "[3] Training loss: 0.21696, Validation loss: 0.14518\n",
      "[4] Training loss: 0.21694, Validation loss: 0.14522\n",
      "[5] Training loss: 0.21690, Validation loss: 0.14531\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22904, Validation loss: 0.35066\n",
      "[2] Training loss: 0.22904, Validation loss: 0.35075\n",
      "[3] Training loss: 0.22903, Validation loss: 0.35083\n",
      "[4] Training loss: 0.22902, Validation loss: 0.35089\n",
      "[5] Training loss: 0.22899, Validation loss: 0.35093\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22514, Validation loss: 0.08424\n",
      "[2] Training loss: 0.22512, Validation loss: 0.08427\n",
      "[3] Training loss: 0.22511, Validation loss: 0.08432\n",
      "[4] Training loss: 0.22509, Validation loss: 0.08438\n",
      "[5] Training loss: 0.22507, Validation loss: 0.08444\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10456, Validation loss: 0.57622\n",
      "[2] Training loss: 0.10456, Validation loss: 0.57640\n",
      "[3] Training loss: 0.10453, Validation loss: 0.57658\n",
      "[4] Training loss: 0.10449, Validation loss: 0.57676\n",
      "[5] Training loss: 0.10442, Validation loss: 0.57693\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20935, Validation loss: 0.45990\n",
      "[2] Training loss: 0.20934, Validation loss: 0.45967\n",
      "[3] Training loss: 0.20928, Validation loss: 0.45963\n",
      "[4] Training loss: 0.20917, Validation loss: 0.45975\n",
      "[5] Training loss: 0.20904, Validation loss: 0.45999\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21682, Validation loss: 0.14531\n",
      "[2] Training loss: 0.21688, Validation loss: 0.14524\n",
      "[3] Training loss: 0.21690, Validation loss: 0.14522\n",
      "[4] Training loss: 0.21689, Validation loss: 0.14526\n",
      "[5] Training loss: 0.21684, Validation loss: 0.14534\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22901, Validation loss: 0.35121\n",
      "[2] Training loss: 0.22902, Validation loss: 0.35137\n",
      "[3] Training loss: 0.22901, Validation loss: 0.35151\n",
      "[4] Training loss: 0.22900, Validation loss: 0.35161\n",
      "[5] Training loss: 0.22898, Validation loss: 0.35169\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22515, Validation loss: 0.08405\n",
      "[2] Training loss: 0.22514, Validation loss: 0.08409\n",
      "[3] Training loss: 0.22512, Validation loss: 0.08415\n",
      "[4] Training loss: 0.22511, Validation loss: 0.08421\n",
      "[5] Training loss: 0.22509, Validation loss: 0.08429\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10459, Validation loss: 0.57550\n",
      "[2] Training loss: 0.10459, Validation loss: 0.57568\n",
      "[3] Training loss: 0.10457, Validation loss: 0.57587\n",
      "[4] Training loss: 0.10452, Validation loss: 0.57607\n",
      "[5] Training loss: 0.10445, Validation loss: 0.57626\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20910, Validation loss: 0.46091\n",
      "[2] Training loss: 0.20909, Validation loss: 0.46053\n",
      "[3] Training loss: 0.20906, Validation loss: 0.46027\n",
      "[4] Training loss: 0.20900, Validation loss: 0.46014\n",
      "[5] Training loss: 0.20892, Validation loss: 0.46012\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21681, Validation loss: 0.14523\n",
      "[2] Training loss: 0.21686, Validation loss: 0.14516\n",
      "[3] Training loss: 0.21688, Validation loss: 0.14515\n",
      "[4] Training loss: 0.21686, Validation loss: 0.14519\n",
      "[5] Training loss: 0.21681, Validation loss: 0.14528\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22895, Validation loss: 0.35063\n",
      "[2] Training loss: 0.22896, Validation loss: 0.35073\n",
      "[3] Training loss: 0.22895, Validation loss: 0.35081\n",
      "[4] Training loss: 0.22893, Validation loss: 0.35087\n",
      "[5] Training loss: 0.22891, Validation loss: 0.35092\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22507, Validation loss: 0.08432\n",
      "[2] Training loss: 0.22505, Validation loss: 0.08436\n",
      "[3] Training loss: 0.22504, Validation loss: 0.08442\n",
      "[4] Training loss: 0.22502, Validation loss: 0.08448\n",
      "[5] Training loss: 0.22500, Validation loss: 0.08456\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10447, Validation loss: 0.57609\n",
      "[2] Training loss: 0.10447, Validation loss: 0.57626\n",
      "[3] Training loss: 0.10445, Validation loss: 0.57643\n",
      "[4] Training loss: 0.10440, Validation loss: 0.57660\n",
      "[5] Training loss: 0.10434, Validation loss: 0.57677\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20936, Validation loss: 0.45949\n",
      "[2] Training loss: 0.20934, Validation loss: 0.45929\n",
      "[3] Training loss: 0.20925, Validation loss: 0.45927\n",
      "[4] Training loss: 0.20913, Validation loss: 0.45944\n",
      "[5] Training loss: 0.20898, Validation loss: 0.45974\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21676, Validation loss: 0.14524\n",
      "[2] Training loss: 0.21682, Validation loss: 0.14516\n",
      "[3] Training loss: 0.21684, Validation loss: 0.14514\n",
      "[4] Training loss: 0.21683, Validation loss: 0.14518\n",
      "[5] Training loss: 0.21679, Validation loss: 0.14526\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22896, Validation loss: 0.35157\n",
      "[2] Training loss: 0.22898, Validation loss: 0.35175\n",
      "[3] Training loss: 0.22898, Validation loss: 0.35189\n",
      "[4] Training loss: 0.22897, Validation loss: 0.35200\n",
      "[5] Training loss: 0.22895, Validation loss: 0.35207\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22509, Validation loss: 0.08426\n",
      "[2] Training loss: 0.22508, Validation loss: 0.08430\n",
      "[3] Training loss: 0.22507, Validation loss: 0.08436\n",
      "[4] Training loss: 0.22505, Validation loss: 0.08443\n",
      "[5] Training loss: 0.22503, Validation loss: 0.08451\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10456, Validation loss: 0.57496\n",
      "[2] Training loss: 0.10456, Validation loss: 0.57516\n",
      "[3] Training loss: 0.10453, Validation loss: 0.57536\n",
      "[4] Training loss: 0.10448, Validation loss: 0.57556\n",
      "[5] Training loss: 0.10441, Validation loss: 0.57576\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20896, Validation loss: 0.46080\n",
      "[2] Training loss: 0.20896, Validation loss: 0.46036\n",
      "[3] Training loss: 0.20893, Validation loss: 0.46006\n",
      "[4] Training loss: 0.20887, Validation loss: 0.45986\n",
      "[5] Training loss: 0.20880, Validation loss: 0.45977\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21676, Validation loss: 0.14512\n",
      "[2] Training loss: 0.21681, Validation loss: 0.14506\n",
      "[3] Training loss: 0.21683, Validation loss: 0.14505\n",
      "[4] Training loss: 0.21681, Validation loss: 0.14509\n",
      "[5] Training loss: 0.21676, Validation loss: 0.14518\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22892, Validation loss: 0.35073\n",
      "[2] Training loss: 0.22892, Validation loss: 0.35081\n",
      "[3] Training loss: 0.22891, Validation loss: 0.35088\n",
      "[4] Training loss: 0.22889, Validation loss: 0.35093\n",
      "[5] Training loss: 0.22887, Validation loss: 0.35097\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22500, Validation loss: 0.08419\n",
      "[2] Training loss: 0.22499, Validation loss: 0.08422\n",
      "[3] Training loss: 0.22497, Validation loss: 0.08427\n",
      "[4] Training loss: 0.22495, Validation loss: 0.08432\n",
      "[5] Training loss: 0.22493, Validation loss: 0.08439\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10442, Validation loss: 0.57626\n",
      "[2] Training loss: 0.10442, Validation loss: 0.57645\n",
      "[3] Training loss: 0.10439, Validation loss: 0.57663\n",
      "[4] Training loss: 0.10434, Validation loss: 0.57680\n",
      "[5] Training loss: 0.10428, Validation loss: 0.57698\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20925, Validation loss: 0.45862\n",
      "[2] Training loss: 0.20923, Validation loss: 0.45843\n",
      "[3] Training loss: 0.20916, Validation loss: 0.45845\n",
      "[4] Training loss: 0.20905, Validation loss: 0.45863\n",
      "[5] Training loss: 0.20892, Validation loss: 0.45894\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21668, Validation loss: 0.14521\n",
      "[2] Training loss: 0.21674, Validation loss: 0.14514\n",
      "[3] Training loss: 0.21676, Validation loss: 0.14512\n",
      "[4] Training loss: 0.21675, Validation loss: 0.14516\n",
      "[5] Training loss: 0.21671, Validation loss: 0.14525\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22890, Validation loss: 0.35172\n",
      "[2] Training loss: 0.22891, Validation loss: 0.35188\n",
      "[3] Training loss: 0.22891, Validation loss: 0.35201\n",
      "[4] Training loss: 0.22890, Validation loss: 0.35212\n",
      "[5] Training loss: 0.22888, Validation loss: 0.35218\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22502, Validation loss: 0.08410\n",
      "[2] Training loss: 0.22501, Validation loss: 0.08416\n",
      "[3] Training loss: 0.22500, Validation loss: 0.08422\n",
      "[4] Training loss: 0.22498, Validation loss: 0.08430\n",
      "[5] Training loss: 0.22496, Validation loss: 0.08438\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10446, Validation loss: 0.57488\n",
      "[2] Training loss: 0.10446, Validation loss: 0.57507\n",
      "[3] Training loss: 0.10443, Validation loss: 0.57527\n",
      "[4] Training loss: 0.10438, Validation loss: 0.57547\n",
      "[5] Training loss: 0.10431, Validation loss: 0.57567\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20895, Validation loss: 0.46024\n",
      "[2] Training loss: 0.20895, Validation loss: 0.45981\n",
      "[3] Training loss: 0.20892, Validation loss: 0.45950\n",
      "[4] Training loss: 0.20886, Validation loss: 0.45931\n",
      "[5] Training loss: 0.20879, Validation loss: 0.45921\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21667, Validation loss: 0.14512\n",
      "[2] Training loss: 0.21672, Validation loss: 0.14505\n",
      "[3] Training loss: 0.21674, Validation loss: 0.14504\n",
      "[4] Training loss: 0.21672, Validation loss: 0.14508\n",
      "[5] Training loss: 0.21667, Validation loss: 0.14517\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22884, Validation loss: 0.35075\n",
      "[2] Training loss: 0.22885, Validation loss: 0.35082\n",
      "[3] Training loss: 0.22884, Validation loss: 0.35089\n",
      "[4] Training loss: 0.22882, Validation loss: 0.35093\n",
      "[5] Training loss: 0.22879, Validation loss: 0.35096\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22493, Validation loss: 0.08399\n",
      "[2] Training loss: 0.22491, Validation loss: 0.08402\n",
      "[3] Training loss: 0.22490, Validation loss: 0.08406\n",
      "[4] Training loss: 0.22488, Validation loss: 0.08411\n",
      "[5] Training loss: 0.22486, Validation loss: 0.08418\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10433, Validation loss: 0.57639\n",
      "[2] Training loss: 0.10433, Validation loss: 0.57658\n",
      "[3] Training loss: 0.10430, Validation loss: 0.57676\n",
      "[4] Training loss: 0.10426, Validation loss: 0.57694\n",
      "[5] Training loss: 0.10419, Validation loss: 0.57711\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20926, Validation loss: 0.45812\n",
      "[2] Training loss: 0.20925, Validation loss: 0.45795\n",
      "[3] Training loss: 0.20917, Validation loss: 0.45798\n",
      "[4] Training loss: 0.20904, Validation loss: 0.45819\n",
      "[5] Training loss: 0.20889, Validation loss: 0.45852\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21662, Validation loss: 0.14513\n",
      "[2] Training loss: 0.21668, Validation loss: 0.14505\n",
      "[3] Training loss: 0.21670, Validation loss: 0.14503\n",
      "[4] Training loss: 0.21669, Validation loss: 0.14506\n",
      "[5] Training loss: 0.21665, Validation loss: 0.14514\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22884, Validation loss: 0.35167\n",
      "[2] Training loss: 0.22885, Validation loss: 0.35187\n",
      "[3] Training loss: 0.22886, Validation loss: 0.35202\n",
      "[4] Training loss: 0.22885, Validation loss: 0.35215\n",
      "[5] Training loss: 0.22883, Validation loss: 0.35224\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22495, Validation loss: 0.08427\n",
      "[2] Training loss: 0.22494, Validation loss: 0.08433\n",
      "[3] Training loss: 0.22493, Validation loss: 0.08440\n",
      "[4] Training loss: 0.22491, Validation loss: 0.08448\n",
      "[5] Training loss: 0.22489, Validation loss: 0.08456\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10439, Validation loss: 0.57461\n",
      "[2] Training loss: 0.10439, Validation loss: 0.57478\n",
      "[3] Training loss: 0.10437, Validation loss: 0.57496\n",
      "[4] Training loss: 0.10432, Validation loss: 0.57515\n",
      "[5] Training loss: 0.10425, Validation loss: 0.57534\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20889, Validation loss: 0.46014\n",
      "[2] Training loss: 0.20889, Validation loss: 0.45971\n",
      "[3] Training loss: 0.20886, Validation loss: 0.45942\n",
      "[4] Training loss: 0.20880, Validation loss: 0.45923\n",
      "[5] Training loss: 0.20873, Validation loss: 0.45915\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21661, Validation loss: 0.14503\n",
      "[2] Training loss: 0.21666, Validation loss: 0.14497\n",
      "[3] Training loss: 0.21668, Validation loss: 0.14496\n",
      "[4] Training loss: 0.21666, Validation loss: 0.14501\n",
      "[5] Training loss: 0.21661, Validation loss: 0.14510\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22879, Validation loss: 0.35103\n",
      "[2] Training loss: 0.22879, Validation loss: 0.35111\n",
      "[3] Training loss: 0.22878, Validation loss: 0.35117\n",
      "[4] Training loss: 0.22876, Validation loss: 0.35122\n",
      "[5] Training loss: 0.22874, Validation loss: 0.35125\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22486, Validation loss: 0.08415\n",
      "[2] Training loss: 0.22485, Validation loss: 0.08418\n",
      "[3] Training loss: 0.22483, Validation loss: 0.08422\n",
      "[4] Training loss: 0.22481, Validation loss: 0.08428\n",
      "[5] Training loss: 0.22479, Validation loss: 0.08434\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10427, Validation loss: 0.57601\n",
      "[2] Training loss: 0.10427, Validation loss: 0.57619\n",
      "[3] Training loss: 0.10424, Validation loss: 0.57638\n",
      "[4] Training loss: 0.10419, Validation loss: 0.57656\n",
      "[5] Training loss: 0.10413, Validation loss: 0.57674\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20915, Validation loss: 0.45800\n",
      "[2] Training loss: 0.20914, Validation loss: 0.45781\n",
      "[3] Training loss: 0.20907, Validation loss: 0.45780\n",
      "[4] Training loss: 0.20896, Validation loss: 0.45794\n",
      "[5] Training loss: 0.20883, Validation loss: 0.45822\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21655, Validation loss: 0.14507\n",
      "[2] Training loss: 0.21660, Validation loss: 0.14500\n",
      "[3] Training loss: 0.21663, Validation loss: 0.14498\n",
      "[4] Training loss: 0.21661, Validation loss: 0.14502\n",
      "[5] Training loss: 0.21657, Validation loss: 0.14510\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22877, Validation loss: 0.35167\n",
      "[2] Training loss: 0.22878, Validation loss: 0.35185\n",
      "[3] Training loss: 0.22878, Validation loss: 0.35199\n",
      "[4] Training loss: 0.22877, Validation loss: 0.35210\n",
      "[5] Training loss: 0.22875, Validation loss: 0.35218\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22488, Validation loss: 0.08412\n",
      "[2] Training loss: 0.22487, Validation loss: 0.08417\n",
      "[3] Training loss: 0.22485, Validation loss: 0.08423\n",
      "[4] Training loss: 0.22484, Validation loss: 0.08430\n",
      "[5] Training loss: 0.22482, Validation loss: 0.08438\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10431, Validation loss: 0.57482\n",
      "[2] Training loss: 0.10431, Validation loss: 0.57500\n",
      "[3] Training loss: 0.10428, Validation loss: 0.57519\n",
      "[4] Training loss: 0.10423, Validation loss: 0.57539\n",
      "[5] Training loss: 0.10416, Validation loss: 0.57558\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20885, Validation loss: 0.45941\n",
      "[2] Training loss: 0.20885, Validation loss: 0.45897\n",
      "[3] Training loss: 0.20882, Validation loss: 0.45868\n",
      "[4] Training loss: 0.20876, Validation loss: 0.45849\n",
      "[5] Training loss: 0.20869, Validation loss: 0.45839\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21654, Validation loss: 0.14499\n",
      "[2] Training loss: 0.21659, Validation loss: 0.14492\n",
      "[3] Training loss: 0.21660, Validation loss: 0.14491\n",
      "[4] Training loss: 0.21658, Validation loss: 0.14496\n",
      "[5] Training loss: 0.21654, Validation loss: 0.14505\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22871, Validation loss: 0.35090\n",
      "[2] Training loss: 0.22871, Validation loss: 0.35098\n",
      "[3] Training loss: 0.22871, Validation loss: 0.35104\n",
      "[4] Training loss: 0.22869, Validation loss: 0.35109\n",
      "[5] Training loss: 0.22866, Validation loss: 0.35112\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22479, Validation loss: 0.08391\n",
      "[2] Training loss: 0.22478, Validation loss: 0.08394\n",
      "[3] Training loss: 0.22476, Validation loss: 0.08399\n",
      "[4] Training loss: 0.22474, Validation loss: 0.08404\n",
      "[5] Training loss: 0.22472, Validation loss: 0.08411\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10418, Validation loss: 0.57624\n",
      "[2] Training loss: 0.10418, Validation loss: 0.57643\n",
      "[3] Training loss: 0.10416, Validation loss: 0.57661\n",
      "[4] Training loss: 0.10411, Validation loss: 0.57679\n",
      "[5] Training loss: 0.10405, Validation loss: 0.57697\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20915, Validation loss: 0.45729\n",
      "[2] Training loss: 0.20914, Validation loss: 0.45711\n",
      "[3] Training loss: 0.20906, Validation loss: 0.45715\n",
      "[4] Training loss: 0.20893, Validation loss: 0.45737\n",
      "[5] Training loss: 0.20878, Validation loss: 0.45773\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21648, Validation loss: 0.14500\n",
      "[2] Training loss: 0.21654, Validation loss: 0.14492\n",
      "[3] Training loss: 0.21656, Validation loss: 0.14491\n",
      "[4] Training loss: 0.21655, Validation loss: 0.14494\n",
      "[5] Training loss: 0.21652, Validation loss: 0.14502\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22871, Validation loss: 0.35185\n",
      "[2] Training loss: 0.22873, Validation loss: 0.35203\n",
      "[3] Training loss: 0.22873, Validation loss: 0.35218\n",
      "[4] Training loss: 0.22872, Validation loss: 0.35231\n",
      "[5] Training loss: 0.22870, Validation loss: 0.35240\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22482, Validation loss: 0.08413\n",
      "[2] Training loss: 0.22481, Validation loss: 0.08419\n",
      "[3] Training loss: 0.22479, Validation loss: 0.08426\n",
      "[4] Training loss: 0.22477, Validation loss: 0.08434\n",
      "[5] Training loss: 0.22475, Validation loss: 0.08443\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10425, Validation loss: 0.57443\n",
      "[2] Training loss: 0.10424, Validation loss: 0.57460\n",
      "[3] Training loss: 0.10422, Validation loss: 0.57479\n",
      "[4] Training loss: 0.10417, Validation loss: 0.57497\n",
      "[5] Training loss: 0.10410, Validation loss: 0.57516\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20879, Validation loss: 0.45947\n",
      "[2] Training loss: 0.20879, Validation loss: 0.45905\n",
      "[3] Training loss: 0.20876, Validation loss: 0.45876\n",
      "[4] Training loss: 0.20870, Validation loss: 0.45859\n",
      "[5] Training loss: 0.20863, Validation loss: 0.45851\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21648, Validation loss: 0.14490\n",
      "[2] Training loss: 0.21653, Validation loss: 0.14484\n",
      "[3] Training loss: 0.21654, Validation loss: 0.14483\n",
      "[4] Training loss: 0.21653, Validation loss: 0.14487\n",
      "[5] Training loss: 0.21648, Validation loss: 0.14496\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22866, Validation loss: 0.35119\n",
      "[2] Training loss: 0.22866, Validation loss: 0.35127\n",
      "[3] Training loss: 0.22866, Validation loss: 0.35133\n",
      "[4] Training loss: 0.22864, Validation loss: 0.35138\n",
      "[5] Training loss: 0.22861, Validation loss: 0.35141\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22473, Validation loss: 0.08417\n",
      "[2] Training loss: 0.22471, Validation loss: 0.08421\n",
      "[3] Training loss: 0.22470, Validation loss: 0.08426\n",
      "[4] Training loss: 0.22468, Validation loss: 0.08432\n",
      "[5] Training loss: 0.22466, Validation loss: 0.08438\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10412, Validation loss: 0.57565\n",
      "[2] Training loss: 0.10412, Validation loss: 0.57583\n",
      "[3] Training loss: 0.10409, Validation loss: 0.57602\n",
      "[4] Training loss: 0.10405, Validation loss: 0.57620\n",
      "[5] Training loss: 0.10398, Validation loss: 0.57638\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20905, Validation loss: 0.45739\n",
      "[2] Training loss: 0.20904, Validation loss: 0.45715\n",
      "[3] Training loss: 0.20898, Validation loss: 0.45710\n",
      "[4] Training loss: 0.20888, Validation loss: 0.45721\n",
      "[5] Training loss: 0.20875, Validation loss: 0.45745\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21640, Validation loss: 0.14497\n",
      "[2] Training loss: 0.21645, Validation loss: 0.14490\n",
      "[3] Training loss: 0.21647, Validation loss: 0.14489\n",
      "[4] Training loss: 0.21646, Validation loss: 0.14492\n",
      "[5] Training loss: 0.21642, Validation loss: 0.14501\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22862, Validation loss: 0.35165\n",
      "[2] Training loss: 0.22863, Validation loss: 0.35181\n",
      "[3] Training loss: 0.22863, Validation loss: 0.35194\n",
      "[4] Training loss: 0.22862, Validation loss: 0.35205\n",
      "[5] Training loss: 0.22860, Validation loss: 0.35212\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22474, Validation loss: 0.08373\n",
      "[2] Training loss: 0.22473, Validation loss: 0.08377\n",
      "[3] Training loss: 0.22472, Validation loss: 0.08382\n",
      "[4] Training loss: 0.22470, Validation loss: 0.08388\n",
      "[5] Training loss: 0.22468, Validation loss: 0.08396\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10415, Validation loss: 0.57507\n",
      "[2] Training loss: 0.10415, Validation loss: 0.57525\n",
      "[3] Training loss: 0.10412, Validation loss: 0.57545\n",
      "[4] Training loss: 0.10407, Validation loss: 0.57564\n",
      "[5] Training loss: 0.10400, Validation loss: 0.57584\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20881, Validation loss: 0.45851\n",
      "[2] Training loss: 0.20880, Validation loss: 0.45814\n",
      "[3] Training loss: 0.20876, Validation loss: 0.45790\n",
      "[4] Training loss: 0.20870, Validation loss: 0.45777\n",
      "[5] Training loss: 0.20863, Validation loss: 0.45775\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21639, Validation loss: 0.14487\n",
      "[2] Training loss: 0.21644, Validation loss: 0.14480\n",
      "[3] Training loss: 0.21646, Validation loss: 0.14479\n",
      "[4] Training loss: 0.21644, Validation loss: 0.14483\n",
      "[5] Training loss: 0.21640, Validation loss: 0.14492\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22858, Validation loss: 0.35097\n",
      "[2] Training loss: 0.22858, Validation loss: 0.35106\n",
      "[3] Training loss: 0.22857, Validation loss: 0.35114\n",
      "[4] Training loss: 0.22856, Validation loss: 0.35121\n",
      "[5] Training loss: 0.22853, Validation loss: 0.35125\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22466, Validation loss: 0.08404\n",
      "[2] Training loss: 0.22464, Validation loss: 0.08409\n",
      "[3] Training loss: 0.22463, Validation loss: 0.08414\n",
      "[4] Training loss: 0.22461, Validation loss: 0.08421\n",
      "[5] Training loss: 0.22459, Validation loss: 0.08428\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10403, Validation loss: 0.57564\n",
      "[2] Training loss: 0.10403, Validation loss: 0.57581\n",
      "[3] Training loss: 0.10400, Validation loss: 0.57597\n",
      "[4] Training loss: 0.10396, Validation loss: 0.57614\n",
      "[5] Training loss: 0.10389, Validation loss: 0.57631\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20906, Validation loss: 0.45713\n",
      "[2] Training loss: 0.20903, Validation loss: 0.45691\n",
      "[3] Training loss: 0.20894, Validation loss: 0.45688\n",
      "[4] Training loss: 0.20882, Validation loss: 0.45699\n",
      "[5] Training loss: 0.20867, Validation loss: 0.45722\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21634, Validation loss: 0.14490\n",
      "[2] Training loss: 0.21640, Validation loss: 0.14483\n",
      "[3] Training loss: 0.21642, Validation loss: 0.14482\n",
      "[4] Training loss: 0.21641, Validation loss: 0.14486\n",
      "[5] Training loss: 0.21637, Validation loss: 0.14494\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22859, Validation loss: 0.35197\n",
      "[2] Training loss: 0.22860, Validation loss: 0.35213\n",
      "[3] Training loss: 0.22860, Validation loss: 0.35227\n",
      "[4] Training loss: 0.22859, Validation loss: 0.35237\n",
      "[5] Training loss: 0.22857, Validation loss: 0.35243\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22468, Validation loss: 0.08401\n",
      "[2] Training loss: 0.22467, Validation loss: 0.08406\n",
      "[3] Training loss: 0.22466, Validation loss: 0.08413\n",
      "[4] Training loss: 0.22464, Validation loss: 0.08420\n",
      "[5] Training loss: 0.22462, Validation loss: 0.08428\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10411, Validation loss: 0.57456\n",
      "[2] Training loss: 0.10411, Validation loss: 0.57476\n",
      "[3] Training loss: 0.10408, Validation loss: 0.57496\n",
      "[4] Training loss: 0.10403, Validation loss: 0.57517\n",
      "[5] Training loss: 0.10395, Validation loss: 0.57537\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20868, Validation loss: 0.45779\n",
      "[2] Training loss: 0.20868, Validation loss: 0.45738\n",
      "[3] Training loss: 0.20864, Validation loss: 0.45711\n",
      "[4] Training loss: 0.20859, Validation loss: 0.45694\n",
      "[5] Training loss: 0.20852, Validation loss: 0.45687\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21634, Validation loss: 0.14479\n",
      "[2] Training loss: 0.21639, Validation loss: 0.14473\n",
      "[3] Training loss: 0.21641, Validation loss: 0.14472\n",
      "[4] Training loss: 0.21639, Validation loss: 0.14476\n",
      "[5] Training loss: 0.21634, Validation loss: 0.14485\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22854, Validation loss: 0.35107\n",
      "[2] Training loss: 0.22854, Validation loss: 0.35116\n",
      "[3] Training loss: 0.22854, Validation loss: 0.35123\n",
      "[4] Training loss: 0.22852, Validation loss: 0.35129\n",
      "[5] Training loss: 0.22849, Validation loss: 0.35133\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22458, Validation loss: 0.08399\n",
      "[2] Training loss: 0.22457, Validation loss: 0.08402\n",
      "[3] Training loss: 0.22455, Validation loss: 0.08407\n",
      "[4] Training loss: 0.22454, Validation loss: 0.08412\n",
      "[5] Training loss: 0.22452, Validation loss: 0.08419\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10396, Validation loss: 0.57568\n",
      "[2] Training loss: 0.10396, Validation loss: 0.57585\n",
      "[3] Training loss: 0.10394, Validation loss: 0.57603\n",
      "[4] Training loss: 0.10389, Validation loss: 0.57620\n",
      "[5] Training loss: 0.10382, Validation loss: 0.57638\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20895, Validation loss: 0.45590\n",
      "[2] Training loss: 0.20895, Validation loss: 0.45568\n",
      "[3] Training loss: 0.20889, Validation loss: 0.45565\n",
      "[4] Training loss: 0.20879, Validation loss: 0.45578\n",
      "[5] Training loss: 0.20866, Validation loss: 0.45604\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21626, Validation loss: 0.14487\n",
      "[2] Training loss: 0.21632, Validation loss: 0.14480\n",
      "[3] Training loss: 0.21634, Validation loss: 0.14478\n",
      "[4] Training loss: 0.21633, Validation loss: 0.14482\n",
      "[5] Training loss: 0.21628, Validation loss: 0.14491\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22850, Validation loss: 0.35176\n",
      "[2] Training loss: 0.22851, Validation loss: 0.35192\n",
      "[3] Training loss: 0.22851, Validation loss: 0.35205\n",
      "[4] Training loss: 0.22850, Validation loss: 0.35216\n",
      "[5] Training loss: 0.22848, Validation loss: 0.35223\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22460, Validation loss: 0.08348\n",
      "[2] Training loss: 0.22459, Validation loss: 0.08352\n",
      "[3] Training loss: 0.22458, Validation loss: 0.08357\n",
      "[4] Training loss: 0.22456, Validation loss: 0.08363\n",
      "[5] Training loss: 0.22454, Validation loss: 0.08370\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10399, Validation loss: 0.57500\n",
      "[2] Training loss: 0.10399, Validation loss: 0.57519\n",
      "[3] Training loss: 0.10397, Validation loss: 0.57538\n",
      "[4] Training loss: 0.10392, Validation loss: 0.57558\n",
      "[5] Training loss: 0.10385, Validation loss: 0.57578\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20872, Validation loss: 0.45735\n",
      "[2] Training loss: 0.20872, Validation loss: 0.45699\n",
      "[3] Training loss: 0.20868, Validation loss: 0.45676\n",
      "[4] Training loss: 0.20861, Validation loss: 0.45665\n",
      "[5] Training loss: 0.20853, Validation loss: 0.45664\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21625, Validation loss: 0.14476\n",
      "[2] Training loss: 0.21631, Validation loss: 0.14469\n",
      "[3] Training loss: 0.21633, Validation loss: 0.14467\n",
      "[4] Training loss: 0.21631, Validation loss: 0.14471\n",
      "[5] Training loss: 0.21627, Validation loss: 0.14480\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22847, Validation loss: 0.35120\n",
      "[2] Training loss: 0.22847, Validation loss: 0.35129\n",
      "[3] Training loss: 0.22847, Validation loss: 0.35137\n",
      "[4] Training loss: 0.22845, Validation loss: 0.35143\n",
      "[5] Training loss: 0.22842, Validation loss: 0.35147\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22453, Validation loss: 0.08410\n",
      "[2] Training loss: 0.22452, Validation loss: 0.08416\n",
      "[3] Training loss: 0.22450, Validation loss: 0.08423\n",
      "[4] Training loss: 0.22448, Validation loss: 0.08431\n",
      "[5] Training loss: 0.22446, Validation loss: 0.08439\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10390, Validation loss: 0.57522\n",
      "[2] Training loss: 0.10390, Validation loss: 0.57539\n",
      "[3] Training loss: 0.10387, Validation loss: 0.57556\n",
      "[4] Training loss: 0.10383, Validation loss: 0.57573\n",
      "[5] Training loss: 0.10376, Validation loss: 0.57589\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20886, Validation loss: 0.45611\n",
      "[2] Training loss: 0.20885, Validation loss: 0.45589\n",
      "[3] Training loss: 0.20879, Validation loss: 0.45586\n",
      "[4] Training loss: 0.20869, Validation loss: 0.45597\n",
      "[5] Training loss: 0.20856, Validation loss: 0.45621\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21620, Validation loss: 0.14478\n",
      "[2] Training loss: 0.21626, Validation loss: 0.14471\n",
      "[3] Training loss: 0.21628, Validation loss: 0.14470\n",
      "[4] Training loss: 0.21627, Validation loss: 0.14474\n",
      "[5] Training loss: 0.21622, Validation loss: 0.14483\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22845, Validation loss: 0.35199\n",
      "[2] Training loss: 0.22846, Validation loss: 0.35216\n",
      "[3] Training loss: 0.22846, Validation loss: 0.35229\n",
      "[4] Training loss: 0.22845, Validation loss: 0.35239\n",
      "[5] Training loss: 0.22843, Validation loss: 0.35245\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22455, Validation loss: 0.08360\n",
      "[2] Training loss: 0.22453, Validation loss: 0.08363\n",
      "[3] Training loss: 0.22452, Validation loss: 0.08368\n",
      "[4] Training loss: 0.22450, Validation loss: 0.08374\n",
      "[5] Training loss: 0.22448, Validation loss: 0.08381\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10394, Validation loss: 0.57475\n",
      "[2] Training loss: 0.10394, Validation loss: 0.57494\n",
      "[3] Training loss: 0.10391, Validation loss: 0.57515\n",
      "[4] Training loss: 0.10386, Validation loss: 0.57535\n",
      "[5] Training loss: 0.10379, Validation loss: 0.57556\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20863, Validation loss: 0.45714\n",
      "[2] Training loss: 0.20863, Validation loss: 0.45677\n",
      "[3] Training loss: 0.20859, Validation loss: 0.45655\n",
      "[4] Training loss: 0.20853, Validation loss: 0.45644\n",
      "[5] Training loss: 0.20845, Validation loss: 0.45642\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21619, Validation loss: 0.14468\n",
      "[2] Training loss: 0.21625, Validation loss: 0.14461\n",
      "[3] Training loss: 0.21626, Validation loss: 0.14460\n",
      "[4] Training loss: 0.21625, Validation loss: 0.14464\n",
      "[5] Training loss: 0.21620, Validation loss: 0.14473\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22840, Validation loss: 0.35113\n",
      "[2] Training loss: 0.22840, Validation loss: 0.35123\n",
      "[3] Training loss: 0.22839, Validation loss: 0.35130\n",
      "[4] Training loss: 0.22838, Validation loss: 0.35136\n",
      "[5] Training loss: 0.22835, Validation loss: 0.35140\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22445, Validation loss: 0.08398\n",
      "[2] Training loss: 0.22444, Validation loss: 0.08403\n",
      "[3] Training loss: 0.22442, Validation loss: 0.08409\n",
      "[4] Training loss: 0.22441, Validation loss: 0.08416\n",
      "[5] Training loss: 0.22439, Validation loss: 0.08423\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10381, Validation loss: 0.57526\n",
      "[2] Training loss: 0.10381, Validation loss: 0.57543\n",
      "[3] Training loss: 0.10378, Validation loss: 0.57559\n",
      "[4] Training loss: 0.10374, Validation loss: 0.57576\n",
      "[5] Training loss: 0.10367, Validation loss: 0.57593\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20889, Validation loss: 0.45585\n",
      "[2] Training loss: 0.20887, Validation loss: 0.45562\n",
      "[3] Training loss: 0.20878, Validation loss: 0.45559\n",
      "[4] Training loss: 0.20866, Validation loss: 0.45572\n",
      "[5] Training loss: 0.20852, Validation loss: 0.45595\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21614, Validation loss: 0.14474\n",
      "[2] Training loss: 0.21619, Validation loss: 0.14467\n",
      "[3] Training loss: 0.21622, Validation loss: 0.14466\n",
      "[4] Training loss: 0.21620, Validation loss: 0.14470\n",
      "[5] Training loss: 0.21616, Validation loss: 0.14478\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22840, Validation loss: 0.35217\n",
      "[2] Training loss: 0.22841, Validation loss: 0.35234\n",
      "[3] Training loss: 0.22841, Validation loss: 0.35247\n",
      "[4] Training loss: 0.22840, Validation loss: 0.35257\n",
      "[5] Training loss: 0.22839, Validation loss: 0.35263\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22448, Validation loss: 0.08385\n",
      "[2] Training loss: 0.22447, Validation loss: 0.08389\n",
      "[3] Training loss: 0.22445, Validation loss: 0.08395\n",
      "[4] Training loss: 0.22443, Validation loss: 0.08402\n",
      "[5] Training loss: 0.22441, Validation loss: 0.08410\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10389, Validation loss: 0.57427\n",
      "[2] Training loss: 0.10389, Validation loss: 0.57447\n",
      "[3] Training loss: 0.10386, Validation loss: 0.57468\n",
      "[4] Training loss: 0.10380, Validation loss: 0.57489\n",
      "[5] Training loss: 0.10373, Validation loss: 0.57510\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20854, Validation loss: 0.45665\n",
      "[2] Training loss: 0.20853, Validation loss: 0.45625\n",
      "[3] Training loss: 0.20850, Validation loss: 0.45601\n",
      "[4] Training loss: 0.20845, Validation loss: 0.45585\n",
      "[5] Training loss: 0.20838, Validation loss: 0.45579\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21613, Validation loss: 0.14463\n",
      "[2] Training loss: 0.21619, Validation loss: 0.14456\n",
      "[3] Training loss: 0.21620, Validation loss: 0.14455\n",
      "[4] Training loss: 0.21618, Validation loss: 0.14459\n",
      "[5] Training loss: 0.21614, Validation loss: 0.14468\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22836, Validation loss: 0.35129\n",
      "[2] Training loss: 0.22836, Validation loss: 0.35138\n",
      "[3] Training loss: 0.22835, Validation loss: 0.35145\n",
      "[4] Training loss: 0.22833, Validation loss: 0.35150\n",
      "[5] Training loss: 0.22831, Validation loss: 0.35154\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22438, Validation loss: 0.08392\n",
      "[2] Training loss: 0.22437, Validation loss: 0.08395\n",
      "[3] Training loss: 0.22435, Validation loss: 0.08400\n",
      "[4] Training loss: 0.22433, Validation loss: 0.08406\n",
      "[5] Training loss: 0.22431, Validation loss: 0.08413\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10374, Validation loss: 0.57524\n",
      "[2] Training loss: 0.10374, Validation loss: 0.57541\n",
      "[3] Training loss: 0.10371, Validation loss: 0.57559\n",
      "[4] Training loss: 0.10367, Validation loss: 0.57577\n",
      "[5] Training loss: 0.10360, Validation loss: 0.57594\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20880, Validation loss: 0.45490\n",
      "[2] Training loss: 0.20879, Validation loss: 0.45469\n",
      "[3] Training loss: 0.20874, Validation loss: 0.45465\n",
      "[4] Training loss: 0.20864, Validation loss: 0.45476\n",
      "[5] Training loss: 0.20851, Validation loss: 0.45497\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21605, Validation loss: 0.14470\n",
      "[2] Training loss: 0.21611, Validation loss: 0.14463\n",
      "[3] Training loss: 0.21613, Validation loss: 0.14462\n",
      "[4] Training loss: 0.21611, Validation loss: 0.14466\n",
      "[5] Training loss: 0.21607, Validation loss: 0.14475\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22831, Validation loss: 0.35190\n",
      "[2] Training loss: 0.22832, Validation loss: 0.35205\n",
      "[3] Training loss: 0.22832, Validation loss: 0.35217\n",
      "[4] Training loss: 0.22830, Validation loss: 0.35227\n",
      "[5] Training loss: 0.22828, Validation loss: 0.35233\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22439, Validation loss: 0.08337\n",
      "[2] Training loss: 0.22438, Validation loss: 0.08341\n",
      "[3] Training loss: 0.22437, Validation loss: 0.08346\n",
      "[4] Training loss: 0.22435, Validation loss: 0.08352\n",
      "[5] Training loss: 0.22433, Validation loss: 0.08359\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10377, Validation loss: 0.57481\n",
      "[2] Training loss: 0.10377, Validation loss: 0.57501\n",
      "[3] Training loss: 0.10374, Validation loss: 0.57520\n",
      "[4] Training loss: 0.10369, Validation loss: 0.57541\n",
      "[5] Training loss: 0.10362, Validation loss: 0.57561\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20859, Validation loss: 0.45593\n",
      "[2] Training loss: 0.20859, Validation loss: 0.45559\n",
      "[3] Training loss: 0.20855, Validation loss: 0.45538\n",
      "[4] Training loss: 0.20848, Validation loss: 0.45528\n",
      "[5] Training loss: 0.20839, Validation loss: 0.45529\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21604, Validation loss: 0.14460\n",
      "[2] Training loss: 0.21610, Validation loss: 0.14452\n",
      "[3] Training loss: 0.21612, Validation loss: 0.14451\n",
      "[4] Training loss: 0.21610, Validation loss: 0.14455\n",
      "[5] Training loss: 0.21606, Validation loss: 0.14463\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22828, Validation loss: 0.35131\n",
      "[2] Training loss: 0.22828, Validation loss: 0.35142\n",
      "[3] Training loss: 0.22828, Validation loss: 0.35150\n",
      "[4] Training loss: 0.22826, Validation loss: 0.35156\n",
      "[5] Training loss: 0.22824, Validation loss: 0.35160\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22433, Validation loss: 0.08404\n",
      "[2] Training loss: 0.22431, Validation loss: 0.08410\n",
      "[3] Training loss: 0.22430, Validation loss: 0.08416\n",
      "[4] Training loss: 0.22428, Validation loss: 0.08424\n",
      "[5] Training loss: 0.22426, Validation loss: 0.08433\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10367, Validation loss: 0.57476\n",
      "[2] Training loss: 0.10368, Validation loss: 0.57492\n",
      "[3] Training loss: 0.10365, Validation loss: 0.57509\n",
      "[4] Training loss: 0.10360, Validation loss: 0.57526\n",
      "[5] Training loss: 0.10354, Validation loss: 0.57543\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20870, Validation loss: 0.45488\n",
      "[2] Training loss: 0.20869, Validation loss: 0.45465\n",
      "[3] Training loss: 0.20864, Validation loss: 0.45461\n",
      "[4] Training loss: 0.20854, Validation loss: 0.45471\n",
      "[5] Training loss: 0.20842, Validation loss: 0.45491\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21600, Validation loss: 0.14462\n",
      "[2] Training loss: 0.21605, Validation loss: 0.14455\n",
      "[3] Training loss: 0.21607, Validation loss: 0.14454\n",
      "[4] Training loss: 0.21606, Validation loss: 0.14458\n",
      "[5] Training loss: 0.21601, Validation loss: 0.14467\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22825, Validation loss: 0.35200\n",
      "[2] Training loss: 0.22826, Validation loss: 0.35216\n",
      "[3] Training loss: 0.22826, Validation loss: 0.35228\n",
      "[4] Training loss: 0.22824, Validation loss: 0.35237\n",
      "[5] Training loss: 0.22822, Validation loss: 0.35243\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22434, Validation loss: 0.08341\n",
      "[2] Training loss: 0.22432, Validation loss: 0.08344\n",
      "[3] Training loss: 0.22431, Validation loss: 0.08348\n",
      "[4] Training loss: 0.22429, Validation loss: 0.08354\n",
      "[5] Training loss: 0.22427, Validation loss: 0.08360\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10371, Validation loss: 0.57462\n",
      "[2] Training loss: 0.10371, Validation loss: 0.57482\n",
      "[3] Training loss: 0.10368, Validation loss: 0.57502\n",
      "[4] Training loss: 0.10363, Validation loss: 0.57523\n",
      "[5] Training loss: 0.10356, Validation loss: 0.57544\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20852, Validation loss: 0.45566\n",
      "[2] Training loss: 0.20852, Validation loss: 0.45531\n",
      "[3] Training loss: 0.20848, Validation loss: 0.45509\n",
      "[4] Training loss: 0.20842, Validation loss: 0.45499\n",
      "[5] Training loss: 0.20834, Validation loss: 0.45498\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21598, Validation loss: 0.14453\n",
      "[2] Training loss: 0.21603, Validation loss: 0.14445\n",
      "[3] Training loss: 0.21605, Validation loss: 0.14444\n",
      "[4] Training loss: 0.21604, Validation loss: 0.14448\n",
      "[5] Training loss: 0.21599, Validation loss: 0.14457\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22820, Validation loss: 0.35120\n",
      "[2] Training loss: 0.22820, Validation loss: 0.35130\n",
      "[3] Training loss: 0.22820, Validation loss: 0.35138\n",
      "[4] Training loss: 0.22818, Validation loss: 0.35144\n",
      "[5] Training loss: 0.22816, Validation loss: 0.35149\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22425, Validation loss: 0.08386\n",
      "[2] Training loss: 0.22424, Validation loss: 0.08391\n",
      "[3] Training loss: 0.22422, Validation loss: 0.08397\n",
      "[4] Training loss: 0.22420, Validation loss: 0.08404\n",
      "[5] Training loss: 0.22418, Validation loss: 0.08412\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10358, Validation loss: 0.57489\n",
      "[2] Training loss: 0.10359, Validation loss: 0.57506\n",
      "[3] Training loss: 0.10356, Validation loss: 0.57522\n",
      "[4] Training loss: 0.10352, Validation loss: 0.57539\n",
      "[5] Training loss: 0.10345, Validation loss: 0.57555\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20875, Validation loss: 0.45455\n",
      "[2] Training loss: 0.20873, Validation loss: 0.45434\n",
      "[3] Training loss: 0.20863, Validation loss: 0.45432\n",
      "[4] Training loss: 0.20851, Validation loss: 0.45445\n",
      "[5] Training loss: 0.20837, Validation loss: 0.45468\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21593, Validation loss: 0.14456\n",
      "[2] Training loss: 0.21599, Validation loss: 0.14449\n",
      "[3] Training loss: 0.21602, Validation loss: 0.14448\n",
      "[4] Training loss: 0.21601, Validation loss: 0.14452\n",
      "[5] Training loss: 0.21597, Validation loss: 0.14460\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22822, Validation loss: 0.35233\n",
      "[2] Training loss: 0.22823, Validation loss: 0.35250\n",
      "[3] Training loss: 0.22823, Validation loss: 0.35263\n",
      "[4] Training loss: 0.22822, Validation loss: 0.35272\n",
      "[5] Training loss: 0.22821, Validation loss: 0.35277\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22428, Validation loss: 0.08377\n",
      "[2] Training loss: 0.22427, Validation loss: 0.08382\n",
      "[3] Training loss: 0.22425, Validation loss: 0.08388\n",
      "[4] Training loss: 0.22423, Validation loss: 0.08395\n",
      "[5] Training loss: 0.22421, Validation loss: 0.08403\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10368, Validation loss: 0.57386\n",
      "[2] Training loss: 0.10368, Validation loss: 0.57407\n",
      "[3] Training loss: 0.10365, Validation loss: 0.57428\n",
      "[4] Training loss: 0.10360, Validation loss: 0.57449\n",
      "[5] Training loss: 0.10352, Validation loss: 0.57471\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20837, Validation loss: 0.45534\n",
      "[2] Training loss: 0.20837, Validation loss: 0.45495\n",
      "[3] Training loss: 0.20833, Validation loss: 0.45468\n",
      "[4] Training loss: 0.20828, Validation loss: 0.45452\n",
      "[5] Training loss: 0.20821, Validation loss: 0.45444\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21593, Validation loss: 0.14444\n",
      "[2] Training loss: 0.21599, Validation loss: 0.14438\n",
      "[3] Training loss: 0.21600, Validation loss: 0.14437\n",
      "[4] Training loss: 0.21598, Validation loss: 0.14441\n",
      "[5] Training loss: 0.21593, Validation loss: 0.14450\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22817, Validation loss: 0.35143\n",
      "[2] Training loss: 0.22817, Validation loss: 0.35151\n",
      "[3] Training loss: 0.22816, Validation loss: 0.35158\n",
      "[4] Training loss: 0.22815, Validation loss: 0.35163\n",
      "[5] Training loss: 0.22812, Validation loss: 0.35167\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22418, Validation loss: 0.08349\n",
      "[2] Training loss: 0.22416, Validation loss: 0.08352\n",
      "[3] Training loss: 0.22415, Validation loss: 0.08356\n",
      "[4] Training loss: 0.22413, Validation loss: 0.08362\n",
      "[5] Training loss: 0.22411, Validation loss: 0.08368\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10352, Validation loss: 0.57523\n",
      "[2] Training loss: 0.10352, Validation loss: 0.57542\n",
      "[3] Training loss: 0.10350, Validation loss: 0.57560\n",
      "[4] Training loss: 0.10345, Validation loss: 0.57579\n",
      "[5] Training loss: 0.10338, Validation loss: 0.57597\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20866, Validation loss: 0.45352\n",
      "[2] Training loss: 0.20866, Validation loss: 0.45334\n",
      "[3] Training loss: 0.20860, Validation loss: 0.45335\n",
      "[4] Training loss: 0.20849, Validation loss: 0.45349\n",
      "[5] Training loss: 0.20836, Validation loss: 0.45373\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21586, Validation loss: 0.14449\n",
      "[2] Training loss: 0.21592, Validation loss: 0.14442\n",
      "[3] Training loss: 0.21594, Validation loss: 0.14440\n",
      "[4] Training loss: 0.21592, Validation loss: 0.14444\n",
      "[5] Training loss: 0.21588, Validation loss: 0.14452\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22813, Validation loss: 0.35195\n",
      "[2] Training loss: 0.22814, Validation loss: 0.35213\n",
      "[3] Training loss: 0.22814, Validation loss: 0.35227\n",
      "[4] Training loss: 0.22813, Validation loss: 0.35238\n",
      "[5] Training loss: 0.22811, Validation loss: 0.35246\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22418, Validation loss: 0.08388\n",
      "[2] Training loss: 0.22417, Validation loss: 0.08394\n",
      "[3] Training loss: 0.22416, Validation loss: 0.08401\n",
      "[4] Training loss: 0.22414, Validation loss: 0.08408\n",
      "[5] Training loss: 0.22412, Validation loss: 0.08417\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10355, Validation loss: 0.57384\n",
      "[2] Training loss: 0.10355, Validation loss: 0.57401\n",
      "[3] Training loss: 0.10352, Validation loss: 0.57418\n",
      "[4] Training loss: 0.10347, Validation loss: 0.57436\n",
      "[5] Training loss: 0.10340, Validation loss: 0.57454\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20841, Validation loss: 0.45495\n",
      "[2] Training loss: 0.20841, Validation loss: 0.45456\n",
      "[3] Training loss: 0.20838, Validation loss: 0.45431\n",
      "[4] Training loss: 0.20832, Validation loss: 0.45416\n",
      "[5] Training loss: 0.20824, Validation loss: 0.45409\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21583, Validation loss: 0.14446\n",
      "[2] Training loss: 0.21588, Validation loss: 0.14440\n",
      "[3] Training loss: 0.21590, Validation loss: 0.14439\n",
      "[4] Training loss: 0.21588, Validation loss: 0.14444\n",
      "[5] Training loss: 0.21583, Validation loss: 0.14453\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22806, Validation loss: 0.35158\n",
      "[2] Training loss: 0.22807, Validation loss: 0.35167\n",
      "[3] Training loss: 0.22806, Validation loss: 0.35174\n",
      "[4] Training loss: 0.22804, Validation loss: 0.35179\n",
      "[5] Training loss: 0.22802, Validation loss: 0.35182\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22412, Validation loss: 0.08334\n",
      "[2] Training loss: 0.22410, Validation loss: 0.08336\n",
      "[3] Training loss: 0.22409, Validation loss: 0.08340\n",
      "[4] Training loss: 0.22407, Validation loss: 0.08344\n",
      "[5] Training loss: 0.22405, Validation loss: 0.08350\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10345, Validation loss: 0.57528\n",
      "[2] Training loss: 0.10345, Validation loss: 0.57547\n",
      "[3] Training loss: 0.10342, Validation loss: 0.57567\n",
      "[4] Training loss: 0.10337, Validation loss: 0.57586\n",
      "[5] Training loss: 0.10331, Validation loss: 0.57605\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20870, Validation loss: 0.45318\n",
      "[2] Training loss: 0.20868, Validation loss: 0.45300\n",
      "[3] Training loss: 0.20857, Validation loss: 0.45301\n",
      "[4] Training loss: 0.20842, Validation loss: 0.45316\n",
      "[5] Training loss: 0.20827, Validation loss: 0.45343\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21581, Validation loss: 0.14440\n",
      "[2] Training loss: 0.21587, Validation loss: 0.14432\n",
      "[3] Training loss: 0.21589, Validation loss: 0.14430\n",
      "[4] Training loss: 0.21589, Validation loss: 0.14433\n",
      "[5] Training loss: 0.21585, Validation loss: 0.14440\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22811, Validation loss: 0.35231\n",
      "[2] Training loss: 0.22812, Validation loss: 0.35249\n",
      "[3] Training loss: 0.22812, Validation loss: 0.35264\n",
      "[4] Training loss: 0.22811, Validation loss: 0.35275\n",
      "[5] Training loss: 0.22810, Validation loss: 0.35283\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22414, Validation loss: 0.08435\n",
      "[2] Training loss: 0.22413, Validation loss: 0.08442\n",
      "[3] Training loss: 0.22411, Validation loss: 0.08451\n",
      "[4] Training loss: 0.22409, Validation loss: 0.08460\n",
      "[5] Training loss: 0.22407, Validation loss: 0.08470\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10354, Validation loss: 0.57310\n",
      "[2] Training loss: 0.10354, Validation loss: 0.57327\n",
      "[3] Training loss: 0.10351, Validation loss: 0.57345\n",
      "[4] Training loss: 0.10346, Validation loss: 0.57363\n",
      "[5] Training loss: 0.10339, Validation loss: 0.57382\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20824, Validation loss: 0.45454\n",
      "[2] Training loss: 0.20824, Validation loss: 0.45412\n",
      "[3] Training loss: 0.20821, Validation loss: 0.45382\n",
      "[4] Training loss: 0.20817, Validation loss: 0.45362\n",
      "[5] Training loss: 0.20810, Validation loss: 0.45352\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21581, Validation loss: 0.14431\n",
      "[2] Training loss: 0.21586, Validation loss: 0.14425\n",
      "[3] Training loss: 0.21587, Validation loss: 0.14425\n",
      "[4] Training loss: 0.21585, Validation loss: 0.14429\n",
      "[5] Training loss: 0.21580, Validation loss: 0.14439\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22805, Validation loss: 0.35181\n",
      "[2] Training loss: 0.22805, Validation loss: 0.35189\n",
      "[3] Training loss: 0.22804, Validation loss: 0.35196\n",
      "[4] Training loss: 0.22803, Validation loss: 0.35201\n",
      "[5] Training loss: 0.22800, Validation loss: 0.35204\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22405, Validation loss: 0.08371\n",
      "[2] Training loss: 0.22404, Validation loss: 0.08373\n",
      "[3] Training loss: 0.22402, Validation loss: 0.08376\n",
      "[4] Training loss: 0.22400, Validation loss: 0.08381\n",
      "[5] Training loss: 0.22398, Validation loss: 0.08387\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10339, Validation loss: 0.57475\n",
      "[2] Training loss: 0.10339, Validation loss: 0.57495\n",
      "[3] Training loss: 0.10337, Validation loss: 0.57515\n",
      "[4] Training loss: 0.10332, Validation loss: 0.57535\n",
      "[5] Training loss: 0.10325, Validation loss: 0.57555\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20854, Validation loss: 0.45224\n",
      "[2] Training loss: 0.20854, Validation loss: 0.45200\n",
      "[3] Training loss: 0.20848, Validation loss: 0.45194\n",
      "[4] Training loss: 0.20838, Validation loss: 0.45204\n",
      "[5] Training loss: 0.20825, Validation loss: 0.45223\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21571, Validation loss: 0.14441\n",
      "[2] Training loss: 0.21577, Validation loss: 0.14434\n",
      "[3] Training loss: 0.21579, Validation loss: 0.14433\n",
      "[4] Training loss: 0.21578, Validation loss: 0.14437\n",
      "[5] Training loss: 0.21573, Validation loss: 0.14445\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22801, Validation loss: 0.35213\n",
      "[2] Training loss: 0.22802, Validation loss: 0.35230\n",
      "[3] Training loss: 0.22802, Validation loss: 0.35243\n",
      "[4] Training loss: 0.22801, Validation loss: 0.35252\n",
      "[5] Training loss: 0.22799, Validation loss: 0.35259\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22405, Validation loss: 0.08343\n",
      "[2] Training loss: 0.22404, Validation loss: 0.08347\n",
      "[3] Training loss: 0.22402, Validation loss: 0.08353\n",
      "[4] Training loss: 0.22400, Validation loss: 0.08360\n",
      "[5] Training loss: 0.22399, Validation loss: 0.08367\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10340, Validation loss: 0.57419\n",
      "[2] Training loss: 0.10340, Validation loss: 0.57437\n",
      "[3] Training loss: 0.10337, Validation loss: 0.57456\n",
      "[4] Training loss: 0.10332, Validation loss: 0.57475\n",
      "[5] Training loss: 0.10325, Validation loss: 0.57494\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20835, Validation loss: 0.45326\n",
      "[2] Training loss: 0.20834, Validation loss: 0.45291\n",
      "[3] Training loss: 0.20831, Validation loss: 0.45270\n",
      "[4] Training loss: 0.20825, Validation loss: 0.45259\n",
      "[5] Training loss: 0.20817, Validation loss: 0.45257\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21569, Validation loss: 0.14435\n",
      "[2] Training loss: 0.21575, Validation loss: 0.14428\n",
      "[3] Training loss: 0.21576, Validation loss: 0.14427\n",
      "[4] Training loss: 0.21575, Validation loss: 0.14431\n",
      "[5] Training loss: 0.21570, Validation loss: 0.14440\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22795, Validation loss: 0.35162\n",
      "[2] Training loss: 0.22795, Validation loss: 0.35173\n",
      "[3] Training loss: 0.22795, Validation loss: 0.35182\n",
      "[4] Training loss: 0.22793, Validation loss: 0.35189\n",
      "[5] Training loss: 0.22790, Validation loss: 0.35193\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22397, Validation loss: 0.08359\n",
      "[2] Training loss: 0.22396, Validation loss: 0.08363\n",
      "[3] Training loss: 0.22394, Validation loss: 0.08368\n",
      "[4] Training loss: 0.22393, Validation loss: 0.08375\n",
      "[5] Training loss: 0.22391, Validation loss: 0.08382\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10328, Validation loss: 0.57473\n",
      "[2] Training loss: 0.10328, Validation loss: 0.57490\n",
      "[3] Training loss: 0.10326, Validation loss: 0.57507\n",
      "[4] Training loss: 0.10321, Validation loss: 0.57525\n",
      "[5] Training loss: 0.10315, Validation loss: 0.57542\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20858, Validation loss: 0.45211\n",
      "[2] Training loss: 0.20856, Validation loss: 0.45191\n",
      "[3] Training loss: 0.20847, Validation loss: 0.45188\n",
      "[4] Training loss: 0.20835, Validation loss: 0.45199\n",
      "[5] Training loss: 0.20821, Validation loss: 0.45220\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21566, Validation loss: 0.14434\n",
      "[2] Training loss: 0.21571, Validation loss: 0.14427\n",
      "[3] Training loss: 0.21574, Validation loss: 0.14425\n",
      "[4] Training loss: 0.21573, Validation loss: 0.14429\n",
      "[5] Training loss: 0.21569, Validation loss: 0.14437\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22796, Validation loss: 0.35248\n",
      "[2] Training loss: 0.22797, Validation loss: 0.35265\n",
      "[3] Training loss: 0.22797, Validation loss: 0.35278\n",
      "[4] Training loss: 0.22796, Validation loss: 0.35288\n",
      "[5] Training loss: 0.22794, Validation loss: 0.35294\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22399, Validation loss: 0.08356\n",
      "[2] Training loss: 0.22398, Validation loss: 0.08361\n",
      "[3] Training loss: 0.22397, Validation loss: 0.08367\n",
      "[4] Training loss: 0.22395, Validation loss: 0.08373\n",
      "[5] Training loss: 0.22393, Validation loss: 0.08381\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10337, Validation loss: 0.57380\n",
      "[2] Training loss: 0.10337, Validation loss: 0.57399\n",
      "[3] Training loss: 0.10334, Validation loss: 0.57419\n",
      "[4] Training loss: 0.10329, Validation loss: 0.57439\n",
      "[5] Training loss: 0.10322, Validation loss: 0.57460\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20821, Validation loss: 0.45296\n",
      "[2] Training loss: 0.20821, Validation loss: 0.45260\n",
      "[3] Training loss: 0.20817, Validation loss: 0.45237\n",
      "[4] Training loss: 0.20812, Validation loss: 0.45224\n",
      "[5] Training loss: 0.20805, Validation loss: 0.45219\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21566, Validation loss: 0.14422\n",
      "[2] Training loss: 0.21571, Validation loss: 0.14415\n",
      "[3] Training loss: 0.21572, Validation loss: 0.14414\n",
      "[4] Training loss: 0.21571, Validation loss: 0.14419\n",
      "[5] Training loss: 0.21566, Validation loss: 0.14428\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22791, Validation loss: 0.35165\n",
      "[2] Training loss: 0.22792, Validation loss: 0.35174\n",
      "[3] Training loss: 0.22791, Validation loss: 0.35182\n",
      "[4] Training loss: 0.22789, Validation loss: 0.35189\n",
      "[5] Training loss: 0.22786, Validation loss: 0.35193\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22390, Validation loss: 0.08372\n",
      "[2] Training loss: 0.22389, Validation loss: 0.08376\n",
      "[3] Training loss: 0.22387, Validation loss: 0.08381\n",
      "[4] Training loss: 0.22386, Validation loss: 0.08387\n",
      "[5] Training loss: 0.22384, Validation loss: 0.08394\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10323, Validation loss: 0.57456\n",
      "[2] Training loss: 0.10323, Validation loss: 0.57473\n",
      "[3] Training loss: 0.10320, Validation loss: 0.57490\n",
      "[4] Training loss: 0.10316, Validation loss: 0.57508\n",
      "[5] Training loss: 0.10309, Validation loss: 0.57525\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20847, Validation loss: 0.45140\n",
      "[2] Training loss: 0.20846, Validation loss: 0.45117\n",
      "[3] Training loss: 0.20840, Validation loss: 0.45111\n",
      "[4] Training loss: 0.20830, Validation loss: 0.45120\n",
      "[5] Training loss: 0.20817, Validation loss: 0.45139\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21557, Validation loss: 0.14434\n",
      "[2] Training loss: 0.21562, Validation loss: 0.14428\n",
      "[3] Training loss: 0.21564, Validation loss: 0.14427\n",
      "[4] Training loss: 0.21563, Validation loss: 0.14431\n",
      "[5] Training loss: 0.21559, Validation loss: 0.14440\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22788, Validation loss: 0.35255\n",
      "[2] Training loss: 0.22789, Validation loss: 0.35271\n",
      "[3] Training loss: 0.22789, Validation loss: 0.35283\n",
      "[4] Training loss: 0.22788, Validation loss: 0.35292\n",
      "[5] Training loss: 0.22786, Validation loss: 0.35297\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22392, Validation loss: 0.08315\n",
      "[2] Training loss: 0.22391, Validation loss: 0.08319\n",
      "[3] Training loss: 0.22390, Validation loss: 0.08325\n",
      "[4] Training loss: 0.22388, Validation loss: 0.08331\n",
      "[5] Training loss: 0.22386, Validation loss: 0.08339\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10325, Validation loss: 0.57397\n",
      "[2] Training loss: 0.10324, Validation loss: 0.57417\n",
      "[3] Training loss: 0.10322, Validation loss: 0.57437\n",
      "[4] Training loss: 0.10316, Validation loss: 0.57458\n",
      "[5] Training loss: 0.10309, Validation loss: 0.57479\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20826, Validation loss: 0.45229\n",
      "[2] Training loss: 0.20825, Validation loss: 0.45195\n",
      "[3] Training loss: 0.20821, Validation loss: 0.45173\n",
      "[4] Training loss: 0.20814, Validation loss: 0.45163\n",
      "[5] Training loss: 0.20806, Validation loss: 0.45161\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21557, Validation loss: 0.14423\n",
      "[2] Training loss: 0.21562, Validation loss: 0.14416\n",
      "[3] Training loss: 0.21564, Validation loss: 0.14414\n",
      "[4] Training loss: 0.21563, Validation loss: 0.14418\n",
      "[5] Training loss: 0.21558, Validation loss: 0.14427\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22785, Validation loss: 0.35171\n",
      "[2] Training loss: 0.22786, Validation loss: 0.35182\n",
      "[3] Training loss: 0.22785, Validation loss: 0.35190\n",
      "[4] Training loss: 0.22784, Validation loss: 0.35197\n",
      "[5] Training loss: 0.22781, Validation loss: 0.35202\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22384, Validation loss: 0.08378\n",
      "[2] Training loss: 0.22383, Validation loss: 0.08384\n",
      "[3] Training loss: 0.22382, Validation loss: 0.08390\n",
      "[4] Training loss: 0.22380, Validation loss: 0.08398\n",
      "[5] Training loss: 0.22378, Validation loss: 0.08406\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10315, Validation loss: 0.57426\n",
      "[2] Training loss: 0.10315, Validation loss: 0.57442\n",
      "[3] Training loss: 0.10313, Validation loss: 0.57459\n",
      "[4] Training loss: 0.10308, Validation loss: 0.57476\n",
      "[5] Training loss: 0.10302, Validation loss: 0.57493\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20837, Validation loss: 0.45114\n",
      "[2] Training loss: 0.20836, Validation loss: 0.45093\n",
      "[3] Training loss: 0.20831, Validation loss: 0.45090\n",
      "[4] Training loss: 0.20821, Validation loss: 0.45099\n",
      "[5] Training loss: 0.20808, Validation loss: 0.45121\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21552, Validation loss: 0.14425\n",
      "[2] Training loss: 0.21557, Validation loss: 0.14418\n",
      "[3] Training loss: 0.21559, Validation loss: 0.14417\n",
      "[4] Training loss: 0.21558, Validation loss: 0.14421\n",
      "[5] Training loss: 0.21553, Validation loss: 0.14430\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22782, Validation loss: 0.35255\n",
      "[2] Training loss: 0.22783, Validation loss: 0.35272\n",
      "[3] Training loss: 0.22783, Validation loss: 0.35285\n",
      "[4] Training loss: 0.22782, Validation loss: 0.35294\n",
      "[5] Training loss: 0.22780, Validation loss: 0.35300\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22386, Validation loss: 0.08311\n",
      "[2] Training loss: 0.22385, Validation loss: 0.08314\n",
      "[3] Training loss: 0.22384, Validation loss: 0.08318\n",
      "[4] Training loss: 0.22382, Validation loss: 0.08324\n",
      "[5] Training loss: 0.22380, Validation loss: 0.08330\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10320, Validation loss: 0.57395\n",
      "[2] Training loss: 0.10319, Validation loss: 0.57414\n",
      "[3] Training loss: 0.10317, Validation loss: 0.57435\n",
      "[4] Training loss: 0.10312, Validation loss: 0.57456\n",
      "[5] Training loss: 0.10304, Validation loss: 0.57477\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20818, Validation loss: 0.45223\n",
      "[2] Training loss: 0.20817, Validation loss: 0.45190\n",
      "[3] Training loss: 0.20814, Validation loss: 0.45168\n",
      "[4] Training loss: 0.20807, Validation loss: 0.45157\n",
      "[5] Training loss: 0.20799, Validation loss: 0.45157\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21550, Validation loss: 0.14414\n",
      "[2] Training loss: 0.21556, Validation loss: 0.14406\n",
      "[3] Training loss: 0.21557, Validation loss: 0.14405\n",
      "[4] Training loss: 0.21556, Validation loss: 0.14409\n",
      "[5] Training loss: 0.21551, Validation loss: 0.14418\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22777, Validation loss: 0.35157\n",
      "[2] Training loss: 0.22778, Validation loss: 0.35167\n",
      "[3] Training loss: 0.22777, Validation loss: 0.35175\n",
      "[4] Training loss: 0.22775, Validation loss: 0.35181\n",
      "[5] Training loss: 0.22773, Validation loss: 0.35186\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22377, Validation loss: 0.08364\n",
      "[2] Training loss: 0.22376, Validation loss: 0.08370\n",
      "[3] Training loss: 0.22374, Validation loss: 0.08376\n",
      "[4] Training loss: 0.22372, Validation loss: 0.08383\n",
      "[5] Training loss: 0.22370, Validation loss: 0.08391\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10307, Validation loss: 0.57442\n",
      "[2] Training loss: 0.10307, Validation loss: 0.57458\n",
      "[3] Training loss: 0.10305, Validation loss: 0.57474\n",
      "[4] Training loss: 0.10300, Validation loss: 0.57491\n",
      "[5] Training loss: 0.10293, Validation loss: 0.57507\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20842, Validation loss: 0.45116\n",
      "[2] Training loss: 0.20840, Validation loss: 0.45095\n",
      "[3] Training loss: 0.20831, Validation loss: 0.45091\n",
      "[4] Training loss: 0.20820, Validation loss: 0.45103\n",
      "[5] Training loss: 0.20805, Validation loss: 0.45126\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21545, Validation loss: 0.14418\n",
      "[2] Training loss: 0.21551, Validation loss: 0.14411\n",
      "[3] Training loss: 0.21553, Validation loss: 0.14410\n",
      "[4] Training loss: 0.21552, Validation loss: 0.14414\n",
      "[5] Training loss: 0.21548, Validation loss: 0.14422\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22778, Validation loss: 0.35280\n",
      "[2] Training loss: 0.22779, Validation loss: 0.35297\n",
      "[3] Training loss: 0.22780, Validation loss: 0.35311\n",
      "[4] Training loss: 0.22779, Validation loss: 0.35321\n",
      "[5] Training loss: 0.22777, Validation loss: 0.35328\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22381, Validation loss: 0.08342\n",
      "[2] Training loss: 0.22379, Validation loss: 0.08346\n",
      "[3] Training loss: 0.22378, Validation loss: 0.08352\n",
      "[4] Training loss: 0.22376, Validation loss: 0.08359\n",
      "[5] Training loss: 0.22374, Validation loss: 0.08366\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10316, Validation loss: 0.57328\n",
      "[2] Training loss: 0.10316, Validation loss: 0.57349\n",
      "[3] Training loss: 0.10313, Validation loss: 0.57369\n",
      "[4] Training loss: 0.10307, Validation loss: 0.57391\n",
      "[5] Training loss: 0.10300, Validation loss: 0.57413\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20804, Validation loss: 0.45204\n",
      "[2] Training loss: 0.20804, Validation loss: 0.45165\n",
      "[3] Training loss: 0.20801, Validation loss: 0.45137\n",
      "[4] Training loss: 0.20795, Validation loss: 0.45119\n",
      "[5] Training loss: 0.20788, Validation loss: 0.45110\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21545, Validation loss: 0.14406\n",
      "[2] Training loss: 0.21551, Validation loss: 0.14400\n",
      "[3] Training loss: 0.21552, Validation loss: 0.14399\n",
      "[4] Training loss: 0.21550, Validation loss: 0.14403\n",
      "[5] Training loss: 0.21546, Validation loss: 0.14412\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22774, Validation loss: 0.35173\n",
      "[2] Training loss: 0.22774, Validation loss: 0.35181\n",
      "[3] Training loss: 0.22773, Validation loss: 0.35188\n",
      "[4] Training loss: 0.22771, Validation loss: 0.35193\n",
      "[5] Training loss: 0.22769, Validation loss: 0.35197\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22370, Validation loss: 0.08330\n",
      "[2] Training loss: 0.22369, Validation loss: 0.08333\n",
      "[3] Training loss: 0.22367, Validation loss: 0.08337\n",
      "[4] Training loss: 0.22365, Validation loss: 0.08343\n",
      "[5] Training loss: 0.22363, Validation loss: 0.08349\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10300, Validation loss: 0.57471\n",
      "[2] Training loss: 0.10300, Validation loss: 0.57489\n",
      "[3] Training loss: 0.10298, Validation loss: 0.57507\n",
      "[4] Training loss: 0.10293, Validation loss: 0.57524\n",
      "[5] Training loss: 0.10287, Validation loss: 0.57542\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20833, Validation loss: 0.45008\n",
      "[2] Training loss: 0.20832, Validation loss: 0.44990\n",
      "[3] Training loss: 0.20825, Validation loss: 0.44992\n",
      "[4] Training loss: 0.20815, Validation loss: 0.45006\n",
      "[5] Training loss: 0.20801, Validation loss: 0.45029\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21538, Validation loss: 0.14410\n",
      "[2] Training loss: 0.21544, Validation loss: 0.14403\n",
      "[3] Training loss: 0.21546, Validation loss: 0.14402\n",
      "[4] Training loss: 0.21545, Validation loss: 0.14405\n",
      "[5] Training loss: 0.21541, Validation loss: 0.14414\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22770, Validation loss: 0.35254\n",
      "[2] Training loss: 0.22771, Validation loss: 0.35273\n",
      "[3] Training loss: 0.22771, Validation loss: 0.35288\n",
      "[4] Training loss: 0.22770, Validation loss: 0.35300\n",
      "[5] Training loss: 0.22768, Validation loss: 0.35308\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22372, Validation loss: 0.08350\n",
      "[2] Training loss: 0.22371, Validation loss: 0.08356\n",
      "[3] Training loss: 0.22369, Validation loss: 0.08362\n",
      "[4] Training loss: 0.22367, Validation loss: 0.08370\n",
      "[5] Training loss: 0.22365, Validation loss: 0.08378\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10304, Validation loss: 0.57330\n",
      "[2] Training loss: 0.10304, Validation loss: 0.57347\n",
      "[3] Training loss: 0.10302, Validation loss: 0.57366\n",
      "[4] Training loss: 0.10297, Validation loss: 0.57384\n",
      "[5] Training loss: 0.10290, Validation loss: 0.57403\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20807, Validation loss: 0.45149\n",
      "[2] Training loss: 0.20806, Validation loss: 0.45112\n",
      "[3] Training loss: 0.20803, Validation loss: 0.45087\n",
      "[4] Training loss: 0.20798, Validation loss: 0.45070\n",
      "[5] Training loss: 0.20790, Validation loss: 0.45062\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21536, Validation loss: 0.14405\n",
      "[2] Training loss: 0.21541, Validation loss: 0.14399\n",
      "[3] Training loss: 0.21543, Validation loss: 0.14398\n",
      "[4] Training loss: 0.21541, Validation loss: 0.14402\n",
      "[5] Training loss: 0.21536, Validation loss: 0.14412\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22764, Validation loss: 0.35181\n",
      "[2] Training loss: 0.22764, Validation loss: 0.35190\n",
      "[3] Training loss: 0.22763, Validation loss: 0.35197\n",
      "[4] Training loss: 0.22762, Validation loss: 0.35202\n",
      "[5] Training loss: 0.22759, Validation loss: 0.35205\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22363, Validation loss: 0.08325\n",
      "[2] Training loss: 0.22362, Validation loss: 0.08328\n",
      "[3] Training loss: 0.22360, Validation loss: 0.08332\n",
      "[4] Training loss: 0.22359, Validation loss: 0.08337\n",
      "[5] Training loss: 0.22357, Validation loss: 0.08343\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10293, Validation loss: 0.57464\n",
      "[2] Training loss: 0.10293, Validation loss: 0.57482\n",
      "[3] Training loss: 0.10290, Validation loss: 0.57501\n",
      "[4] Training loss: 0.10286, Validation loss: 0.57519\n",
      "[5] Training loss: 0.10279, Validation loss: 0.57537\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20836, Validation loss: 0.44967\n",
      "[2] Training loss: 0.20833, Validation loss: 0.44950\n",
      "[3] Training loss: 0.20823, Validation loss: 0.44951\n",
      "[4] Training loss: 0.20810, Validation loss: 0.44966\n",
      "[5] Training loss: 0.20795, Validation loss: 0.44990\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21532, Validation loss: 0.14403\n",
      "[2] Training loss: 0.21538, Validation loss: 0.14395\n",
      "[3] Training loss: 0.21541, Validation loss: 0.14393\n",
      "[4] Training loss: 0.21540, Validation loss: 0.14397\n",
      "[5] Training loss: 0.21536, Validation loss: 0.14405\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22766, Validation loss: 0.35275\n",
      "[2] Training loss: 0.22767, Validation loss: 0.35294\n",
      "[3] Training loss: 0.22767, Validation loss: 0.35309\n",
      "[4] Training loss: 0.22767, Validation loss: 0.35322\n",
      "[5] Training loss: 0.22765, Validation loss: 0.35330\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22366, Validation loss: 0.08370\n",
      "[2] Training loss: 0.22365, Validation loss: 0.08376\n",
      "[3] Training loss: 0.22363, Validation loss: 0.08383\n",
      "[4] Training loss: 0.22361, Validation loss: 0.08391\n",
      "[5] Training loss: 0.22359, Validation loss: 0.08400\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10302, Validation loss: 0.57290\n",
      "[2] Training loss: 0.10301, Validation loss: 0.57307\n",
      "[3] Training loss: 0.10299, Validation loss: 0.57326\n",
      "[4] Training loss: 0.10293, Validation loss: 0.57345\n",
      "[5] Training loss: 0.10286, Validation loss: 0.57365\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20795, Validation loss: 0.45089\n",
      "[2] Training loss: 0.20794, Validation loss: 0.45049\n",
      "[3] Training loss: 0.20791, Validation loss: 0.45022\n",
      "[4] Training loss: 0.20786, Validation loss: 0.45003\n",
      "[5] Training loss: 0.20779, Validation loss: 0.44994\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21532, Validation loss: 0.14393\n",
      "[2] Training loss: 0.21537, Validation loss: 0.14387\n",
      "[3] Training loss: 0.21539, Validation loss: 0.14386\n",
      "[4] Training loss: 0.21537, Validation loss: 0.14391\n",
      "[5] Training loss: 0.21532, Validation loss: 0.14400\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22761, Validation loss: 0.35194\n",
      "[2] Training loss: 0.22761, Validation loss: 0.35202\n",
      "[3] Training loss: 0.22760, Validation loss: 0.35209\n",
      "[4] Training loss: 0.22759, Validation loss: 0.35214\n",
      "[5] Training loss: 0.22756, Validation loss: 0.35218\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22357, Validation loss: 0.08351\n",
      "[2] Training loss: 0.22355, Validation loss: 0.08354\n",
      "[3] Training loss: 0.22354, Validation loss: 0.08358\n",
      "[4] Training loss: 0.22352, Validation loss: 0.08364\n",
      "[5] Training loss: 0.22350, Validation loss: 0.08370\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10287, Validation loss: 0.57425\n",
      "[2] Training loss: 0.10287, Validation loss: 0.57444\n",
      "[3] Training loss: 0.10285, Validation loss: 0.57462\n",
      "[4] Training loss: 0.10280, Validation loss: 0.57481\n",
      "[5] Training loss: 0.10273, Validation loss: 0.57499\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20822, Validation loss: 0.44883\n",
      "[2] Training loss: 0.20822, Validation loss: 0.44863\n",
      "[3] Training loss: 0.20816, Validation loss: 0.44860\n",
      "[4] Training loss: 0.20806, Validation loss: 0.44873\n",
      "[5] Training loss: 0.20793, Validation loss: 0.44894\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21523, Validation loss: 0.14405\n",
      "[2] Training loss: 0.21529, Validation loss: 0.14398\n",
      "[3] Training loss: 0.21531, Validation loss: 0.14397\n",
      "[4] Training loss: 0.21530, Validation loss: 0.14401\n",
      "[5] Training loss: 0.21525, Validation loss: 0.14410\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22758, Validation loss: 0.35291\n",
      "[2] Training loss: 0.22759, Validation loss: 0.35309\n",
      "[3] Training loss: 0.22759, Validation loss: 0.35324\n",
      "[4] Training loss: 0.22757, Validation loss: 0.35335\n",
      "[5] Training loss: 0.22755, Validation loss: 0.35342\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22359, Validation loss: 0.08327\n",
      "[2] Training loss: 0.22358, Validation loss: 0.08332\n",
      "[3] Training loss: 0.22356, Validation loss: 0.08338\n",
      "[4] Training loss: 0.22354, Validation loss: 0.08345\n",
      "[5] Training loss: 0.22352, Validation loss: 0.08352\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10288, Validation loss: 0.57309\n",
      "[2] Training loss: 0.10288, Validation loss: 0.57328\n",
      "[3] Training loss: 0.10285, Validation loss: 0.57348\n",
      "[4] Training loss: 0.10280, Validation loss: 0.57368\n",
      "[5] Training loss: 0.10273, Validation loss: 0.57389\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20799, Validation loss: 0.44990\n",
      "[2] Training loss: 0.20799, Validation loss: 0.44952\n",
      "[3] Training loss: 0.20796, Validation loss: 0.44925\n",
      "[4] Training loss: 0.20790, Validation loss: 0.44908\n",
      "[5] Training loss: 0.20782, Validation loss: 0.44899\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21522, Validation loss: 0.14398\n",
      "[2] Training loss: 0.21528, Validation loss: 0.14391\n",
      "[3] Training loss: 0.21529, Validation loss: 0.14390\n",
      "[4] Training loss: 0.21528, Validation loss: 0.14394\n",
      "[5] Training loss: 0.21523, Validation loss: 0.14403\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22753, Validation loss: 0.35194\n",
      "[2] Training loss: 0.22754, Validation loss: 0.35203\n",
      "[3] Training loss: 0.22753, Validation loss: 0.35210\n",
      "[4] Training loss: 0.22751, Validation loss: 0.35216\n",
      "[5] Training loss: 0.22749, Validation loss: 0.35220\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22350, Validation loss: 0.08320\n",
      "[2] Training loss: 0.22349, Validation loss: 0.08323\n",
      "[3] Training loss: 0.22347, Validation loss: 0.08328\n",
      "[4] Training loss: 0.22346, Validation loss: 0.08333\n",
      "[5] Training loss: 0.22344, Validation loss: 0.08339\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10278, Validation loss: 0.57441\n",
      "[2] Training loss: 0.10278, Validation loss: 0.57460\n",
      "[3] Training loss: 0.10275, Validation loss: 0.57478\n",
      "[4] Training loss: 0.10270, Validation loss: 0.57496\n",
      "[5] Training loss: 0.10264, Validation loss: 0.57515\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20821, Validation loss: 0.44789\n",
      "[2] Training loss: 0.20819, Validation loss: 0.44773\n",
      "[3] Training loss: 0.20812, Validation loss: 0.44774\n",
      "[4] Training loss: 0.20800, Validation loss: 0.44789\n",
      "[5] Training loss: 0.20785, Validation loss: 0.44815\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21518, Validation loss: 0.14396\n",
      "[2] Training loss: 0.21524, Validation loss: 0.14388\n",
      "[3] Training loss: 0.21527, Validation loss: 0.14386\n",
      "[4] Training loss: 0.21526, Validation loss: 0.14390\n",
      "[5] Training loss: 0.21521, Validation loss: 0.14398\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22753, Validation loss: 0.35280\n",
      "[2] Training loss: 0.22754, Validation loss: 0.35299\n",
      "[3] Training loss: 0.22754, Validation loss: 0.35314\n",
      "[4] Training loss: 0.22753, Validation loss: 0.35327\n",
      "[5] Training loss: 0.22751, Validation loss: 0.35336\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22352, Validation loss: 0.08352\n",
      "[2] Training loss: 0.22351, Validation loss: 0.08357\n",
      "[3] Training loss: 0.22349, Validation loss: 0.08364\n",
      "[4] Training loss: 0.22347, Validation loss: 0.08372\n",
      "[5] Training loss: 0.22345, Validation loss: 0.08380\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10282, Validation loss: 0.57284\n",
      "[2] Training loss: 0.10282, Validation loss: 0.57301\n",
      "[3] Training loss: 0.10280, Validation loss: 0.57319\n",
      "[4] Training loss: 0.10274, Validation loss: 0.57338\n",
      "[5] Training loss: 0.10268, Validation loss: 0.57357\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20791, Validation loss: 0.44937\n",
      "[2] Training loss: 0.20790, Validation loss: 0.44900\n",
      "[3] Training loss: 0.20787, Validation loss: 0.44875\n",
      "[4] Training loss: 0.20782, Validation loss: 0.44859\n",
      "[5] Training loss: 0.20775, Validation loss: 0.44850\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21517, Validation loss: 0.14388\n",
      "[2] Training loss: 0.21522, Validation loss: 0.14381\n",
      "[3] Training loss: 0.21524, Validation loss: 0.14380\n",
      "[4] Training loss: 0.21522, Validation loss: 0.14385\n",
      "[5] Training loss: 0.21517, Validation loss: 0.14394\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22747, Validation loss: 0.35210\n",
      "[2] Training loss: 0.22747, Validation loss: 0.35219\n",
      "[3] Training loss: 0.22746, Validation loss: 0.35226\n",
      "[4] Training loss: 0.22745, Validation loss: 0.35232\n",
      "[5] Training loss: 0.22742, Validation loss: 0.35237\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22343, Validation loss: 0.08338\n",
      "[2] Training loss: 0.22342, Validation loss: 0.08340\n",
      "[3] Training loss: 0.22340, Validation loss: 0.08345\n",
      "[4] Training loss: 0.22338, Validation loss: 0.08350\n",
      "[5] Training loss: 0.22336, Validation loss: 0.08356\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10270, Validation loss: 0.57401\n",
      "[2] Training loss: 0.10270, Validation loss: 0.57419\n",
      "[3] Training loss: 0.10267, Validation loss: 0.57438\n",
      "[4] Training loss: 0.10263, Validation loss: 0.57456\n",
      "[5] Training loss: 0.10256, Validation loss: 0.57475\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20819, Validation loss: 0.44743\n",
      "[2] Training loss: 0.20818, Validation loss: 0.44722\n",
      "[3] Training loss: 0.20812, Validation loss: 0.44718\n",
      "[4] Training loss: 0.20801, Validation loss: 0.44729\n",
      "[5] Training loss: 0.20787, Validation loss: 0.44750\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21510, Validation loss: 0.14392\n",
      "[2] Training loss: 0.21516, Validation loss: 0.14385\n",
      "[3] Training loss: 0.21518, Validation loss: 0.14383\n",
      "[4] Training loss: 0.21516, Validation loss: 0.14387\n",
      "[5] Training loss: 0.21512, Validation loss: 0.14395\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22744, Validation loss: 0.35264\n",
      "[2] Training loss: 0.22745, Validation loss: 0.35282\n",
      "[3] Training loss: 0.22745, Validation loss: 0.35297\n",
      "[4] Training loss: 0.22744, Validation loss: 0.35309\n",
      "[5] Training loss: 0.22742, Validation loss: 0.35318\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22345, Validation loss: 0.08290\n",
      "[2] Training loss: 0.22344, Validation loss: 0.08294\n",
      "[3] Training loss: 0.22342, Validation loss: 0.08299\n",
      "[4] Training loss: 0.22341, Validation loss: 0.08305\n",
      "[5] Training loss: 0.22339, Validation loss: 0.08311\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10274, Validation loss: 0.57339\n",
      "[2] Training loss: 0.10274, Validation loss: 0.57358\n",
      "[3] Training loss: 0.10271, Validation loss: 0.57377\n",
      "[4] Training loss: 0.10266, Validation loss: 0.57396\n",
      "[5] Training loss: 0.10259, Validation loss: 0.57416\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20792, Validation loss: 0.44863\n",
      "[2] Training loss: 0.20792, Validation loss: 0.44830\n",
      "[3] Training loss: 0.20788, Validation loss: 0.44809\n",
      "[4] Training loss: 0.20782, Validation loss: 0.44798\n",
      "[5] Training loss: 0.20774, Validation loss: 0.44797\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21509, Validation loss: 0.14381\n",
      "[2] Training loss: 0.21514, Validation loss: 0.14374\n",
      "[3] Training loss: 0.21516, Validation loss: 0.14373\n",
      "[4] Training loss: 0.21514, Validation loss: 0.14377\n",
      "[5] Training loss: 0.21510, Validation loss: 0.14386\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22739, Validation loss: 0.35195\n",
      "[2] Training loss: 0.22739, Validation loss: 0.35205\n",
      "[3] Training loss: 0.22739, Validation loss: 0.35214\n",
      "[4] Training loss: 0.22737, Validation loss: 0.35222\n",
      "[5] Training loss: 0.22734, Validation loss: 0.35227\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22337, Validation loss: 0.08331\n",
      "[2] Training loss: 0.22335, Validation loss: 0.08335\n",
      "[3] Training loss: 0.22334, Validation loss: 0.08341\n",
      "[4] Training loss: 0.22332, Validation loss: 0.08348\n",
      "[5] Training loss: 0.22330, Validation loss: 0.08355\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10262, Validation loss: 0.57392\n",
      "[2] Training loss: 0.10262, Validation loss: 0.57409\n",
      "[3] Training loss: 0.10260, Validation loss: 0.57426\n",
      "[4] Training loss: 0.10255, Validation loss: 0.57443\n",
      "[5] Training loss: 0.10249, Validation loss: 0.57460\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20816, Validation loss: 0.44734\n",
      "[2] Training loss: 0.20814, Validation loss: 0.44713\n",
      "[3] Training loss: 0.20805, Validation loss: 0.44709\n",
      "[4] Training loss: 0.20793, Validation loss: 0.44717\n",
      "[5] Training loss: 0.20778, Validation loss: 0.44737\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21505, Validation loss: 0.14382\n",
      "[2] Training loss: 0.21511, Validation loss: 0.14374\n",
      "[3] Training loss: 0.21513, Validation loss: 0.14373\n",
      "[4] Training loss: 0.21512, Validation loss: 0.14376\n",
      "[5] Training loss: 0.21508, Validation loss: 0.14385\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22740, Validation loss: 0.35301\n",
      "[2] Training loss: 0.22741, Validation loss: 0.35319\n",
      "[3] Training loss: 0.22741, Validation loss: 0.35334\n",
      "[4] Training loss: 0.22740, Validation loss: 0.35346\n",
      "[5] Training loss: 0.22738, Validation loss: 0.35354\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22339, Validation loss: 0.08330\n",
      "[2] Training loss: 0.22338, Validation loss: 0.08335\n",
      "[3] Training loss: 0.22337, Validation loss: 0.08340\n",
      "[4] Training loss: 0.22335, Validation loss: 0.08347\n",
      "[5] Training loss: 0.22333, Validation loss: 0.08354\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10272, Validation loss: 0.57280\n",
      "[2] Training loss: 0.10272, Validation loss: 0.57299\n",
      "[3] Training loss: 0.10269, Validation loss: 0.57319\n",
      "[4] Training loss: 0.10264, Validation loss: 0.57339\n",
      "[5] Training loss: 0.10256, Validation loss: 0.57361\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20777, Validation loss: 0.44808\n",
      "[2] Training loss: 0.20777, Validation loss: 0.44773\n",
      "[3] Training loss: 0.20774, Validation loss: 0.44748\n",
      "[4] Training loss: 0.20768, Validation loss: 0.44733\n",
      "[5] Training loss: 0.20761, Validation loss: 0.44727\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21505, Validation loss: 0.14369\n",
      "[2] Training loss: 0.21510, Validation loss: 0.14363\n",
      "[3] Training loss: 0.21512, Validation loss: 0.14362\n",
      "[4] Training loss: 0.21510, Validation loss: 0.14366\n",
      "[5] Training loss: 0.21505, Validation loss: 0.14375\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22735, Validation loss: 0.35207\n",
      "[2] Training loss: 0.22736, Validation loss: 0.35217\n",
      "[3] Training loss: 0.22735, Validation loss: 0.35225\n",
      "[4] Training loss: 0.22733, Validation loss: 0.35232\n",
      "[5] Training loss: 0.22730, Validation loss: 0.35237\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22330, Validation loss: 0.08337\n",
      "[2] Training loss: 0.22329, Validation loss: 0.08340\n",
      "[3] Training loss: 0.22327, Validation loss: 0.08345\n",
      "[4] Training loss: 0.22325, Validation loss: 0.08350\n",
      "[5] Training loss: 0.22323, Validation loss: 0.08356\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10257, Validation loss: 0.57386\n",
      "[2] Training loss: 0.10257, Validation loss: 0.57403\n",
      "[3] Training loss: 0.10255, Validation loss: 0.57421\n",
      "[4] Training loss: 0.10250, Validation loss: 0.57439\n",
      "[5] Training loss: 0.10243, Validation loss: 0.57457\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20805, Validation loss: 0.44622\n",
      "[2] Training loss: 0.20804, Validation loss: 0.44598\n",
      "[3] Training loss: 0.20798, Validation loss: 0.44591\n",
      "[4] Training loss: 0.20788, Validation loss: 0.44598\n",
      "[5] Training loss: 0.20775, Validation loss: 0.44617\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21496, Validation loss: 0.14381\n",
      "[2] Training loss: 0.21502, Validation loss: 0.14374\n",
      "[3] Training loss: 0.21504, Validation loss: 0.14373\n",
      "[4] Training loss: 0.21502, Validation loss: 0.14378\n",
      "[5] Training loss: 0.21498, Validation loss: 0.14387\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22732, Validation loss: 0.35309\n",
      "[2] Training loss: 0.22733, Validation loss: 0.35328\n",
      "[3] Training loss: 0.22733, Validation loss: 0.35342\n",
      "[4] Training loss: 0.22732, Validation loss: 0.35354\n",
      "[5] Training loss: 0.22730, Validation loss: 0.35362\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22332, Validation loss: 0.08294\n",
      "[2] Training loss: 0.22331, Validation loss: 0.08298\n",
      "[3] Training loss: 0.22330, Validation loss: 0.08303\n",
      "[4] Training loss: 0.22328, Validation loss: 0.08310\n",
      "[5] Training loss: 0.22326, Validation loss: 0.08317\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10259, Validation loss: 0.57300\n",
      "[2] Training loss: 0.10259, Validation loss: 0.57319\n",
      "[3] Training loss: 0.10256, Validation loss: 0.57339\n",
      "[4] Training loss: 0.10251, Validation loss: 0.57360\n",
      "[5] Training loss: 0.10244, Validation loss: 0.57381\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20782, Validation loss: 0.44706\n",
      "[2] Training loss: 0.20782, Validation loss: 0.44672\n",
      "[3] Training loss: 0.20778, Validation loss: 0.44649\n",
      "[4] Training loss: 0.20772, Validation loss: 0.44636\n",
      "[5] Training loss: 0.20764, Validation loss: 0.44632\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21495, Validation loss: 0.14372\n",
      "[2] Training loss: 0.21500, Validation loss: 0.14365\n",
      "[3] Training loss: 0.21502, Validation loss: 0.14363\n",
      "[4] Training loss: 0.21500, Validation loss: 0.14368\n",
      "[5] Training loss: 0.21496, Validation loss: 0.14376\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22728, Validation loss: 0.35207\n",
      "[2] Training loss: 0.22728, Validation loss: 0.35217\n",
      "[3] Training loss: 0.22727, Validation loss: 0.35225\n",
      "[4] Training loss: 0.22725, Validation loss: 0.35232\n",
      "[5] Training loss: 0.22723, Validation loss: 0.35237\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22323, Validation loss: 0.08329\n",
      "[2] Training loss: 0.22321, Validation loss: 0.08333\n",
      "[3] Training loss: 0.22320, Validation loss: 0.08338\n",
      "[4] Training loss: 0.22318, Validation loss: 0.08345\n",
      "[5] Training loss: 0.22316, Validation loss: 0.08352\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10247, Validation loss: 0.57385\n",
      "[2] Training loss: 0.10247, Validation loss: 0.57402\n",
      "[3] Training loss: 0.10244, Validation loss: 0.57419\n",
      "[4] Training loss: 0.10240, Validation loss: 0.57436\n",
      "[5] Training loss: 0.10233, Validation loss: 0.57453\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20808, Validation loss: 0.44567\n",
      "[2] Training loss: 0.20806, Validation loss: 0.44546\n",
      "[3] Training loss: 0.20796, Validation loss: 0.44543\n",
      "[4] Training loss: 0.20784, Validation loss: 0.44552\n",
      "[5] Training loss: 0.20770, Validation loss: 0.44572\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21491, Validation loss: 0.14372\n",
      "[2] Training loss: 0.21497, Validation loss: 0.14364\n",
      "[3] Training loss: 0.21499, Validation loss: 0.14362\n",
      "[4] Training loss: 0.21498, Validation loss: 0.14366\n",
      "[5] Training loss: 0.21494, Validation loss: 0.14374\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22728, Validation loss: 0.35319\n",
      "[2] Training loss: 0.22729, Validation loss: 0.35338\n",
      "[3] Training loss: 0.22729, Validation loss: 0.35353\n",
      "[4] Training loss: 0.22728, Validation loss: 0.35365\n",
      "[5] Training loss: 0.22726, Validation loss: 0.35374\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22326, Validation loss: 0.08327\n",
      "[2] Training loss: 0.22325, Validation loss: 0.08331\n",
      "[3] Training loss: 0.22323, Validation loss: 0.08337\n",
      "[4] Training loss: 0.22321, Validation loss: 0.08344\n",
      "[5] Training loss: 0.22319, Validation loss: 0.08351\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10257, Validation loss: 0.57266\n",
      "[2] Training loss: 0.10257, Validation loss: 0.57285\n",
      "[3] Training loss: 0.10254, Validation loss: 0.57305\n",
      "[4] Training loss: 0.10249, Validation loss: 0.57325\n",
      "[5] Training loss: 0.10241, Validation loss: 0.57346\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20769, Validation loss: 0.44652\n",
      "[2] Training loss: 0.20768, Validation loss: 0.44617\n",
      "[3] Training loss: 0.20765, Validation loss: 0.44591\n",
      "[4] Training loss: 0.20759, Validation loss: 0.44576\n",
      "[5] Training loss: 0.20752, Validation loss: 0.44569\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21491, Validation loss: 0.14359\n",
      "[2] Training loss: 0.21496, Validation loss: 0.14352\n",
      "[3] Training loss: 0.21498, Validation loss: 0.14352\n",
      "[4] Training loss: 0.21496, Validation loss: 0.14356\n",
      "[5] Training loss: 0.21491, Validation loss: 0.14365\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22724, Validation loss: 0.35220\n",
      "[2] Training loss: 0.22724, Validation loss: 0.35230\n",
      "[3] Training loss: 0.22723, Validation loss: 0.35238\n",
      "[4] Training loss: 0.22721, Validation loss: 0.35245\n",
      "[5] Training loss: 0.22718, Validation loss: 0.35250\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22316, Validation loss: 0.08324\n",
      "[2] Training loss: 0.22315, Validation loss: 0.08327\n",
      "[3] Training loss: 0.22313, Validation loss: 0.08332\n",
      "[4] Training loss: 0.22311, Validation loss: 0.08337\n",
      "[5] Training loss: 0.22309, Validation loss: 0.08343\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10242, Validation loss: 0.57379\n",
      "[2] Training loss: 0.10242, Validation loss: 0.57397\n",
      "[3] Training loss: 0.10239, Validation loss: 0.57414\n",
      "[4] Training loss: 0.10235, Validation loss: 0.57432\n",
      "[5] Training loss: 0.10228, Validation loss: 0.57450\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20796, Validation loss: 0.44470\n",
      "[2] Training loss: 0.20795, Validation loss: 0.44452\n",
      "[3] Training loss: 0.20789, Validation loss: 0.44449\n",
      "[4] Training loss: 0.20779, Validation loss: 0.44461\n",
      "[5] Training loss: 0.20766, Validation loss: 0.44482\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21483, Validation loss: 0.14366\n",
      "[2] Training loss: 0.21489, Validation loss: 0.14359\n",
      "[3] Training loss: 0.21491, Validation loss: 0.14357\n",
      "[4] Training loss: 0.21489, Validation loss: 0.14361\n",
      "[5] Training loss: 0.21485, Validation loss: 0.14370\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22719, Validation loss: 0.35307\n",
      "[2] Training loss: 0.22720, Validation loss: 0.35325\n",
      "[3] Training loss: 0.22720, Validation loss: 0.35341\n",
      "[4] Training loss: 0.22718, Validation loss: 0.35353\n",
      "[5] Training loss: 0.22716, Validation loss: 0.35362\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22318, Validation loss: 0.08287\n",
      "[2] Training loss: 0.22317, Validation loss: 0.08291\n",
      "[3] Training loss: 0.22315, Validation loss: 0.08296\n",
      "[4] Training loss: 0.22313, Validation loss: 0.08301\n",
      "[5] Training loss: 0.22312, Validation loss: 0.08308\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10244, Validation loss: 0.57302\n",
      "[2] Training loss: 0.10244, Validation loss: 0.57321\n",
      "[3] Training loss: 0.10242, Validation loss: 0.57340\n",
      "[4] Training loss: 0.10236, Validation loss: 0.57359\n",
      "[5] Training loss: 0.10229, Validation loss: 0.57379\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20774, Validation loss: 0.44587\n",
      "[2] Training loss: 0.20773, Validation loss: 0.44555\n",
      "[3] Training loss: 0.20770, Validation loss: 0.44538\n",
      "[4] Training loss: 0.20763, Validation loss: 0.44529\n",
      "[5] Training loss: 0.20756, Validation loss: 0.44528\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21481, Validation loss: 0.14358\n",
      "[2] Training loss: 0.21486, Validation loss: 0.14351\n",
      "[3] Training loss: 0.21488, Validation loss: 0.14350\n",
      "[4] Training loss: 0.21486, Validation loss: 0.14354\n",
      "[5] Training loss: 0.21482, Validation loss: 0.14363\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22714, Validation loss: 0.35241\n",
      "[2] Training loss: 0.22714, Validation loss: 0.35250\n",
      "[3] Training loss: 0.22713, Validation loss: 0.35259\n",
      "[4] Training loss: 0.22712, Validation loss: 0.35266\n",
      "[5] Training loss: 0.22709, Validation loss: 0.35271\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22310, Validation loss: 0.08302\n",
      "[2] Training loss: 0.22309, Validation loss: 0.08306\n",
      "[3] Training loss: 0.22307, Validation loss: 0.08311\n",
      "[4] Training loss: 0.22306, Validation loss: 0.08317\n",
      "[5] Training loss: 0.22304, Validation loss: 0.08324\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10233, Validation loss: 0.57369\n",
      "[2] Training loss: 0.10233, Validation loss: 0.57386\n",
      "[3] Training loss: 0.10231, Validation loss: 0.57404\n",
      "[4] Training loss: 0.10226, Validation loss: 0.57422\n",
      "[5] Training loss: 0.10219, Validation loss: 0.57440\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20797, Validation loss: 0.44475\n",
      "[2] Training loss: 0.20795, Validation loss: 0.44451\n",
      "[3] Training loss: 0.20785, Validation loss: 0.44442\n",
      "[4] Training loss: 0.20773, Validation loss: 0.44446\n",
      "[5] Training loss: 0.20758, Validation loss: 0.44459\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21477, Validation loss: 0.14357\n",
      "[2] Training loss: 0.21483, Validation loss: 0.14350\n",
      "[3] Training loss: 0.21486, Validation loss: 0.14348\n",
      "[4] Training loss: 0.21485, Validation loss: 0.14352\n",
      "[5] Training loss: 0.21481, Validation loss: 0.14360\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22715, Validation loss: 0.35327\n",
      "[2] Training loss: 0.22716, Validation loss: 0.35345\n",
      "[3] Training loss: 0.22716, Validation loss: 0.35359\n",
      "[4] Training loss: 0.22715, Validation loss: 0.35369\n",
      "[5] Training loss: 0.22713, Validation loss: 0.35377\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22312, Validation loss: 0.08320\n",
      "[2] Training loss: 0.22311, Validation loss: 0.08325\n",
      "[3] Training loss: 0.22310, Validation loss: 0.08331\n",
      "[4] Training loss: 0.22308, Validation loss: 0.08339\n",
      "[5] Training loss: 0.22306, Validation loss: 0.08347\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10243, Validation loss: 0.57262\n",
      "[2] Training loss: 0.10243, Validation loss: 0.57282\n",
      "[3] Training loss: 0.10240, Validation loss: 0.57302\n",
      "[4] Training loss: 0.10235, Validation loss: 0.57323\n",
      "[5] Training loss: 0.10227, Validation loss: 0.57344\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20759, Validation loss: 0.44491\n",
      "[2] Training loss: 0.20758, Validation loss: 0.44458\n",
      "[3] Training loss: 0.20755, Validation loss: 0.44439\n",
      "[4] Training loss: 0.20749, Validation loss: 0.44432\n",
      "[5] Training loss: 0.20741, Validation loss: 0.44431\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21478, Validation loss: 0.14344\n",
      "[2] Training loss: 0.21483, Validation loss: 0.14337\n",
      "[3] Training loss: 0.21485, Validation loss: 0.14336\n",
      "[4] Training loss: 0.21483, Validation loss: 0.14340\n",
      "[5] Training loss: 0.21478, Validation loss: 0.14350\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22712, Validation loss: 0.35240\n",
      "[2] Training loss: 0.22712, Validation loss: 0.35251\n",
      "[3] Training loss: 0.22711, Validation loss: 0.35260\n",
      "[4] Training loss: 0.22710, Validation loss: 0.35268\n",
      "[5] Training loss: 0.22707, Validation loss: 0.35273\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22304, Validation loss: 0.08310\n",
      "[2] Training loss: 0.22303, Validation loss: 0.08313\n",
      "[3] Training loss: 0.22302, Validation loss: 0.08317\n",
      "[4] Training loss: 0.22300, Validation loss: 0.08322\n",
      "[5] Training loss: 0.22298, Validation loss: 0.08328\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10230, Validation loss: 0.57364\n",
      "[2] Training loss: 0.10230, Validation loss: 0.57382\n",
      "[3] Training loss: 0.10227, Validation loss: 0.57400\n",
      "[4] Training loss: 0.10222, Validation loss: 0.57419\n",
      "[5] Training loss: 0.10216, Validation loss: 0.57437\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20779, Validation loss: 0.44389\n",
      "[2] Training loss: 0.20778, Validation loss: 0.44371\n",
      "[3] Training loss: 0.20772, Validation loss: 0.44369\n",
      "[4] Training loss: 0.20763, Validation loss: 0.44380\n",
      "[5] Training loss: 0.20750, Validation loss: 0.44400\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21470, Validation loss: 0.14351\n",
      "[2] Training loss: 0.21476, Validation loss: 0.14344\n",
      "[3] Training loss: 0.21478, Validation loss: 0.14342\n",
      "[4] Training loss: 0.21477, Validation loss: 0.14346\n",
      "[5] Training loss: 0.21472, Validation loss: 0.14355\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22708, Validation loss: 0.35341\n",
      "[2] Training loss: 0.22709, Validation loss: 0.35360\n",
      "[3] Training loss: 0.22709, Validation loss: 0.35375\n",
      "[4] Training loss: 0.22708, Validation loss: 0.35387\n",
      "[5] Training loss: 0.22706, Validation loss: 0.35396\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22305, Validation loss: 0.08312\n",
      "[2] Training loss: 0.22304, Validation loss: 0.08317\n",
      "[3] Training loss: 0.22303, Validation loss: 0.08323\n",
      "[4] Training loss: 0.22301, Validation loss: 0.08331\n",
      "[5] Training loss: 0.22299, Validation loss: 0.08338\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10230, Validation loss: 0.57243\n",
      "[2] Training loss: 0.10230, Validation loss: 0.57260\n",
      "[3] Training loss: 0.10227, Validation loss: 0.57279\n",
      "[4] Training loss: 0.10222, Validation loss: 0.57299\n",
      "[5] Training loss: 0.10215, Validation loss: 0.57318\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20762, Validation loss: 0.44499\n",
      "[2] Training loss: 0.20761, Validation loss: 0.44464\n",
      "[3] Training loss: 0.20758, Validation loss: 0.44440\n",
      "[4] Training loss: 0.20752, Validation loss: 0.44424\n",
      "[5] Training loss: 0.20745, Validation loss: 0.44416\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21468, Validation loss: 0.14346\n",
      "[2] Training loss: 0.21473, Validation loss: 0.14340\n",
      "[3] Training loss: 0.21474, Validation loss: 0.14339\n",
      "[4] Training loss: 0.21472, Validation loss: 0.14344\n",
      "[5] Training loss: 0.21468, Validation loss: 0.14353\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22702, Validation loss: 0.35246\n",
      "[2] Training loss: 0.22702, Validation loss: 0.35255\n",
      "[3] Training loss: 0.22701, Validation loss: 0.35262\n",
      "[4] Training loss: 0.22699, Validation loss: 0.35268\n",
      "[5] Training loss: 0.22697, Validation loss: 0.35272\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22296, Validation loss: 0.08291\n",
      "[2] Training loss: 0.22295, Validation loss: 0.08293\n",
      "[3] Training loss: 0.22294, Validation loss: 0.08297\n",
      "[4] Training loss: 0.22292, Validation loss: 0.08302\n",
      "[5] Training loss: 0.22290, Validation loss: 0.08308\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10219, Validation loss: 0.57380\n",
      "[2] Training loss: 0.10219, Validation loss: 0.57398\n",
      "[3] Training loss: 0.10216, Validation loss: 0.57417\n",
      "[4] Training loss: 0.10211, Validation loss: 0.57435\n",
      "[5] Training loss: 0.10205, Validation loss: 0.57454\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20791, Validation loss: 0.44318\n",
      "[2] Training loss: 0.20788, Validation loss: 0.44302\n",
      "[3] Training loss: 0.20778, Validation loss: 0.44302\n",
      "[4] Training loss: 0.20765, Validation loss: 0.44315\n",
      "[5] Training loss: 0.20750, Validation loss: 0.44335\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21464, Validation loss: 0.14344\n",
      "[2] Training loss: 0.21470, Validation loss: 0.14337\n",
      "[3] Training loss: 0.21472, Validation loss: 0.14335\n",
      "[4] Training loss: 0.21472, Validation loss: 0.14338\n",
      "[5] Training loss: 0.21468, Validation loss: 0.14346\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22703, Validation loss: 0.35347\n",
      "[2] Training loss: 0.22704, Validation loss: 0.35367\n",
      "[3] Training loss: 0.22705, Validation loss: 0.35383\n",
      "[4] Training loss: 0.22704, Validation loss: 0.35396\n",
      "[5] Training loss: 0.22702, Validation loss: 0.35405\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22299, Validation loss: 0.08336\n",
      "[2] Training loss: 0.22297, Validation loss: 0.08342\n",
      "[3] Training loss: 0.22296, Validation loss: 0.08349\n",
      "[4] Training loss: 0.22294, Validation loss: 0.08357\n",
      "[5] Training loss: 0.22292, Validation loss: 0.08366\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10228, Validation loss: 0.57208\n",
      "[2] Training loss: 0.10228, Validation loss: 0.57226\n",
      "[3] Training loss: 0.10225, Validation loss: 0.57245\n",
      "[4] Training loss: 0.10220, Validation loss: 0.57264\n",
      "[5] Training loss: 0.10212, Validation loss: 0.57284\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20750, Validation loss: 0.44428\n",
      "[2] Training loss: 0.20749, Validation loss: 0.44390\n",
      "[3] Training loss: 0.20746, Validation loss: 0.44363\n",
      "[4] Training loss: 0.20741, Validation loss: 0.44345\n",
      "[5] Training loss: 0.20734, Validation loss: 0.44335\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21464, Validation loss: 0.14333\n",
      "[2] Training loss: 0.21469, Validation loss: 0.14327\n",
      "[3] Training loss: 0.21471, Validation loss: 0.14326\n",
      "[4] Training loss: 0.21469, Validation loss: 0.14331\n",
      "[5] Training loss: 0.21464, Validation loss: 0.14340\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22698, Validation loss: 0.35257\n",
      "[2] Training loss: 0.22698, Validation loss: 0.35266\n",
      "[3] Training loss: 0.22698, Validation loss: 0.35273\n",
      "[4] Training loss: 0.22696, Validation loss: 0.35279\n",
      "[5] Training loss: 0.22693, Validation loss: 0.35283\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22289, Validation loss: 0.08309\n",
      "[2] Training loss: 0.22288, Validation loss: 0.08311\n",
      "[3] Training loss: 0.22286, Validation loss: 0.08315\n",
      "[4] Training loss: 0.22285, Validation loss: 0.08320\n",
      "[5] Training loss: 0.22283, Validation loss: 0.08325\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10213, Validation loss: 0.57356\n",
      "[2] Training loss: 0.10213, Validation loss: 0.57375\n",
      "[3] Training loss: 0.10211, Validation loss: 0.57394\n",
      "[4] Training loss: 0.10206, Validation loss: 0.57412\n",
      "[5] Training loss: 0.10199, Validation loss: 0.57431\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20778, Validation loss: 0.44217\n",
      "[2] Training loss: 0.20777, Validation loss: 0.44195\n",
      "[3] Training loss: 0.20771, Validation loss: 0.44190\n",
      "[4] Training loss: 0.20761, Validation loss: 0.44199\n",
      "[5] Training loss: 0.20747, Validation loss: 0.44218\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21455, Validation loss: 0.14344\n",
      "[2] Training loss: 0.21461, Validation loss: 0.14337\n",
      "[3] Training loss: 0.21463, Validation loss: 0.14336\n",
      "[4] Training loss: 0.21462, Validation loss: 0.14341\n",
      "[5] Training loss: 0.21457, Validation loss: 0.14349\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22695, Validation loss: 0.35356\n",
      "[2] Training loss: 0.22696, Validation loss: 0.35374\n",
      "[3] Training loss: 0.22696, Validation loss: 0.35390\n",
      "[4] Training loss: 0.22695, Validation loss: 0.35401\n",
      "[5] Training loss: 0.22693, Validation loss: 0.35409\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22291, Validation loss: 0.08297\n",
      "[2] Training loss: 0.22290, Validation loss: 0.08302\n",
      "[3] Training loss: 0.22289, Validation loss: 0.08309\n",
      "[4] Training loss: 0.22287, Validation loss: 0.08316\n",
      "[5] Training loss: 0.22285, Validation loss: 0.08324\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10214, Validation loss: 0.57238\n",
      "[2] Training loss: 0.10214, Validation loss: 0.57256\n",
      "[3] Training loss: 0.10211, Validation loss: 0.57275\n",
      "[4] Training loss: 0.10206, Validation loss: 0.57295\n",
      "[5] Training loss: 0.10199, Validation loss: 0.57316\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20754, Validation loss: 0.44324\n",
      "[2] Training loss: 0.20754, Validation loss: 0.44289\n",
      "[3] Training loss: 0.20750, Validation loss: 0.44267\n",
      "[4] Training loss: 0.20745, Validation loss: 0.44255\n",
      "[5] Training loss: 0.20737, Validation loss: 0.44248\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21454, Validation loss: 0.14337\n",
      "[2] Training loss: 0.21459, Validation loss: 0.14331\n",
      "[3] Training loss: 0.21461, Validation loss: 0.14330\n",
      "[4] Training loss: 0.21459, Validation loss: 0.14334\n",
      "[5] Training loss: 0.21454, Validation loss: 0.14343\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22690, Validation loss: 0.35268\n",
      "[2] Training loss: 0.22690, Validation loss: 0.35278\n",
      "[3] Training loss: 0.22689, Validation loss: 0.35285\n",
      "[4] Training loss: 0.22688, Validation loss: 0.35291\n",
      "[5] Training loss: 0.22685, Validation loss: 0.35295\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22283, Validation loss: 0.08266\n",
      "[2] Training loss: 0.22282, Validation loss: 0.08268\n",
      "[3] Training loss: 0.22280, Validation loss: 0.08272\n",
      "[4] Training loss: 0.22279, Validation loss: 0.08277\n",
      "[5] Training loss: 0.22277, Validation loss: 0.08282\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10203, Validation loss: 0.57375\n",
      "[2] Training loss: 0.10203, Validation loss: 0.57394\n",
      "[3] Training loss: 0.10201, Validation loss: 0.57412\n",
      "[4] Training loss: 0.10196, Validation loss: 0.57431\n",
      "[5] Training loss: 0.10189, Validation loss: 0.57449\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20780, Validation loss: 0.44179\n",
      "[2] Training loss: 0.20778, Validation loss: 0.44161\n",
      "[3] Training loss: 0.20768, Validation loss: 0.44158\n",
      "[4] Training loss: 0.20755, Validation loss: 0.44169\n",
      "[5] Training loss: 0.20740, Validation loss: 0.44188\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21450, Validation loss: 0.14334\n",
      "[2] Training loss: 0.21457, Validation loss: 0.14326\n",
      "[3] Training loss: 0.21459, Validation loss: 0.14324\n",
      "[4] Training loss: 0.21458, Validation loss: 0.14328\n",
      "[5] Training loss: 0.21454, Validation loss: 0.14336\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22691, Validation loss: 0.35363\n",
      "[2] Training loss: 0.22693, Validation loss: 0.35382\n",
      "[3] Training loss: 0.22693, Validation loss: 0.35397\n",
      "[4] Training loss: 0.22692, Validation loss: 0.35409\n",
      "[5] Training loss: 0.22690, Validation loss: 0.35417\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22285, Validation loss: 0.08330\n",
      "[2] Training loss: 0.22284, Validation loss: 0.08337\n",
      "[3] Training loss: 0.22283, Validation loss: 0.08345\n",
      "[4] Training loss: 0.22281, Validation loss: 0.08353\n",
      "[5] Training loss: 0.22279, Validation loss: 0.08362\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10213, Validation loss: 0.57192\n",
      "[2] Training loss: 0.10213, Validation loss: 0.57210\n",
      "[3] Training loss: 0.10210, Validation loss: 0.57228\n",
      "[4] Training loss: 0.10204, Validation loss: 0.57248\n",
      "[5] Training loss: 0.10197, Validation loss: 0.57268\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20740, Validation loss: 0.44277\n",
      "[2] Training loss: 0.20740, Validation loss: 0.44241\n",
      "[3] Training loss: 0.20737, Validation loss: 0.44216\n",
      "[4] Training loss: 0.20732, Validation loss: 0.44201\n",
      "[5] Training loss: 0.20725, Validation loss: 0.44192\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21450, Validation loss: 0.14322\n",
      "[2] Training loss: 0.21456, Validation loss: 0.14316\n",
      "[3] Training loss: 0.21457, Validation loss: 0.14315\n",
      "[4] Training loss: 0.21455, Validation loss: 0.14320\n",
      "[5] Training loss: 0.21450, Validation loss: 0.14329\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22686, Validation loss: 0.35272\n",
      "[2] Training loss: 0.22686, Validation loss: 0.35281\n",
      "[3] Training loss: 0.22685, Validation loss: 0.35289\n",
      "[4] Training loss: 0.22683, Validation loss: 0.35295\n",
      "[5] Training loss: 0.22681, Validation loss: 0.35299\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22276, Validation loss: 0.08296\n",
      "[2] Training loss: 0.22275, Validation loss: 0.08298\n",
      "[3] Training loss: 0.22273, Validation loss: 0.08301\n",
      "[4] Training loss: 0.22272, Validation loss: 0.08306\n",
      "[5] Training loss: 0.22270, Validation loss: 0.08311\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10199, Validation loss: 0.57340\n",
      "[2] Training loss: 0.10199, Validation loss: 0.57359\n",
      "[3] Training loss: 0.10196, Validation loss: 0.57378\n",
      "[4] Training loss: 0.10191, Validation loss: 0.57397\n",
      "[5] Training loss: 0.10185, Validation loss: 0.57416\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20770, Validation loss: 0.44096\n",
      "[2] Training loss: 0.20769, Validation loss: 0.44078\n",
      "[3] Training loss: 0.20763, Validation loss: 0.44077\n",
      "[4] Training loss: 0.20752, Validation loss: 0.44088\n",
      "[5] Training loss: 0.20739, Validation loss: 0.44108\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21442, Validation loss: 0.14328\n",
      "[2] Training loss: 0.21448, Validation loss: 0.14321\n",
      "[3] Training loss: 0.21450, Validation loss: 0.14320\n",
      "[4] Training loss: 0.21449, Validation loss: 0.14324\n",
      "[5] Training loss: 0.21445, Validation loss: 0.14333\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22683, Validation loss: 0.35341\n",
      "[2] Training loss: 0.22684, Validation loss: 0.35360\n",
      "[3] Training loss: 0.22684, Validation loss: 0.35377\n",
      "[4] Training loss: 0.22683, Validation loss: 0.35390\n",
      "[5] Training loss: 0.22681, Validation loss: 0.35399\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22277, Validation loss: 0.08302\n",
      "[2] Training loss: 0.22276, Validation loss: 0.08307\n",
      "[3] Training loss: 0.22275, Validation loss: 0.08312\n",
      "[4] Training loss: 0.22273, Validation loss: 0.08319\n",
      "[5] Training loss: 0.22271, Validation loss: 0.08327\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10200, Validation loss: 0.57219\n",
      "[2] Training loss: 0.10200, Validation loss: 0.57236\n",
      "[3] Training loss: 0.10197, Validation loss: 0.57254\n",
      "[4] Training loss: 0.10192, Validation loss: 0.57273\n",
      "[5] Training loss: 0.10185, Validation loss: 0.57292\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20745, Validation loss: 0.44229\n",
      "[2] Training loss: 0.20744, Validation loss: 0.44197\n",
      "[3] Training loss: 0.20741, Validation loss: 0.44176\n",
      "[4] Training loss: 0.20735, Validation loss: 0.44162\n",
      "[5] Training loss: 0.20728, Validation loss: 0.44156\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21440, Validation loss: 0.14324\n",
      "[2] Training loss: 0.21446, Validation loss: 0.14318\n",
      "[3] Training loss: 0.21447, Validation loss: 0.14317\n",
      "[4] Training loss: 0.21445, Validation loss: 0.14321\n",
      "[5] Training loss: 0.21441, Validation loss: 0.14331\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22677, Validation loss: 0.35295\n",
      "[2] Training loss: 0.22677, Validation loss: 0.35306\n",
      "[3] Training loss: 0.22676, Validation loss: 0.35314\n",
      "[4] Training loss: 0.22675, Validation loss: 0.35320\n",
      "[5] Training loss: 0.22672, Validation loss: 0.35325\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22270, Validation loss: 0.08284\n",
      "[2] Training loss: 0.22268, Validation loss: 0.08287\n",
      "[3] Training loss: 0.22267, Validation loss: 0.08291\n",
      "[4] Training loss: 0.22265, Validation loss: 0.08296\n",
      "[5] Training loss: 0.22263, Validation loss: 0.08302\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10189, Validation loss: 0.57319\n",
      "[2] Training loss: 0.10189, Validation loss: 0.57338\n",
      "[3] Training loss: 0.10186, Validation loss: 0.57356\n",
      "[4] Training loss: 0.10182, Validation loss: 0.57375\n",
      "[5] Training loss: 0.10175, Validation loss: 0.57394\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20771, Validation loss: 0.44076\n",
      "[2] Training loss: 0.20768, Validation loss: 0.44056\n",
      "[3] Training loss: 0.20759, Validation loss: 0.44049\n",
      "[4] Training loss: 0.20748, Validation loss: 0.44055\n",
      "[5] Training loss: 0.20733, Validation loss: 0.44068\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21437, Validation loss: 0.14322\n",
      "[2] Training loss: 0.21443, Validation loss: 0.14315\n",
      "[3] Training loss: 0.21445, Validation loss: 0.14313\n",
      "[4] Training loss: 0.21444, Validation loss: 0.14317\n",
      "[5] Training loss: 0.21440, Validation loss: 0.14325\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22677, Validation loss: 0.35361\n",
      "[2] Training loss: 0.22678, Validation loss: 0.35379\n",
      "[3] Training loss: 0.22678, Validation loss: 0.35393\n",
      "[4] Training loss: 0.22678, Validation loss: 0.35404\n",
      "[5] Training loss: 0.22676, Validation loss: 0.35411\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22271, Validation loss: 0.08301\n",
      "[2] Training loss: 0.22270, Validation loss: 0.08306\n",
      "[3] Training loss: 0.22269, Validation loss: 0.08312\n",
      "[4] Training loss: 0.22267, Validation loss: 0.08319\n",
      "[5] Training loss: 0.22265, Validation loss: 0.08326\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10197, Validation loss: 0.57220\n",
      "[2] Training loss: 0.10197, Validation loss: 0.57238\n",
      "[3] Training loss: 0.10194, Validation loss: 0.57258\n",
      "[4] Training loss: 0.10189, Validation loss: 0.57277\n",
      "[5] Training loss: 0.10182, Validation loss: 0.57298\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20734, Validation loss: 0.44128\n",
      "[2] Training loss: 0.20734, Validation loss: 0.44095\n",
      "[3] Training loss: 0.20731, Validation loss: 0.44076\n",
      "[4] Training loss: 0.20725, Validation loss: 0.44066\n",
      "[5] Training loss: 0.20717, Validation loss: 0.44063\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21436, Validation loss: 0.14311\n",
      "[2] Training loss: 0.21442, Validation loss: 0.14304\n",
      "[3] Training loss: 0.21443, Validation loss: 0.14304\n",
      "[4] Training loss: 0.21441, Validation loss: 0.14308\n",
      "[5] Training loss: 0.21437, Validation loss: 0.14317\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22673, Validation loss: 0.35306\n",
      "[2] Training loss: 0.22673, Validation loss: 0.35318\n",
      "[3] Training loss: 0.22672, Validation loss: 0.35326\n",
      "[4] Training loss: 0.22671, Validation loss: 0.35333\n",
      "[5] Training loss: 0.22668, Validation loss: 0.35338\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22264, Validation loss: 0.08284\n",
      "[2] Training loss: 0.22262, Validation loss: 0.08287\n",
      "[3] Training loss: 0.22261, Validation loss: 0.08292\n",
      "[4] Training loss: 0.22259, Validation loss: 0.08298\n",
      "[5] Training loss: 0.22257, Validation loss: 0.08304\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10184, Validation loss: 0.57300\n",
      "[2] Training loss: 0.10184, Validation loss: 0.57318\n",
      "[3] Training loss: 0.10181, Validation loss: 0.57337\n",
      "[4] Training loss: 0.10176, Validation loss: 0.57356\n",
      "[5] Training loss: 0.10170, Validation loss: 0.57374\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20758, Validation loss: 0.44018\n",
      "[2] Training loss: 0.20757, Validation loss: 0.43999\n",
      "[3] Training loss: 0.20750, Validation loss: 0.43996\n",
      "[4] Training loss: 0.20740, Validation loss: 0.44004\n",
      "[5] Training loss: 0.20727, Validation loss: 0.44021\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21430, Validation loss: 0.14314\n",
      "[2] Training loss: 0.21436, Validation loss: 0.14307\n",
      "[3] Training loss: 0.21438, Validation loss: 0.14306\n",
      "[4] Training loss: 0.21436, Validation loss: 0.14310\n",
      "[5] Training loss: 0.21432, Validation loss: 0.14318\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22670, Validation loss: 0.35353\n",
      "[2] Training loss: 0.22671, Validation loss: 0.35370\n",
      "[3] Training loss: 0.22671, Validation loss: 0.35383\n",
      "[4] Training loss: 0.22670, Validation loss: 0.35393\n",
      "[5] Training loss: 0.22668, Validation loss: 0.35400\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22264, Validation loss: 0.08286\n",
      "[2] Training loss: 0.22263, Validation loss: 0.08291\n",
      "[3] Training loss: 0.22261, Validation loss: 0.08297\n",
      "[4] Training loss: 0.22260, Validation loss: 0.08303\n",
      "[5] Training loss: 0.22258, Validation loss: 0.08310\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10188, Validation loss: 0.57228\n",
      "[2] Training loss: 0.10188, Validation loss: 0.57246\n",
      "[3] Training loss: 0.10185, Validation loss: 0.57265\n",
      "[4] Training loss: 0.10180, Validation loss: 0.57285\n",
      "[5] Training loss: 0.10173, Validation loss: 0.57304\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20735, Validation loss: 0.44105\n",
      "[2] Training loss: 0.20734, Validation loss: 0.44077\n",
      "[3] Training loss: 0.20731, Validation loss: 0.44062\n",
      "[4] Training loss: 0.20724, Validation loss: 0.44057\n",
      "[5] Training loss: 0.20716, Validation loss: 0.44055\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21428, Validation loss: 0.14307\n",
      "[2] Training loss: 0.21433, Validation loss: 0.14301\n",
      "[3] Training loss: 0.21435, Validation loss: 0.14301\n",
      "[4] Training loss: 0.21433, Validation loss: 0.14306\n",
      "[5] Training loss: 0.21428, Validation loss: 0.14315\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22666, Validation loss: 0.35327\n",
      "[2] Training loss: 0.22666, Validation loss: 0.35340\n",
      "[3] Training loss: 0.22665, Validation loss: 0.35351\n",
      "[4] Training loss: 0.22664, Validation loss: 0.35359\n",
      "[5] Training loss: 0.22661, Validation loss: 0.35365\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22258, Validation loss: 0.08269\n",
      "[2] Training loss: 0.22257, Validation loss: 0.08272\n",
      "[3] Training loss: 0.22255, Validation loss: 0.08277\n",
      "[4] Training loss: 0.22254, Validation loss: 0.08283\n",
      "[5] Training loss: 0.22252, Validation loss: 0.08289\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10176, Validation loss: 0.57272\n",
      "[2] Training loss: 0.10176, Validation loss: 0.57291\n",
      "[3] Training loss: 0.10173, Validation loss: 0.57310\n",
      "[4] Training loss: 0.10169, Validation loss: 0.57329\n",
      "[5] Training loss: 0.10162, Validation loss: 0.57349\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20753, Validation loss: 0.44016\n",
      "[2] Training loss: 0.20752, Validation loss: 0.43994\n",
      "[3] Training loss: 0.20745, Validation loss: 0.43985\n",
      "[4] Training loss: 0.20735, Validation loss: 0.43988\n",
      "[5] Training loss: 0.20721, Validation loss: 0.44000\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21423, Validation loss: 0.14308\n",
      "[2] Training loss: 0.21429, Validation loss: 0.14301\n",
      "[3] Training loss: 0.21431, Validation loss: 0.14299\n",
      "[4] Training loss: 0.21430, Validation loss: 0.14303\n",
      "[5] Training loss: 0.21426, Validation loss: 0.14311\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22665, Validation loss: 0.35361\n",
      "[2] Training loss: 0.22666, Validation loss: 0.35377\n",
      "[3] Training loss: 0.22666, Validation loss: 0.35390\n",
      "[4] Training loss: 0.22665, Validation loss: 0.35399\n",
      "[5] Training loss: 0.22663, Validation loss: 0.35407\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22258, Validation loss: 0.08295\n",
      "[2] Training loss: 0.22257, Validation loss: 0.08300\n",
      "[3] Training loss: 0.22255, Validation loss: 0.08306\n",
      "[4] Training loss: 0.22253, Validation loss: 0.08313\n",
      "[5] Training loss: 0.22251, Validation loss: 0.08321\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10181, Validation loss: 0.57208\n",
      "[2] Training loss: 0.10181, Validation loss: 0.57226\n",
      "[3] Training loss: 0.10178, Validation loss: 0.57245\n",
      "[4] Training loss: 0.10173, Validation loss: 0.57264\n",
      "[5] Training loss: 0.10166, Validation loss: 0.57284\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20728, Validation loss: 0.44041\n",
      "[2] Training loss: 0.20728, Validation loss: 0.44012\n",
      "[3] Training loss: 0.20724, Validation loss: 0.43996\n",
      "[4] Training loss: 0.20718, Validation loss: 0.43989\n",
      "[5] Training loss: 0.20710, Validation loss: 0.43989\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21422, Validation loss: 0.14301\n",
      "[2] Training loss: 0.21427, Validation loss: 0.14294\n",
      "[3] Training loss: 0.21429, Validation loss: 0.14293\n",
      "[4] Training loss: 0.21427, Validation loss: 0.14298\n",
      "[5] Training loss: 0.21422, Validation loss: 0.14307\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22660, Validation loss: 0.35323\n",
      "[2] Training loss: 0.22660, Validation loss: 0.35335\n",
      "[3] Training loss: 0.22659, Validation loss: 0.35345\n",
      "[4] Training loss: 0.22658, Validation loss: 0.35353\n",
      "[5] Training loss: 0.22655, Validation loss: 0.35358\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22251, Validation loss: 0.08272\n",
      "[2] Training loss: 0.22250, Validation loss: 0.08275\n",
      "[3] Training loss: 0.22249, Validation loss: 0.08280\n",
      "[4] Training loss: 0.22247, Validation loss: 0.08285\n",
      "[5] Training loss: 0.22245, Validation loss: 0.08292\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10170, Validation loss: 0.57276\n",
      "[2] Training loss: 0.10170, Validation loss: 0.57295\n",
      "[3] Training loss: 0.10167, Validation loss: 0.57314\n",
      "[4] Training loss: 0.10163, Validation loss: 0.57333\n",
      "[5] Training loss: 0.10156, Validation loss: 0.57352\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20747, Validation loss: 0.43963\n",
      "[2] Training loss: 0.20747, Validation loss: 0.43943\n",
      "[3] Training loss: 0.20740, Validation loss: 0.43938\n",
      "[4] Training loss: 0.20730, Validation loss: 0.43943\n",
      "[5] Training loss: 0.20717, Validation loss: 0.43956\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21416, Validation loss: 0.14302\n",
      "[2] Training loss: 0.21422, Validation loss: 0.14295\n",
      "[3] Training loss: 0.21424, Validation loss: 0.14294\n",
      "[4] Training loss: 0.21423, Validation loss: 0.14298\n",
      "[5] Training loss: 0.21419, Validation loss: 0.14307\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22658, Validation loss: 0.35375\n",
      "[2] Training loss: 0.22659, Validation loss: 0.35393\n",
      "[3] Training loss: 0.22659, Validation loss: 0.35407\n",
      "[4] Training loss: 0.22658, Validation loss: 0.35418\n",
      "[5] Training loss: 0.22656, Validation loss: 0.35426\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22252, Validation loss: 0.08271\n",
      "[2] Training loss: 0.22250, Validation loss: 0.08275\n",
      "[3] Training loss: 0.22249, Validation loss: 0.08281\n",
      "[4] Training loss: 0.22247, Validation loss: 0.08287\n",
      "[5] Training loss: 0.22245, Validation loss: 0.08295\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10173, Validation loss: 0.57207\n",
      "[2] Training loss: 0.10173, Validation loss: 0.57225\n",
      "[3] Training loss: 0.10170, Validation loss: 0.57244\n",
      "[4] Training loss: 0.10165, Validation loss: 0.57264\n",
      "[5] Training loss: 0.10158, Validation loss: 0.57284\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20724, Validation loss: 0.44010\n",
      "[2] Training loss: 0.20724, Validation loss: 0.43977\n",
      "[3] Training loss: 0.20720, Validation loss: 0.43955\n",
      "[4] Training loss: 0.20715, Validation loss: 0.43944\n",
      "[5] Training loss: 0.20707, Validation loss: 0.43938\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21415, Validation loss: 0.14296\n",
      "[2] Training loss: 0.21420, Validation loss: 0.14290\n",
      "[3] Training loss: 0.21422, Validation loss: 0.14289\n",
      "[4] Training loss: 0.21420, Validation loss: 0.14294\n",
      "[5] Training loss: 0.21415, Validation loss: 0.14303\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22653, Validation loss: 0.35312\n",
      "[2] Training loss: 0.22653, Validation loss: 0.35323\n",
      "[3] Training loss: 0.22652, Validation loss: 0.35331\n",
      "[4] Training loss: 0.22650, Validation loss: 0.35338\n",
      "[5] Training loss: 0.22648, Validation loss: 0.35342\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22244, Validation loss: 0.08260\n",
      "[2] Training loss: 0.22242, Validation loss: 0.08264\n",
      "[3] Training loss: 0.22241, Validation loss: 0.08268\n",
      "[4] Training loss: 0.22239, Validation loss: 0.08274\n",
      "[5] Training loss: 0.22237, Validation loss: 0.08280\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10161, Validation loss: 0.57291\n",
      "[2] Training loss: 0.10161, Validation loss: 0.57309\n",
      "[3] Training loss: 0.10158, Validation loss: 0.57328\n",
      "[4] Training loss: 0.10153, Validation loss: 0.57346\n",
      "[5] Training loss: 0.10147, Validation loss: 0.57365\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20750, Validation loss: 0.43875\n",
      "[2] Training loss: 0.20749, Validation loss: 0.43854\n",
      "[3] Training loss: 0.20741, Validation loss: 0.43849\n",
      "[4] Training loss: 0.20729, Validation loss: 0.43856\n",
      "[5] Training loss: 0.20714, Validation loss: 0.43872\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21410, Validation loss: 0.14295\n",
      "[2] Training loss: 0.21416, Validation loss: 0.14287\n",
      "[3] Training loss: 0.21419, Validation loss: 0.14286\n",
      "[4] Training loss: 0.21417, Validation loss: 0.14289\n",
      "[5] Training loss: 0.21413, Validation loss: 0.14298\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22652, Validation loss: 0.35371\n",
      "[2] Training loss: 0.22654, Validation loss: 0.35389\n",
      "[3] Training loss: 0.22654, Validation loss: 0.35403\n",
      "[4] Training loss: 0.22653, Validation loss: 0.35415\n",
      "[5] Training loss: 0.22651, Validation loss: 0.35423\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22245, Validation loss: 0.08289\n",
      "[2] Training loss: 0.22243, Validation loss: 0.08294\n",
      "[3] Training loss: 0.22242, Validation loss: 0.08300\n",
      "[4] Training loss: 0.22240, Validation loss: 0.08307\n",
      "[5] Training loss: 0.22238, Validation loss: 0.08315\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10168, Validation loss: 0.57187\n",
      "[2] Training loss: 0.10168, Validation loss: 0.57204\n",
      "[3] Training loss: 0.10165, Validation loss: 0.57223\n",
      "[4] Training loss: 0.10160, Validation loss: 0.57241\n",
      "[5] Training loss: 0.10153, Validation loss: 0.57261\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20718, Validation loss: 0.43946\n",
      "[2] Training loss: 0.20717, Validation loss: 0.43914\n",
      "[3] Training loss: 0.20714, Validation loss: 0.43893\n",
      "[4] Training loss: 0.20708, Validation loss: 0.43882\n",
      "[5] Training loss: 0.20700, Validation loss: 0.43879\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21409, Validation loss: 0.14286\n",
      "[2] Training loss: 0.21415, Validation loss: 0.14280\n",
      "[3] Training loss: 0.21416, Validation loss: 0.14279\n",
      "[4] Training loss: 0.21414, Validation loss: 0.14284\n",
      "[5] Training loss: 0.21409, Validation loss: 0.14293\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22647, Validation loss: 0.35329\n",
      "[2] Training loss: 0.22648, Validation loss: 0.35340\n",
      "[3] Training loss: 0.22647, Validation loss: 0.35349\n",
      "[4] Training loss: 0.22645, Validation loss: 0.35356\n",
      "[5] Training loss: 0.22643, Validation loss: 0.35361\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22237, Validation loss: 0.08276\n",
      "[2] Training loss: 0.22236, Validation loss: 0.08280\n",
      "[3] Training loss: 0.22234, Validation loss: 0.08285\n",
      "[4] Training loss: 0.22233, Validation loss: 0.08290\n",
      "[5] Training loss: 0.22231, Validation loss: 0.08297\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10155, Validation loss: 0.57255\n",
      "[2] Training loss: 0.10155, Validation loss: 0.57274\n",
      "[3] Training loss: 0.10152, Validation loss: 0.57292\n",
      "[4] Training loss: 0.10147, Validation loss: 0.57311\n",
      "[5] Training loss: 0.10141, Validation loss: 0.57330\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20741, Validation loss: 0.43826\n",
      "[2] Training loss: 0.20741, Validation loss: 0.43803\n",
      "[3] Training loss: 0.20735, Validation loss: 0.43796\n",
      "[4] Training loss: 0.20725, Validation loss: 0.43800\n",
      "[5] Training loss: 0.20711, Validation loss: 0.43815\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21402, Validation loss: 0.14292\n",
      "[2] Training loss: 0.21408, Validation loss: 0.14285\n",
      "[3] Training loss: 0.21410, Validation loss: 0.14284\n",
      "[4] Training loss: 0.21409, Validation loss: 0.14288\n",
      "[5] Training loss: 0.21405, Validation loss: 0.14297\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22645, Validation loss: 0.35387\n",
      "[2] Training loss: 0.22646, Validation loss: 0.35404\n",
      "[3] Training loss: 0.22646, Validation loss: 0.35419\n",
      "[4] Training loss: 0.22645, Validation loss: 0.35430\n",
      "[5] Training loss: 0.22643, Validation loss: 0.35437\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22238, Validation loss: 0.08262\n",
      "[2] Training loss: 0.22237, Validation loss: 0.08266\n",
      "[3] Training loss: 0.22236, Validation loss: 0.08271\n",
      "[4] Training loss: 0.22234, Validation loss: 0.08278\n",
      "[5] Training loss: 0.22232, Validation loss: 0.08285\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10158, Validation loss: 0.57182\n",
      "[2] Training loss: 0.10158, Validation loss: 0.57200\n",
      "[3] Training loss: 0.10155, Validation loss: 0.57219\n",
      "[4] Training loss: 0.10150, Validation loss: 0.57239\n",
      "[5] Training loss: 0.10143, Validation loss: 0.57259\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20717, Validation loss: 0.43898\n",
      "[2] Training loss: 0.20716, Validation loss: 0.43868\n",
      "[3] Training loss: 0.20713, Validation loss: 0.43852\n",
      "[4] Training loss: 0.20706, Validation loss: 0.43844\n",
      "[5] Training loss: 0.20698, Validation loss: 0.43843\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21401, Validation loss: 0.14284\n",
      "[2] Training loss: 0.21407, Validation loss: 0.14277\n",
      "[3] Training loss: 0.21408, Validation loss: 0.14277\n",
      "[4] Training loss: 0.21407, Validation loss: 0.14281\n",
      "[5] Training loss: 0.21402, Validation loss: 0.14290\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22641, Validation loss: 0.35335\n",
      "[2] Training loss: 0.22641, Validation loss: 0.35347\n",
      "[3] Training loss: 0.22640, Validation loss: 0.35356\n",
      "[4] Training loss: 0.22638, Validation loss: 0.35363\n",
      "[5] Training loss: 0.22636, Validation loss: 0.35368\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22232, Validation loss: 0.08260\n",
      "[2] Training loss: 0.22230, Validation loss: 0.08264\n",
      "[3] Training loss: 0.22229, Validation loss: 0.08269\n",
      "[4] Training loss: 0.22227, Validation loss: 0.08275\n",
      "[5] Training loss: 0.22225, Validation loss: 0.08281\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10148, Validation loss: 0.57262\n",
      "[2] Training loss: 0.10148, Validation loss: 0.57280\n",
      "[3] Training loss: 0.10145, Validation loss: 0.57298\n",
      "[4] Training loss: 0.10140, Validation loss: 0.57317\n",
      "[5] Training loss: 0.10134, Validation loss: 0.57336\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20735, Validation loss: 0.43806\n",
      "[2] Training loss: 0.20734, Validation loss: 0.43780\n",
      "[3] Training loss: 0.20727, Validation loss: 0.43768\n",
      "[4] Training loss: 0.20716, Validation loss: 0.43769\n",
      "[5] Training loss: 0.20702, Validation loss: 0.43779\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21397, Validation loss: 0.14283\n",
      "[2] Training loss: 0.21403, Validation loss: 0.14276\n",
      "[3] Training loss: 0.21405, Validation loss: 0.14275\n",
      "[4] Training loss: 0.21404, Validation loss: 0.14278\n",
      "[5] Training loss: 0.21400, Validation loss: 0.14287\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22640, Validation loss: 0.35392\n",
      "[2] Training loss: 0.22641, Validation loss: 0.35409\n",
      "[3] Training loss: 0.22641, Validation loss: 0.35422\n",
      "[4] Training loss: 0.22640, Validation loss: 0.35432\n",
      "[5] Training loss: 0.22639, Validation loss: 0.35438\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22233, Validation loss: 0.08263\n",
      "[2] Training loss: 0.22232, Validation loss: 0.08268\n",
      "[3] Training loss: 0.22230, Validation loss: 0.08274\n",
      "[4] Training loss: 0.22229, Validation loss: 0.08281\n",
      "[5] Training loss: 0.22227, Validation loss: 0.08289\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10155, Validation loss: 0.57185\n",
      "[2] Training loss: 0.10155, Validation loss: 0.57204\n",
      "[3] Training loss: 0.10152, Validation loss: 0.57224\n",
      "[4] Training loss: 0.10147, Validation loss: 0.57244\n",
      "[5] Training loss: 0.10140, Validation loss: 0.57265\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20705, Validation loss: 0.43826\n",
      "[2] Training loss: 0.20705, Validation loss: 0.43795\n",
      "[3] Training loss: 0.20702, Validation loss: 0.43778\n",
      "[4] Training loss: 0.20696, Validation loss: 0.43770\n",
      "[5] Training loss: 0.20688, Validation loss: 0.43767\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21397, Validation loss: 0.14272\n",
      "[2] Training loss: 0.21402, Validation loss: 0.14266\n",
      "[3] Training loss: 0.21404, Validation loss: 0.14265\n",
      "[4] Training loss: 0.21402, Validation loss: 0.14270\n",
      "[5] Training loss: 0.21397, Validation loss: 0.14280\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22637, Validation loss: 0.35325\n",
      "[2] Training loss: 0.22637, Validation loss: 0.35338\n",
      "[3] Training loss: 0.22636, Validation loss: 0.35348\n",
      "[4] Training loss: 0.22635, Validation loss: 0.35356\n",
      "[5] Training loss: 0.22632, Validation loss: 0.35362\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22225, Validation loss: 0.08252\n",
      "[2] Training loss: 0.22224, Validation loss: 0.08256\n",
      "[3] Training loss: 0.22222, Validation loss: 0.08261\n",
      "[4] Training loss: 0.22220, Validation loss: 0.08266\n",
      "[5] Training loss: 0.22218, Validation loss: 0.08273\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10141, Validation loss: 0.57251\n",
      "[2] Training loss: 0.10141, Validation loss: 0.57269\n",
      "[3] Training loss: 0.10138, Validation loss: 0.57287\n",
      "[4] Training loss: 0.10133, Validation loss: 0.57306\n",
      "[5] Training loss: 0.10127, Validation loss: 0.57325\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20728, Validation loss: 0.43719\n",
      "[2] Training loss: 0.20727, Validation loss: 0.43701\n",
      "[3] Training loss: 0.20722, Validation loss: 0.43697\n",
      "[4] Training loss: 0.20712, Validation loss: 0.43706\n",
      "[5] Training loss: 0.20699, Validation loss: 0.43722\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21390, Validation loss: 0.14278\n",
      "[2] Training loss: 0.21396, Validation loss: 0.14271\n",
      "[3] Training loss: 0.21398, Validation loss: 0.14269\n",
      "[4] Training loss: 0.21396, Validation loss: 0.14273\n",
      "[5] Training loss: 0.21392, Validation loss: 0.14282\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22633, Validation loss: 0.35375\n",
      "[2] Training loss: 0.22634, Validation loss: 0.35393\n",
      "[3] Training loss: 0.22634, Validation loss: 0.35407\n",
      "[4] Training loss: 0.22633, Validation loss: 0.35418\n",
      "[5] Training loss: 0.22631, Validation loss: 0.35427\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22225, Validation loss: 0.08249\n",
      "[2] Training loss: 0.22223, Validation loss: 0.08253\n",
      "[3] Training loss: 0.22222, Validation loss: 0.08258\n",
      "[4] Training loss: 0.22220, Validation loss: 0.08264\n",
      "[5] Training loss: 0.22219, Validation loss: 0.08271\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10143, Validation loss: 0.57191\n",
      "[2] Training loss: 0.10143, Validation loss: 0.57208\n",
      "[3] Training loss: 0.10140, Validation loss: 0.57226\n",
      "[4] Training loss: 0.10135, Validation loss: 0.57244\n",
      "[5] Training loss: 0.10128, Validation loss: 0.57263\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20708, Validation loss: 0.43809\n",
      "[2] Training loss: 0.20707, Validation loss: 0.43778\n",
      "[3] Training loss: 0.20704, Validation loss: 0.43759\n",
      "[4] Training loss: 0.20698, Validation loss: 0.43749\n",
      "[5] Training loss: 0.20690, Validation loss: 0.43745\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21387, Validation loss: 0.14273\n",
      "[2] Training loss: 0.21393, Validation loss: 0.14267\n",
      "[3] Training loss: 0.21394, Validation loss: 0.14266\n",
      "[4] Training loss: 0.21392, Validation loss: 0.14271\n",
      "[5] Training loss: 0.21388, Validation loss: 0.14280\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22627, Validation loss: 0.35333\n",
      "[2] Training loss: 0.22627, Validation loss: 0.35344\n",
      "[3] Training loss: 0.22627, Validation loss: 0.35353\n",
      "[4] Training loss: 0.22625, Validation loss: 0.35359\n",
      "[5] Training loss: 0.22622, Validation loss: 0.35364\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22218, Validation loss: 0.08252\n",
      "[2] Training loss: 0.22216, Validation loss: 0.08256\n",
      "[3] Training loss: 0.22215, Validation loss: 0.08261\n",
      "[4] Training loss: 0.22213, Validation loss: 0.08267\n",
      "[5] Training loss: 0.22211, Validation loss: 0.08273\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10132, Validation loss: 0.57244\n",
      "[2] Training loss: 0.10132, Validation loss: 0.57262\n",
      "[3] Training loss: 0.10130, Validation loss: 0.57280\n",
      "[4] Training loss: 0.10125, Validation loss: 0.57299\n",
      "[5] Training loss: 0.10118, Validation loss: 0.57317\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20734, Validation loss: 0.43685\n",
      "[2] Training loss: 0.20731, Validation loss: 0.43663\n",
      "[3] Training loss: 0.20722, Validation loss: 0.43657\n",
      "[4] Training loss: 0.20709, Validation loss: 0.43661\n",
      "[5] Training loss: 0.20694, Validation loss: 0.43672\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21384, Validation loss: 0.14271\n",
      "[2] Training loss: 0.21390, Validation loss: 0.14263\n",
      "[3] Training loss: 0.21393, Validation loss: 0.14261\n",
      "[4] Training loss: 0.21392, Validation loss: 0.14265\n",
      "[5] Training loss: 0.21387, Validation loss: 0.14273\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22629, Validation loss: 0.35412\n",
      "[2] Training loss: 0.22630, Validation loss: 0.35430\n",
      "[3] Training loss: 0.22630, Validation loss: 0.35445\n",
      "[4] Training loss: 0.22629, Validation loss: 0.35456\n",
      "[5] Training loss: 0.22627, Validation loss: 0.35464\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22219, Validation loss: 0.08278\n",
      "[2] Training loss: 0.22218, Validation loss: 0.08283\n",
      "[3] Training loss: 0.22216, Validation loss: 0.08289\n",
      "[4] Training loss: 0.22215, Validation loss: 0.08296\n",
      "[5] Training loss: 0.22213, Validation loss: 0.08304\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10142, Validation loss: 0.57139\n",
      "[2] Training loss: 0.10142, Validation loss: 0.57157\n",
      "[3] Training loss: 0.10139, Validation loss: 0.57176\n",
      "[4] Training loss: 0.10134, Validation loss: 0.57196\n",
      "[5] Training loss: 0.10126, Validation loss: 0.57216\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20694, Validation loss: 0.43714\n",
      "[2] Training loss: 0.20694, Validation loss: 0.43680\n",
      "[3] Training loss: 0.20690, Validation loss: 0.43656\n",
      "[4] Training loss: 0.20685, Validation loss: 0.43642\n",
      "[5] Training loss: 0.20678, Validation loss: 0.43634\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21384, Validation loss: 0.14261\n",
      "[2] Training loss: 0.21389, Validation loss: 0.14254\n",
      "[3] Training loss: 0.21391, Validation loss: 0.14254\n",
      "[4] Training loss: 0.21389, Validation loss: 0.14258\n",
      "[5] Training loss: 0.21384, Validation loss: 0.14268\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22624, Validation loss: 0.35344\n",
      "[2] Training loss: 0.22624, Validation loss: 0.35355\n",
      "[3] Training loss: 0.22623, Validation loss: 0.35363\n",
      "[4] Training loss: 0.22621, Validation loss: 0.35369\n",
      "[5] Training loss: 0.22619, Validation loss: 0.35373\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22211, Validation loss: 0.08241\n",
      "[2] Training loss: 0.22209, Validation loss: 0.08244\n",
      "[3] Training loss: 0.22208, Validation loss: 0.08248\n",
      "[4] Training loss: 0.22206, Validation loss: 0.08253\n",
      "[5] Training loss: 0.22204, Validation loss: 0.08259\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10127, Validation loss: 0.57260\n",
      "[2] Training loss: 0.10127, Validation loss: 0.57279\n",
      "[3] Training loss: 0.10124, Validation loss: 0.57298\n",
      "[4] Training loss: 0.10120, Validation loss: 0.57317\n",
      "[5] Training loss: 0.10113, Validation loss: 0.57336\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20722, Validation loss: 0.43551\n",
      "[2] Training loss: 0.20721, Validation loss: 0.43528\n",
      "[3] Training loss: 0.20715, Validation loss: 0.43522\n",
      "[4] Training loss: 0.20705, Validation loss: 0.43527\n",
      "[5] Training loss: 0.20692, Validation loss: 0.43544\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21375, Validation loss: 0.14270\n",
      "[2] Training loss: 0.21381, Validation loss: 0.14264\n",
      "[3] Training loss: 0.21383, Validation loss: 0.14263\n",
      "[4] Training loss: 0.21382, Validation loss: 0.14267\n",
      "[5] Training loss: 0.21378, Validation loss: 0.14276\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22620, Validation loss: 0.35420\n",
      "[2] Training loss: 0.22621, Validation loss: 0.35438\n",
      "[3] Training loss: 0.22621, Validation loss: 0.35452\n",
      "[4] Training loss: 0.22620, Validation loss: 0.35463\n",
      "[5] Training loss: 0.22618, Validation loss: 0.35471\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22211, Validation loss: 0.08220\n",
      "[2] Training loss: 0.22210, Validation loss: 0.08225\n",
      "[3] Training loss: 0.22209, Validation loss: 0.08231\n",
      "[4] Training loss: 0.22207, Validation loss: 0.08237\n",
      "[5] Training loss: 0.22205, Validation loss: 0.08245\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10127, Validation loss: 0.57177\n",
      "[2] Training loss: 0.10127, Validation loss: 0.57195\n",
      "[3] Training loss: 0.10125, Validation loss: 0.57214\n",
      "[4] Training loss: 0.10120, Validation loss: 0.57233\n",
      "[5] Training loss: 0.10113, Validation loss: 0.57253\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20700, Validation loss: 0.43641\n",
      "[2] Training loss: 0.20700, Validation loss: 0.43612\n",
      "[3] Training loss: 0.20696, Validation loss: 0.43592\n",
      "[4] Training loss: 0.20690, Validation loss: 0.43581\n",
      "[5] Training loss: 0.20682, Validation loss: 0.43578\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21374, Validation loss: 0.14264\n",
      "[2] Training loss: 0.21380, Validation loss: 0.14257\n",
      "[3] Training loss: 0.21381, Validation loss: 0.14256\n",
      "[4] Training loss: 0.21380, Validation loss: 0.14260\n",
      "[5] Training loss: 0.21375, Validation loss: 0.14269\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22616, Validation loss: 0.35347\n",
      "[2] Training loss: 0.22616, Validation loss: 0.35358\n",
      "[3] Training loss: 0.22615, Validation loss: 0.35366\n",
      "[4] Training loss: 0.22614, Validation loss: 0.35373\n",
      "[5] Training loss: 0.22611, Validation loss: 0.35377\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22204, Validation loss: 0.08251\n",
      "[2] Training loss: 0.22202, Validation loss: 0.08255\n",
      "[3] Training loss: 0.22201, Validation loss: 0.08261\n",
      "[4] Training loss: 0.22199, Validation loss: 0.08267\n",
      "[5] Training loss: 0.22197, Validation loss: 0.08274\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10116, Validation loss: 0.57229\n",
      "[2] Training loss: 0.10116, Validation loss: 0.57246\n",
      "[3] Training loss: 0.10114, Validation loss: 0.57264\n",
      "[4] Training loss: 0.10109, Validation loss: 0.57282\n",
      "[5] Training loss: 0.10102, Validation loss: 0.57299\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20724, Validation loss: 0.43522\n",
      "[2] Training loss: 0.20722, Validation loss: 0.43500\n",
      "[3] Training loss: 0.20714, Validation loss: 0.43491\n",
      "[4] Training loss: 0.20702, Validation loss: 0.43493\n",
      "[5] Training loss: 0.20687, Validation loss: 0.43506\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21370, Validation loss: 0.14262\n",
      "[2] Training loss: 0.21376, Validation loss: 0.14255\n",
      "[3] Training loss: 0.21379, Validation loss: 0.14254\n",
      "[4] Training loss: 0.21378, Validation loss: 0.14257\n",
      "[5] Training loss: 0.21373, Validation loss: 0.14266\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22616, Validation loss: 0.35446\n",
      "[2] Training loss: 0.22617, Validation loss: 0.35465\n",
      "[3] Training loss: 0.22618, Validation loss: 0.35479\n",
      "[4] Training loss: 0.22617, Validation loss: 0.35490\n",
      "[5] Training loss: 0.22615, Validation loss: 0.35498\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22206, Validation loss: 0.08248\n",
      "[2] Training loss: 0.22205, Validation loss: 0.08253\n",
      "[3] Training loss: 0.22204, Validation loss: 0.08258\n",
      "[4] Training loss: 0.22202, Validation loss: 0.08265\n",
      "[5] Training loss: 0.22200, Validation loss: 0.08272\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10126, Validation loss: 0.57133\n",
      "[2] Training loss: 0.10126, Validation loss: 0.57152\n",
      "[3] Training loss: 0.10123, Validation loss: 0.57172\n",
      "[4] Training loss: 0.10118, Validation loss: 0.57193\n",
      "[5] Training loss: 0.10111, Validation loss: 0.57214\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20687, Validation loss: 0.43570\n",
      "[2] Training loss: 0.20686, Validation loss: 0.43536\n",
      "[3] Training loss: 0.20683, Validation loss: 0.43511\n",
      "[4] Training loss: 0.20677, Validation loss: 0.43495\n",
      "[5] Training loss: 0.20670, Validation loss: 0.43486\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21370, Validation loss: 0.14250\n",
      "[2] Training loss: 0.21376, Validation loss: 0.14243\n",
      "[3] Training loss: 0.21377, Validation loss: 0.14242\n",
      "[4] Training loss: 0.21375, Validation loss: 0.14247\n",
      "[5] Training loss: 0.21370, Validation loss: 0.14256\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22612, Validation loss: 0.35334\n",
      "[2] Training loss: 0.22612, Validation loss: 0.35343\n",
      "[3] Training loss: 0.22611, Validation loss: 0.35350\n",
      "[4] Training loss: 0.22609, Validation loss: 0.35355\n",
      "[5] Training loss: 0.22607, Validation loss: 0.35359\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22197, Validation loss: 0.08246\n",
      "[2] Training loss: 0.22196, Validation loss: 0.08249\n",
      "[3] Training loss: 0.22194, Validation loss: 0.08253\n",
      "[4] Training loss: 0.22192, Validation loss: 0.08259\n",
      "[5] Training loss: 0.22190, Validation loss: 0.08265\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10112, Validation loss: 0.57269\n",
      "[2] Training loss: 0.10112, Validation loss: 0.57286\n",
      "[3] Training loss: 0.10110, Validation loss: 0.57304\n",
      "[4] Training loss: 0.10105, Validation loss: 0.57322\n",
      "[5] Training loss: 0.10098, Validation loss: 0.57340\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20714, Validation loss: 0.43390\n",
      "[2] Training loss: 0.20714, Validation loss: 0.43368\n",
      "[3] Training loss: 0.20707, Validation loss: 0.43362\n",
      "[4] Training loss: 0.20697, Validation loss: 0.43369\n",
      "[5] Training loss: 0.20683, Validation loss: 0.43391\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21362, Validation loss: 0.14259\n",
      "[2] Training loss: 0.21368, Validation loss: 0.14252\n",
      "[3] Training loss: 0.21370, Validation loss: 0.14251\n",
      "[4] Training loss: 0.21369, Validation loss: 0.14256\n",
      "[5] Training loss: 0.21365, Validation loss: 0.14265\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22609, Validation loss: 0.35460\n",
      "[2] Training loss: 0.22610, Validation loss: 0.35481\n",
      "[3] Training loss: 0.22610, Validation loss: 0.35497\n",
      "[4] Training loss: 0.22609, Validation loss: 0.35510\n",
      "[5] Training loss: 0.22607, Validation loss: 0.35519\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22199, Validation loss: 0.08220\n",
      "[2] Training loss: 0.22198, Validation loss: 0.08224\n",
      "[3] Training loss: 0.22197, Validation loss: 0.08230\n",
      "[4] Training loss: 0.22195, Validation loss: 0.08237\n",
      "[5] Training loss: 0.22193, Validation loss: 0.08244\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10114, Validation loss: 0.57137\n",
      "[2] Training loss: 0.10114, Validation loss: 0.57156\n",
      "[3] Training loss: 0.10111, Validation loss: 0.57175\n",
      "[4] Training loss: 0.10106, Validation loss: 0.57194\n",
      "[5] Training loss: 0.10099, Validation loss: 0.57214\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20689, Validation loss: 0.43525\n",
      "[2] Training loss: 0.20688, Validation loss: 0.43494\n",
      "[3] Training loss: 0.20684, Validation loss: 0.43472\n",
      "[4] Training loss: 0.20679, Validation loss: 0.43458\n",
      "[5] Training loss: 0.20671, Validation loss: 0.43450\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21361, Validation loss: 0.14252\n",
      "[2] Training loss: 0.21367, Validation loss: 0.14245\n",
      "[3] Training loss: 0.21368, Validation loss: 0.14244\n",
      "[4] Training loss: 0.21367, Validation loss: 0.14249\n",
      "[5] Training loss: 0.21362, Validation loss: 0.14258\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22605, Validation loss: 0.35362\n",
      "[2] Training loss: 0.22605, Validation loss: 0.35370\n",
      "[3] Training loss: 0.22604, Validation loss: 0.35377\n",
      "[4] Training loss: 0.22602, Validation loss: 0.35382\n",
      "[5] Training loss: 0.22600, Validation loss: 0.35385\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22191, Validation loss: 0.08216\n",
      "[2] Training loss: 0.22189, Validation loss: 0.08219\n",
      "[3] Training loss: 0.22188, Validation loss: 0.08224\n",
      "[4] Training loss: 0.22186, Validation loss: 0.08229\n",
      "[5] Training loss: 0.22184, Validation loss: 0.08236\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10103, Validation loss: 0.57261\n",
      "[2] Training loss: 0.10103, Validation loss: 0.57279\n",
      "[3] Training loss: 0.10100, Validation loss: 0.57297\n",
      "[4] Training loss: 0.10095, Validation loss: 0.57315\n",
      "[5] Training loss: 0.10089, Validation loss: 0.57333\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20715, Validation loss: 0.43361\n",
      "[2] Training loss: 0.20713, Validation loss: 0.43338\n",
      "[3] Training loss: 0.20703, Validation loss: 0.43330\n",
      "[4] Training loss: 0.20690, Validation loss: 0.43336\n",
      "[5] Training loss: 0.20675, Validation loss: 0.43350\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21357, Validation loss: 0.14252\n",
      "[2] Training loss: 0.21363, Validation loss: 0.14244\n",
      "[3] Training loss: 0.21365, Validation loss: 0.14243\n",
      "[4] Training loss: 0.21364, Validation loss: 0.14247\n",
      "[5] Training loss: 0.21360, Validation loss: 0.14255\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22605, Validation loss: 0.35464\n",
      "[2] Training loss: 0.22606, Validation loss: 0.35482\n",
      "[3] Training loss: 0.22606, Validation loss: 0.35496\n",
      "[4] Training loss: 0.22605, Validation loss: 0.35507\n",
      "[5] Training loss: 0.22603, Validation loss: 0.35514\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22193, Validation loss: 0.08245\n",
      "[2] Training loss: 0.22192, Validation loss: 0.08251\n",
      "[3] Training loss: 0.22191, Validation loss: 0.08258\n",
      "[4] Training loss: 0.22189, Validation loss: 0.08266\n",
      "[5] Training loss: 0.22187, Validation loss: 0.08274\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10112, Validation loss: 0.57117\n",
      "[2] Training loss: 0.10112, Validation loss: 0.57136\n",
      "[3] Training loss: 0.10109, Validation loss: 0.57156\n",
      "[4] Training loss: 0.10104, Validation loss: 0.57176\n",
      "[5] Training loss: 0.10096, Validation loss: 0.57197\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20677, Validation loss: 0.43413\n",
      "[2] Training loss: 0.20676, Validation loss: 0.43387\n",
      "[3] Training loss: 0.20673, Validation loss: 0.43372\n",
      "[4] Training loss: 0.20666, Validation loss: 0.43365\n",
      "[5] Training loss: 0.20659, Validation loss: 0.43365\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21358, Validation loss: 0.14237\n",
      "[2] Training loss: 0.21363, Validation loss: 0.14230\n",
      "[3] Training loss: 0.21365, Validation loss: 0.14230\n",
      "[4] Training loss: 0.21363, Validation loss: 0.14234\n",
      "[5] Training loss: 0.21358, Validation loss: 0.14243\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22602, Validation loss: 0.35371\n",
      "[2] Training loss: 0.22602, Validation loss: 0.35381\n",
      "[3] Training loss: 0.22602, Validation loss: 0.35389\n",
      "[4] Training loss: 0.22600, Validation loss: 0.35396\n",
      "[5] Training loss: 0.22597, Validation loss: 0.35400\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22186, Validation loss: 0.08232\n",
      "[2] Training loss: 0.22184, Validation loss: 0.08235\n",
      "[3] Training loss: 0.22183, Validation loss: 0.08239\n",
      "[4] Training loss: 0.22181, Validation loss: 0.08245\n",
      "[5] Training loss: 0.22179, Validation loss: 0.08251\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10099, Validation loss: 0.57225\n",
      "[2] Training loss: 0.10099, Validation loss: 0.57243\n",
      "[3] Training loss: 0.10097, Validation loss: 0.57261\n",
      "[4] Training loss: 0.10092, Validation loss: 0.57279\n",
      "[5] Training loss: 0.10085, Validation loss: 0.57297\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20696, Validation loss: 0.43333\n",
      "[2] Training loss: 0.20695, Validation loss: 0.43313\n",
      "[3] Training loss: 0.20689, Validation loss: 0.43306\n",
      "[4] Training loss: 0.20680, Validation loss: 0.43311\n",
      "[5] Training loss: 0.20667, Validation loss: 0.43328\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21350, Validation loss: 0.14245\n",
      "[2] Training loss: 0.21355, Validation loss: 0.14238\n",
      "[3] Training loss: 0.21357, Validation loss: 0.14237\n",
      "[4] Training loss: 0.21356, Validation loss: 0.14242\n",
      "[5] Training loss: 0.21351, Validation loss: 0.14251\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22598, Validation loss: 0.35456\n",
      "[2] Training loss: 0.22598, Validation loss: 0.35474\n",
      "[3] Training loss: 0.22598, Validation loss: 0.35489\n",
      "[4] Training loss: 0.22597, Validation loss: 0.35500\n",
      "[5] Training loss: 0.22595, Validation loss: 0.35508\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22186, Validation loss: 0.08214\n",
      "[2] Training loss: 0.22185, Validation loss: 0.08219\n",
      "[3] Training loss: 0.22184, Validation loss: 0.08224\n",
      "[4] Training loss: 0.22182, Validation loss: 0.08231\n",
      "[5] Training loss: 0.22180, Validation loss: 0.08239\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10100, Validation loss: 0.57144\n",
      "[2] Training loss: 0.10099, Validation loss: 0.57163\n",
      "[3] Training loss: 0.10097, Validation loss: 0.57182\n",
      "[4] Training loss: 0.10092, Validation loss: 0.57201\n",
      "[5] Training loss: 0.10085, Validation loss: 0.57221\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20680, Validation loss: 0.43405\n",
      "[2] Training loss: 0.20679, Validation loss: 0.43374\n",
      "[3] Training loss: 0.20676, Validation loss: 0.43357\n",
      "[4] Training loss: 0.20670, Validation loss: 0.43351\n",
      "[5] Training loss: 0.20662, Validation loss: 0.43350\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21348, Validation loss: 0.14239\n",
      "[2] Training loss: 0.21353, Validation loss: 0.14233\n",
      "[3] Training loss: 0.21355, Validation loss: 0.14232\n",
      "[4] Training loss: 0.21353, Validation loss: 0.14236\n",
      "[5] Training loss: 0.21348, Validation loss: 0.14245\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22592, Validation loss: 0.35387\n",
      "[2] Training loss: 0.22593, Validation loss: 0.35397\n",
      "[3] Training loss: 0.22592, Validation loss: 0.35406\n",
      "[4] Training loss: 0.22590, Validation loss: 0.35413\n",
      "[5] Training loss: 0.22588, Validation loss: 0.35418\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22179, Validation loss: 0.08221\n",
      "[2] Training loss: 0.22178, Validation loss: 0.08225\n",
      "[3] Training loss: 0.22176, Validation loss: 0.08229\n",
      "[4] Training loss: 0.22175, Validation loss: 0.08235\n",
      "[5] Training loss: 0.22173, Validation loss: 0.08242\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10090, Validation loss: 0.57213\n",
      "[2] Training loss: 0.10090, Validation loss: 0.57230\n",
      "[3] Training loss: 0.10087, Validation loss: 0.57247\n",
      "[4] Training loss: 0.10082, Validation loss: 0.57265\n",
      "[5] Training loss: 0.10076, Validation loss: 0.57283\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20698, Validation loss: 0.43298\n",
      "[2] Training loss: 0.20696, Validation loss: 0.43278\n",
      "[3] Training loss: 0.20687, Validation loss: 0.43273\n",
      "[4] Training loss: 0.20676, Validation loss: 0.43279\n",
      "[5] Training loss: 0.20663, Validation loss: 0.43293\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21344, Validation loss: 0.14239\n",
      "[2] Training loss: 0.21350, Validation loss: 0.14232\n",
      "[3] Training loss: 0.21352, Validation loss: 0.14230\n",
      "[4] Training loss: 0.21351, Validation loss: 0.14234\n",
      "[5] Training loss: 0.21347, Validation loss: 0.14243\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22592, Validation loss: 0.35469\n",
      "[2] Training loss: 0.22593, Validation loss: 0.35485\n",
      "[3] Training loss: 0.22593, Validation loss: 0.35499\n",
      "[4] Training loss: 0.22592, Validation loss: 0.35508\n",
      "[5] Training loss: 0.22590, Validation loss: 0.35515\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22180, Validation loss: 0.08222\n",
      "[2] Training loss: 0.22179, Validation loss: 0.08227\n",
      "[3] Training loss: 0.22178, Validation loss: 0.08233\n",
      "[4] Training loss: 0.22176, Validation loss: 0.08239\n",
      "[5] Training loss: 0.22174, Validation loss: 0.08247\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10096, Validation loss: 0.57127\n",
      "[2] Training loss: 0.10096, Validation loss: 0.57146\n",
      "[3] Training loss: 0.10093, Validation loss: 0.57166\n",
      "[4] Training loss: 0.10088, Validation loss: 0.57187\n",
      "[5] Training loss: 0.10081, Validation loss: 0.57207\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20668, Validation loss: 0.43329\n",
      "[2] Training loss: 0.20668, Validation loss: 0.43300\n",
      "[3] Training loss: 0.20665, Validation loss: 0.43282\n",
      "[4] Training loss: 0.20659, Validation loss: 0.43275\n",
      "[5] Training loss: 0.20651, Validation loss: 0.43274\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21344, Validation loss: 0.14227\n",
      "[2] Training loss: 0.21349, Validation loss: 0.14221\n",
      "[3] Training loss: 0.21351, Validation loss: 0.14220\n",
      "[4] Training loss: 0.21349, Validation loss: 0.14224\n",
      "[5] Training loss: 0.21344, Validation loss: 0.14234\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22588, Validation loss: 0.35381\n",
      "[2] Training loss: 0.22588, Validation loss: 0.35392\n",
      "[3] Training loss: 0.22588, Validation loss: 0.35401\n",
      "[4] Training loss: 0.22586, Validation loss: 0.35408\n",
      "[5] Training loss: 0.22583, Validation loss: 0.35413\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22173, Validation loss: 0.08204\n",
      "[2] Training loss: 0.22171, Validation loss: 0.08207\n",
      "[3] Training loss: 0.22170, Validation loss: 0.08211\n",
      "[4] Training loss: 0.22168, Validation loss: 0.08217\n",
      "[5] Training loss: 0.22166, Validation loss: 0.08223\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10084, Validation loss: 0.57226\n",
      "[2] Training loss: 0.10084, Validation loss: 0.57244\n",
      "[3] Training loss: 0.10082, Validation loss: 0.57262\n",
      "[4] Training loss: 0.10077, Validation loss: 0.57280\n",
      "[5] Training loss: 0.10070, Validation loss: 0.57298\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20690, Validation loss: 0.43217\n",
      "[2] Training loss: 0.20689, Validation loss: 0.43205\n",
      "[3] Training loss: 0.20683, Validation loss: 0.43208\n",
      "[4] Training loss: 0.20673, Validation loss: 0.43221\n",
      "[5] Training loss: 0.20660, Validation loss: 0.43240\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21337, Validation loss: 0.14233\n",
      "[2] Training loss: 0.21343, Validation loss: 0.14226\n",
      "[3] Training loss: 0.21345, Validation loss: 0.14225\n",
      "[4] Training loss: 0.21343, Validation loss: 0.14229\n",
      "[5] Training loss: 0.21339, Validation loss: 0.14238\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22585, Validation loss: 0.35458\n",
      "[2] Training loss: 0.22586, Validation loss: 0.35476\n",
      "[3] Training loss: 0.22586, Validation loss: 0.35491\n",
      "[4] Training loss: 0.22585, Validation loss: 0.35502\n",
      "[5] Training loss: 0.22582, Validation loss: 0.35511\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22173, Validation loss: 0.08224\n",
      "[2] Training loss: 0.22172, Validation loss: 0.08229\n",
      "[3] Training loss: 0.22171, Validation loss: 0.08235\n",
      "[4] Training loss: 0.22169, Validation loss: 0.08242\n",
      "[5] Training loss: 0.22167, Validation loss: 0.08250\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10085, Validation loss: 0.57110\n",
      "[2] Training loss: 0.10085, Validation loss: 0.57127\n",
      "[3] Training loss: 0.10082, Validation loss: 0.57145\n",
      "[4] Training loss: 0.10077, Validation loss: 0.57164\n",
      "[5] Training loss: 0.10070, Validation loss: 0.57183\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20670, Validation loss: 0.43324\n",
      "[2] Training loss: 0.20669, Validation loss: 0.43295\n",
      "[3] Training loss: 0.20666, Validation loss: 0.43276\n",
      "[4] Training loss: 0.20661, Validation loss: 0.43265\n",
      "[5] Training loss: 0.20653, Validation loss: 0.43259\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21334, Validation loss: 0.14230\n",
      "[2] Training loss: 0.21339, Validation loss: 0.14223\n",
      "[3] Training loss: 0.21341, Validation loss: 0.14223\n",
      "[4] Training loss: 0.21339, Validation loss: 0.14227\n",
      "[5] Training loss: 0.21334, Validation loss: 0.14237\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22579, Validation loss: 0.35390\n",
      "[2] Training loss: 0.22579, Validation loss: 0.35400\n",
      "[3] Training loss: 0.22578, Validation loss: 0.35407\n",
      "[4] Training loss: 0.22577, Validation loss: 0.35413\n",
      "[5] Training loss: 0.22574, Validation loss: 0.35416\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22166, Validation loss: 0.08180\n",
      "[2] Training loss: 0.22165, Validation loss: 0.08183\n",
      "[3] Training loss: 0.22163, Validation loss: 0.08186\n",
      "[4] Training loss: 0.22161, Validation loss: 0.08191\n",
      "[5] Training loss: 0.22160, Validation loss: 0.08197\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10075, Validation loss: 0.57238\n",
      "[2] Training loss: 0.10075, Validation loss: 0.57256\n",
      "[3] Training loss: 0.10072, Validation loss: 0.57275\n",
      "[4] Training loss: 0.10068, Validation loss: 0.57294\n",
      "[5] Training loss: 0.10061, Validation loss: 0.57313\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20698, Validation loss: 0.43169\n",
      "[2] Training loss: 0.20695, Validation loss: 0.43157\n",
      "[3] Training loss: 0.20684, Validation loss: 0.43162\n",
      "[4] Training loss: 0.20669, Validation loss: 0.43177\n",
      "[5] Training loss: 0.20654, Validation loss: 0.43199\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21332, Validation loss: 0.14223\n",
      "[2] Training loss: 0.21338, Validation loss: 0.14215\n",
      "[3] Training loss: 0.21341, Validation loss: 0.14213\n",
      "[4] Training loss: 0.21340, Validation loss: 0.14217\n",
      "[5] Training loss: 0.21336, Validation loss: 0.14225\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22582, Validation loss: 0.35476\n",
      "[2] Training loss: 0.22584, Validation loss: 0.35496\n",
      "[3] Training loss: 0.22584, Validation loss: 0.35513\n",
      "[4] Training loss: 0.22583, Validation loss: 0.35525\n",
      "[5] Training loss: 0.22581, Validation loss: 0.35535\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22167, Validation loss: 0.08284\n",
      "[2] Training loss: 0.22166, Validation loss: 0.08292\n",
      "[3] Training loss: 0.22165, Validation loss: 0.08300\n",
      "[4] Training loss: 0.22163, Validation loss: 0.08309\n",
      "[5] Training loss: 0.22161, Validation loss: 0.08318\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10084, Validation loss: 0.57028\n",
      "[2] Training loss: 0.10084, Validation loss: 0.57044\n",
      "[3] Training loss: 0.10082, Validation loss: 0.57062\n",
      "[4] Training loss: 0.10076, Validation loss: 0.57080\n",
      "[5] Training loss: 0.10069, Validation loss: 0.57099\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20655, Validation loss: 0.43291\n",
      "[2] Training loss: 0.20655, Validation loss: 0.43260\n",
      "[3] Training loss: 0.20652, Validation loss: 0.43237\n",
      "[4] Training loss: 0.20647, Validation loss: 0.43222\n",
      "[5] Training loss: 0.20640, Validation loss: 0.43213\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21331, Validation loss: 0.14215\n",
      "[2] Training loss: 0.21337, Validation loss: 0.14209\n",
      "[3] Training loss: 0.21338, Validation loss: 0.14209\n",
      "[4] Training loss: 0.21336, Validation loss: 0.14214\n",
      "[5] Training loss: 0.21331, Validation loss: 0.14224\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22577, Validation loss: 0.35426\n",
      "[2] Training loss: 0.22577, Validation loss: 0.35435\n",
      "[3] Training loss: 0.22576, Validation loss: 0.35442\n",
      "[4] Training loss: 0.22574, Validation loss: 0.35447\n",
      "[5] Training loss: 0.22571, Validation loss: 0.35450\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22159, Validation loss: 0.08222\n",
      "[2] Training loss: 0.22158, Validation loss: 0.08224\n",
      "[3] Training loss: 0.22156, Validation loss: 0.08227\n",
      "[4] Training loss: 0.22154, Validation loss: 0.08232\n",
      "[5] Training loss: 0.22152, Validation loss: 0.08237\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10071, Validation loss: 0.57181\n",
      "[2] Training loss: 0.10071, Validation loss: 0.57201\n",
      "[3] Training loss: 0.10068, Validation loss: 0.57221\n",
      "[4] Training loss: 0.10063, Validation loss: 0.57241\n",
      "[5] Training loss: 0.10056, Validation loss: 0.57261\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20683, Validation loss: 0.43091\n",
      "[2] Training loss: 0.20682, Validation loss: 0.43075\n",
      "[3] Training loss: 0.20676, Validation loss: 0.43073\n",
      "[4] Training loss: 0.20666, Validation loss: 0.43084\n",
      "[5] Training loss: 0.20654, Validation loss: 0.43103\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21323, Validation loss: 0.14225\n",
      "[2] Training loss: 0.21328, Validation loss: 0.14218\n",
      "[3] Training loss: 0.21330, Validation loss: 0.14217\n",
      "[4] Training loss: 0.21329, Validation loss: 0.14222\n",
      "[5] Training loss: 0.21325, Validation loss: 0.14231\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22573, Validation loss: 0.35460\n",
      "[2] Training loss: 0.22574, Validation loss: 0.35477\n",
      "[3] Training loss: 0.22574, Validation loss: 0.35491\n",
      "[4] Training loss: 0.22572, Validation loss: 0.35501\n",
      "[5] Training loss: 0.22570, Validation loss: 0.35509\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22159, Validation loss: 0.08185\n",
      "[2] Training loss: 0.22158, Validation loss: 0.08189\n",
      "[3] Training loss: 0.22156, Validation loss: 0.08194\n",
      "[4] Training loss: 0.22155, Validation loss: 0.08200\n",
      "[5] Training loss: 0.22153, Validation loss: 0.08207\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10070, Validation loss: 0.57129\n",
      "[2] Training loss: 0.10070, Validation loss: 0.57146\n",
      "[3] Training loss: 0.10067, Validation loss: 0.57165\n",
      "[4] Training loss: 0.10062, Validation loss: 0.57184\n",
      "[5] Training loss: 0.10055, Validation loss: 0.57202\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20665, Validation loss: 0.43204\n",
      "[2] Training loss: 0.20664, Validation loss: 0.43179\n",
      "[3] Training loss: 0.20661, Validation loss: 0.43164\n",
      "[4] Training loss: 0.20654, Validation loss: 0.43156\n",
      "[5] Training loss: 0.20646, Validation loss: 0.43154\n",
      "Finished Training for Fold 5\n",
      "Fold 1\n",
      "[1] Training loss: 0.21321, Validation loss: 0.14220\n",
      "[2] Training loss: 0.21326, Validation loss: 0.14213\n",
      "[3] Training loss: 0.21328, Validation loss: 0.14212\n",
      "[4] Training loss: 0.21326, Validation loss: 0.14216\n",
      "[5] Training loss: 0.21321, Validation loss: 0.14226\n",
      "Finished Training for Fold 1\n",
      "Fold 2\n",
      "[1] Training loss: 0.22567, Validation loss: 0.35411\n",
      "[2] Training loss: 0.22568, Validation loss: 0.35423\n",
      "[3] Training loss: 0.22567, Validation loss: 0.35433\n",
      "[4] Training loss: 0.22565, Validation loss: 0.35440\n",
      "[5] Training loss: 0.22563, Validation loss: 0.35445\n",
      "Finished Training for Fold 2\n",
      "Fold 3\n",
      "[1] Training loss: 0.22152, Validation loss: 0.08213\n",
      "[2] Training loss: 0.22151, Validation loss: 0.08218\n",
      "[3] Training loss: 0.22149, Validation loss: 0.08224\n",
      "[4] Training loss: 0.22148, Validation loss: 0.08230\n",
      "[5] Training loss: 0.22146, Validation loss: 0.08238\n",
      "Finished Training for Fold 3\n",
      "Fold 4\n",
      "[1] Training loss: 0.10059, Validation loss: 0.57149\n",
      "[2] Training loss: 0.10059, Validation loss: 0.57166\n",
      "[3] Training loss: 0.10057, Validation loss: 0.57183\n",
      "[4] Training loss: 0.10052, Validation loss: 0.57201\n",
      "[5] Training loss: 0.10046, Validation loss: 0.57218\n",
      "Finished Training for Fold 4\n",
      "Fold 5\n",
      "[1] Training loss: 0.20687, Validation loss: 0.43103\n",
      "[2] Training loss: 0.20684, Validation loss: 0.43089\n",
      "[3] Training loss: 0.20676, Validation loss: 0.43086\n",
      "Finished training with best val loss of [0.14208851754665375, 0.3399982154369354, 0.08180215209722519, 0.5702815651893616, 0.43073299527168274]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "encoder.eval()\n",
    "def train_model():\n",
    "    model.train()\n",
    "    # Assuming you have your dataset X, y\n",
    "    kf = KFold(n_splits=5) # Example of 5-fold cross-validation\n",
    "    patience = 0\n",
    "    best_loss = 5*[100000]\n",
    "    while True:\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "            print(f'Fold {fold+1}')\n",
    "\n",
    "            X_t, X_val = X_train[train_index], X_train[val_index]\n",
    "            y_t, y_v = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            for epoch in range(5):  # 100 epochs\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(encoder.encoder(X_t))\n",
    "                loss = criterion(outputs, y_t)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Validation step\n",
    "                with torch.no_grad():\n",
    "                    val_outputs = model(encoder.encoder(X_val))\n",
    "                    val_loss = criterion(val_outputs, y_v)\n",
    "\n",
    "                print(f'[{epoch + 1}] Training loss: {loss.item():.5f}, Validation loss: {val_loss.item():.5f}')\n",
    "                if val_loss.item() < best_loss[fold]:\n",
    "                    best_loss[fold] = val_loss.item()\n",
    "                    patience = 0\n",
    "                elif val_loss.item() > best_loss[fold]: \n",
    "                    patience += 1\n",
    "                    if patience == 50:\n",
    "                        print(f'Finished training with best val loss of {best_loss}')\n",
    "                        return\n",
    "\n",
    "            print('Finished Training for Fold', fold+1)\n",
    "train_model()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4166.7998,  5983.4922,  2386.8806, -5036.3882, -1504.5848, -5358.5151,\n",
       "         8057.4048, -3305.3721,  3258.9795], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.encoder(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/x8ft0wb94sd72p6bpk_ld9fc0000gn/T/ipykernel_1573/2212870831.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = model(torch.tensor(encoder.encoder(X_test), dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    \n",
    "    outputs = model(torch.tensor(encoder.encoder(X_test), dtype=torch.float32))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 16\n",
    "plt.scatter([x for x in range(y_test.shape[0])], y_test[:,n], label='Actual')\n",
    "plt.scatter([x for x in range(y_test.shape[0])], outputs[:,n], label='Predicted')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel(df.columns[n+3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'TOC RMSE : 44.134229348605515')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=16\n",
    "plt.boxplot([y_test[:,n], outputs[:,n]], labels=['Actual', 'Pred'])\n",
    "loss = torch.nn.MSELoss()\n",
    "plt.title(f'{df.columns[n+3]} RMSE : {np.sqrt(loss(y_test[:,n], outputs[:,n]).item())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_comps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
